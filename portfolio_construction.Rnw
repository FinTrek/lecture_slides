% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='scriptsize', fig.width=4, fig.height=4)
options(digits=3)
options(width=60, dev='pdf')
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
% bbold package for unitary vector or matrix symbol
\usepackage{bbold}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape,bg=red,fg=red}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}



% Title page setup
\title[Portfolio Construction]{Portfolio Construction}
\subtitle{FRE6871 \& FRE7241, Spring 2017}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{\today}



%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Asset Allocation and Portfolio Construction}


%%%%%%%%%%%%%%%
\subsection{draft: Asset Allocation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Asset allocation means dividing an investment portfolio among different asset classes, such as large company stocks, small company stocks, international stocks, bonds, commodities, cash, etc. 
      \vskip1ex
      The goal of asset allocation is to diversify the sources of returns and to reduce risk, depending on the investor's risk tolerance, investment goals, and investment time horizon, 
      \vskip1ex
      For example, an investor who needs to fund college for her children might put some of her investments into government bonds that mature when her children will need to pay for college, 
      \vskip1ex
      1,600 years ago \href{http://finance.yahoo.com/news/naive-diversification-vs-optimization-163550886.html}{rabbi Isaac bar Aha} proposed a simple heuristic method (rule of thumb) for asset allocation: "put a third in land, a third in merchandise, and a third in cash", 
      <<echo=TRUE,eval=FALSE>>=
library(PortfolioAnalytics)
# use ETF returns from package HighFreq
library(HighFreq)
portf_names <- c("VTI", "IEF", "DBC", "XLF",
        "VNQ", "XLP", "XLV", "XLU", "XLB", "XLE")
# initial portfolio to equal weights
portf_init <- rep(1/NROW(portf_names),
                  NROW(portf_names))
# named vector
names(portf_init) <- portf_names
# create portfolio object
portf_init <- portfolio.spec(
  assets=portf_init)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(PortfolioAnalytics)
# add constraints
portf_maxSR <- add.constraint(
  portfolio=portf_init,  # initial portfolio
  type="weight_sum",  # constraint sum weights
  min_sum=0.9, max_sum=1.1)
# add constraints
portf_maxSR <- add.constraint(
  portfolio=portf_maxSR,
  type="long_only")  # box constraint min=0, max=1
# add objectives
portf_maxSR <- add.objective(
  portfolio=portf_maxSR,
  type="return",  # maximize mean return
  name="mean")
# add objectives
portf_maxSR <- add.objective(
  portfolio=portf_maxSR,
  type="risk",  # minimize StdDev
  name="StdDev")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Portfolio Construction}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Portfolio construction means determining the amounts to be invested different assets, such as specific stocks, bonds, commodities, etc.
      \vskip1ex
      Portfolio optimization is one approach to portfolio construction, 
      \vskip1ex
      Heuristic Methods for Portfolio Construction
      \vskip1ex
      \href{https://www.london.edu/faculty-and-research/faculty/profiles/d/demiguel-v}{Victor DeMiguel} and others have demonstrated that optimized portfolios perform poorly out-of-sample, and that simple heuristic methods can perform better than portfolio optimization, 
      \vskip1ex
      <<echo=TRUE,eval=FALSE>>=
library(PortfolioAnalytics)
# use ETF returns from package HighFreq
library(HighFreq)
portf_names <- c("VTI", "IEF", "DBC", "XLF",
        "VNQ", "XLP", "XLV", "XLU", "XLB", "XLE")
# initial portfolio to equal weights
portf_init <- rep(1/NROW(portf_names),
                  NROW(portf_names))
# named vector
names(portf_init) <- portf_names
# create portfolio object
portf_init <- portfolio.spec(
  assets=portf_init)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(PortfolioAnalytics)
# add constraints
portf_maxSR <- add.constraint(
  portfolio=portf_init,  # initial portfolio
  type="weight_sum",  # constraint sum weights
  min_sum=0.9, max_sum=1.1)
# add constraints
portf_maxSR <- add.constraint(
  portfolio=portf_maxSR,
  type="long_only")  # box constraint min=0, max=1
# add objectives
portf_maxSR <- add.objective(
  portfolio=portf_maxSR,
  type="return",  # maximize mean return
  name="mean")
# add objectives
portf_maxSR <- add.objective(
  portfolio=portf_maxSR,
  type="risk",  # minimize StdDev
  name="StdDev")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Equity Portfolio Optimization}


%%%%%%%%%%%%%%%
\subsection{Random Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<random_portfolios,echo=TRUE,eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
# vector of symbol names
sym_bols <- c("VTI", "IEF", "XLP")
n_weights <- NROW(sym_bols)
# calculate random portfolios
n_portf <- 1000
ret_sd <- sapply(1:n_portf, function(in_dex) {
  weight_s <- runif(n_weights, min=0, max=10)
  weight_s <- weight_s/sum(weight_s)
  portf_rets <- na.omit(env_etf$re_turns[, sym_bols]) %*% weight_s
  100*c(ret=mean(portf_rets), sd=sd(portf_rets))
})  # end sapply
# plot scatterplot of random portfolios
x11(width=(wid_th <- 6), height=(hei_ght <- 5))
plot(x=ret_sd[2, ], y=ret_sd[1, ], xlim=c(0, max(ret_sd[2, ])), 
     main="Random portfolios", 
     ylim=c(min(0, min(ret_sd[1, ])), max(ret_sd[1, ])), 
     xlab=rownames(ret_sd)[2], ylab=rownames(ret_sd)[1])
# vector of initial portfolio weights equal to 1
weight_s <- rep(1, n_weights)
names(weight_s) <- sym_bols
# objective function equal to standard deviation of returns
object_ive <- function(weight_s) {
  portf_rets <- na.omit(rutils::env_etf$re_turns[, sym_bols]) %*% weight_s
  sd(portf_rets)/sum(weight_s)
}  # end object_ive
# object_ive() for equal weight portfolio
object_ive(weight_s)
object_ive(2*weight_s)
# perform portfolio optimization
optim_run <- optim(par=weight_s,
                   fn=object_ive,
                   method="L-BFGS-B",
                   upper=rep(10, n_weights),
                   lower=rep(-10, n_weights))
# Rescale the optimal weights
weight_s <- optim_run$par/sum(optim_run$par)
# minimum variance portfolio returns
library(quantmod)
optim_rets <- xts(x=env_etf$re_turns[, sym_bols] %*% weight_s,
                  order.by=index(env_etf$re_turns))
chart_Series(x=cumsum(optim_rets), name="minvar portfolio")
# Add red point for minimum variance portfolio
optim_sd <- 100*sd(optim_rets)
optim_ret <- 100*mean(optim_rets)
points(x=optim_sd, y=optim_ret, col="red", lwd=3)
text(x=optim_sd, y=optim_ret, labels="minvar", pos=2, cex=0.8)
# objective function equal to minus Sharpe ratio
risk_free <- 0.01
object_ive <- function(weight_s) {
  portf_rets <- 100*env_etf$re_turns[, names(weight_s)] %*% weight_s / sum(weight_s)
  -mean(portf_rets-risk_free)/sd(portf_rets)
}  # end object_ive
# perform portfolio optimization
optim_run <- optim(par=weight_s,
                   fn=object_ive,
                   method="L-BFGS-B",
                   upper=rep(10, n_weights),
                   lower=rep(-10, n_weights))
# maximum Sharpe ratio portfolio returns
weight_s <- optim_run$par/sum(optim_run$par)
optim_rets <- xts(x=env_etf$re_turns[, sym_bols] %*% weight_s,
                  order.by=index(env_etf$re_turns))
chart_Series(x=cumsum(optim_rets), name="maxSR portfolio")
optim_sd <- 100*sd(optim_rets)
optim_ret <- 100*mean(optim_rets)
points(x=optim_sd, y=optim_ret, 
       col="blue", lwd=3)
text(x=optim_sd, y=optim_ret,
     labels="maxSR", pos=2, cex=0.8)
max_Sharpe <- (optim_ret-risk_free)/optim_sd
# Add points for individual assets
re_turns <- 100*sapply(env_etf$re_turns[, sym_bols], mean)
std_devs <- 100*sapply(env_etf$re_turns[, sym_bols], sd)
points(std_devs, re_turns, col="green", lwd=3)
text(std_devs, re_turns, labels=names(re_turns), pos=2, cex=0.8)
# Add point at risk_free rate and draw Capital Market Line
points(x=0, y=risk_free)
text(0, risk_free, labels="risk-free", pos=4, cex=0.8)
abline(a=risk_free, b=max_Sharpe, col="blue")
range_s <- par("usr")
text(optim_sd/3, (optim_ret+risk_free)/2.5, 
     labels="Capital Market Line", cex=0.8, , pos=3, 
     srt=45*atan(max_Sharpe*(range_s[2]-range_s[1])/
                   (range_s[4]-range_s[3])*
                   hei_ght/wid_th)/(0.25*pi))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/random_portfolios.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Capital Market Line and Two-asset Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<corr_two_assets,echo=TRUE,eval=FALSE>>=
risk_free <- 0.01
re_turns <- c(asset1=0.02, asset2=0.04)
std_devs <- c(asset1=0.8, asset2=1.6)
cor_rel <- 0.6
co_var <- matrix(c(1, cor_rel, cor_rel, 1), nc=2)
co_var <- t(t(std_devs*co_var)*std_devs)
weight_s <- seq(from=-1, to=2, length.out=31)
weight_s <- cbind(weight_s, 1-weight_s)
portf_rets <- weight_s %*% re_turns
portf_sd <- 
  sqrt(rowSums(weight_s * (weight_s %*% co_var)))
sharpe_ratios <- (portf_rets-risk_free)/portf_sd
in_dex <- which.max(sharpe_ratios)
max_Sharpe <- max(sharpe_ratios)
# plot efficient frontier
x11(width=(wid_th <- 6), height=(hei_ght <- 5))
par(mar=c(3,3,2,1)+0.1, oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
plot(portf_sd, portf_rets, t="l", 
 main=paste0("Two assets correlation = ", 100*cor_rel, "%"), 
 xlim=c(0, max(portf_sd)), 
 ylim=c(0, max(portf_rets)))
# Add red point for maximum Sharpe ratio portfolio
points(portf_sd[in_dex], portf_rets[in_dex], 
       col="red", lwd=3)
text(x=portf_sd[in_dex], y=portf_rets[in_dex], 
     labels=paste(c("maxSR\n", 
       structure(c(weight_s[in_dex], 1-weight_s[in_dex]), 
               names=names(re_turns))), collapse=" "), 
     pos=2, cex=0.8)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/cml_two_assets.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Add points for individual assets
points(std_devs, re_turns, col="green", lwd=3)
text(std_devs, re_turns, labels=names(re_turns), pos=2, cex=0.8)
# Add point at risk_free rate and draw Capital Market Line
points(x=0, y=risk_free)
text(0, risk_free, labels="risk-free", pos=4, cex=0.8)
abline(a=risk_free, b=max_Sharpe, col="blue")
range_s <- par("usr")
text(portf_sd[in_dex]/2, (portf_rets[in_dex]+risk_free)/2, 
     labels="Capital Market Line", cex=0.8, , pos=3, 
     srt=45*atan(max_Sharpe*(range_s[2]-range_s[1])/
                   (range_s[4]-range_s[3])*
                   hei_ght/wid_th)/(0.25*pi))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Efficient Frontier for Stock and Bond Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<eff_front_two_assets,echo=TRUE,eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
# vector of symbol names
sym_bols <- c("VTI", "IEF")
# matrix of portfolio weights
weight_s <- seq(from=-1, to=2, length.out=31)
weight_s <- cbind(weight_s, 1-weight_s)
# calculate portfolio returns and volatilities
portf_rets <- env_etf$re_turns[, sym_bols]
ret_sd <- portf_rets %*% t(weight_s)
ret_sd <- 100*cbind(colMeans(ret_sd), 
  matrixStats::colSds(ret_sd))
colnames(ret_sd) <- c("returns", "StdDev")
risk_free <- 5.0/260
ret_sd <- cbind(ret_sd, (ret_sd[, 1]-risk_free)/ret_sd[, 2])
colnames(ret_sd)[3] <- "Sharpe"
in_dex <- which.max(ret_sd[, "Sharpe"])
max_Sharpe <- ret_sd[in_dex, "Sharpe"]
# plot scatterplot of portfolios in x11() window
x11(width=(wid_th <- 6), height=(hei_ght <- 4))
par(mar=c(3,3,2,1)+0.1, oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
plot(x=ret_sd[, 2], y=ret_sd[, 1], t="l", 
     xlim=c(0, max(ret_sd[, 2]/2)), 
     ylim=c(min(0, min(ret_sd[, 1])), max(ret_sd[, 1])), 
     xlab=colnames(ret_sd)[2], ylab=colnames(ret_sd)[1])
title(main="Stock and bond portfolios", line=-1)
# Add red point for maximum Sharpe ratio portfolio
points(x=ret_sd[in_dex, 2], y=ret_sd[in_dex, 1], 
       col="red", lwd=3)
text(x=ret_sd[in_dex, 2], y=ret_sd[in_dex, 1], 
     labels=paste(c("maxSR\n", 
       structure(c(weight_s[in_dex, 1], weight_s[in_dex, 2]), 
               names=sym_bols)), collapse=" "), 
     pos=3, cex=0.8)
# Add points for individual assets
re_turns <- 100*sapply(portf_rets, mean)
std_devs <- 100*sapply(portf_rets, sd)
points(std_devs, re_turns, col="green", lwd=3)
text(std_devs, re_turns, labels=names(re_turns), pos=2, cex=0.8)
# Add point at risk_free rate and draw Capital Market Line
points(x=0, y=risk_free)
text(0, risk_free, labels="risk-free", pos=4, cex=0.8)
abline(a=risk_free, b=max_Sharpe, col="blue")
range_s <- par("usr")
text(ret_sd[in_dex, 2]/3, (ret_sd[in_dex, 1]+risk_free)/2, 
     labels="Capital Market Line", cex=0.8, , pos=3, 
     srt=45*atan(max_Sharpe*(range_s[2]-range_s[1])/
                   (range_s[4]-range_s[3])*
                   hei_ght/wid_th)/(0.25*pi))
# plot max Sharpe ratio portfolio returns
library(quantmod)
optim_rets <- 
  xts(x=env_etf$re_turns[, sym_bols] %*% 
        c(weight_s[in_dex], 1-weight_s[in_dex]),
      order.by=index(env_etf$re_turns))
chart_Series(x=cumsum(optim_rets), 
             name="Max Sharpe two-asset portfolio")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/eff_front_two_assets.png}\\
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/maxsr_two_assets.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Efficient Frontier and Capital Market Line}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Efficient Frontier is the set of portfolios that have the highest returns among all portfolios with the same risk (standard deviation of returns),
      \vskip1ex
      The Capital Market Line (CML) is the line drawn from the risk-free asset to the tangency point on the Efficient Frontier,
      \vskip1ex
      The tangency point on the Efficient Frontier is considered to be the Market Portfolio,
      <<optim_random,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PortfolioAnalytics)
# plot the efficient frontier
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/eff_front_two_assets.png}\\
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/maxsr_two_assets.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Objective Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{optimize()} performs \emph{one-dimensional} optimization over a single independent variable,
      \vskip1ex
      \texttt{optimize()} searches for the minimum of the objective function with respect to its first argument, in the specified interval,
        <<eval=FALSE,echo=TRUE>>=
# create vector of symbols for optimized portfolio
sym_bols <- c("VTI", "VNQ", "DBC")
# create initial vector of portfolio weights
weight_s <- rep(1, NROW(sym_bols))
names(weight_s) <- sym_bols
# objective equal to minus Sharpe ratio
object_ive <- function(weight_s) {
  portf_rets <- env_etf$re_turns[, sym_bols] %*% weight_s
  -mean(portf_rets)/sd(portf_rets)
}  # end object_ive
# objective for equal weight portfolio
object_ive(weight_s)
      @
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/portf_optim_one_dim-1}
      \vspace{-5em}
        <<portf_optim_one_dim,eval=FALSE,echo=(-(1:1)),fig.show='hide'>>=
par(oma=c(1, 1, 1, 1), mgp=c(2, 1, 0), mar=c(5, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# vectorize objective function with respect to third weight
vec_object <- Vectorize(
  FUN=function(weight) object_ive(c(1, 1, weight)),
  vectorize.args="weight"
)  # end Vectorize
# plot objective function with respect to third weight
curve(expr=vec_object,
      type="l", xlim=c(-4.0, 1.0),
      xlab=paste("weight of", names(weight_s[3])),
      ylab="", lwd=2)
title(main="Objective Function", line=-1)  # add title
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Perspective Plot of Portfolio Objective Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{persp()} plots a 3d perspective surface plot of a function specified over a grid of argument values,
      \vskip1ex
      The function \texttt{outer()} calculates the values of a function over a grid spanned by two variables, and returns a matrix of function values,
      <<portf_persp,echo=TRUE,eval=FALSE,fig.width=10,fig.height=10,fig.show='hide'>>=
# vectorize function with respect to all weights
vec_object <- Vectorize(
  FUN=function(w1, w2, w3)
    object_ive(c(w1, w2, w3)),
  vectorize.args=c("w2", "w3"))  # end Vectorize
# calculate objective on 2-d (w2 x w3) parameter grid
w2 <- seq(-5, 5, length=50)
w3 <- seq(-5, 5, length=50)
grid_object <- outer(w2, w3, FUN=vec_object, w1=1)
rownames(grid_object) <- round(w2, 2)
colnames(grid_object) <- round(w3, 2)
# perspective plot of objective function
persp(w2, w3, -grid_object,
      theta=45, phi=30, shade=0.5,
      col=rainbow(50), border="green",
      main="objective function")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/portf_persp-1}
    \vspace{-5em}
      <<echo=TRUE,eval=FALSE,fig.width=10,fig.height=10>>=
# interactive perspective plot of objective function
library(rgl)
persp3d(z=-grid_object, zlab="objective",
        col="green", main="objective function")
persp3d(
  x=function(w2, w3)
    -vec_object(w1=1, w2, w3),
  xlim=c(-5, 5), ylim=c(-5, 5),
  col="green", axes=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Multi-dimensional Portfolio Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{optim()} performs \emph{multi-dimensional} optimization,
      \vskip1ex
      The argument \texttt{par} are the initial parameter values,
      \vskip1ex
      The argument \texttt{fn} is the objective function to be minimized,
      \vskip1ex
      The argument of the objective function which is to be optimized, must be a vector argument,
      \vskip1ex
      \texttt{optim()} accepts additional parameters bound to the dots \texttt{"..."} argument, and passes them to the \texttt{fn} objective function,
      \vskip1ex
      The arguments \texttt{lower} and \texttt{upper} specify the search range for the variables of the objective function \texttt{fn},
      \vskip1ex
      \texttt{method="L-BFGS-B"} specifies the quasi-Newton optimization method,
      \vskip1ex
      \texttt{optim()} returns a list containing the location of the minimum and the objective function value,
    \column{0.5\textwidth}
    \vspace{-1em}
      <<eval=FALSE,echo=TRUE>>=
# optimization to find weights with maximum Sharpe ratio
optim_run <- optim(par=weight_s,
                   fn=object_ive,
                   method="L-BFGS-B",
                   upper=c(1.1, 10, 10),
                   lower=c(0.9, -10, -10))
# optimal parameters
optim_run$par
# optimal Sharpe ratio
-object_ive(optim_run$par)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimized Portfolio Plot}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The optimized portfolio has both long and short positions, and outperforms its individual component assets,
      \vskip1ex
      \vspace{-1em}
      <<optim_portf_basic,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=8,fig.show='hide'>>=
par(oma=c(1, 0, 1, 0), mgp=c(2, 1, 0), mar=c(2, 1, 2, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
library(PortfolioAnalytics)
# returns of optimal portfolio
optim_rets <- xts(env_etf$re_turns[, sym_bols] %*%
      optim_run$par, order.by=index(env_etf$re_turns))
# assign colnames to this xts
colnames(optim_rets) <- "optim_rets"
# plot in two vertical panels
layout(matrix(c(1,2), 2),
       widths=c(1,1), heights=c(1,3))
# barplot of optimal portfolio weights
barplot(optim_run$par,
        names.arg=names(optim_run$par),
        las=3, ylab="", xlab="Symbol", main="")
# plot optimal returns with "VTI", "VNQ" and "DBC"
chart.CumReturns(
  cbind(optim_rets, env_etf$re_turns[, names(weight_s)]),
  lwd=2, ylab="", legend.loc="topleft", main="")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_portf_basic-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Mean Variance Portfolio Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A Simple Linear Regression is a linear model between a response variable \texttt{z} and a single explanatory variable \texttt{x}, defined by the formula:
      \begin{displaymath}
        z_i = \alpha + \beta x_i + \varepsilon_i
      \end{displaymath}
      The data consists of $n$ observations of the response and explanatory variables, with the index $i$ ranging from $1$ to $n$,
      \vskip1ex
      $\alpha$ and $\beta$ are the unknown regression coefficients,
      \vskip1ex
      $\varepsilon_i$ are the residuals, assumed to be normally distributed, independent, and stationary,
      \vskip1ex
      In the Ordinary Least Squares method (OLS), the regression parameters are estimated by minimizing the sum of squared residuals, also called the residual sum of squares (RSS):
      \begin{align*}
        RSS = \sum_{i=1}^{n} {\varepsilon_i^2} = \sum_{i=1}^{n} {(z_i - \alpha - \beta x_i)^2}\\ = (z - \alpha \mathbb{1} - \beta x)^T (z - \alpha \mathbb{1} - \beta x)
      \end{align*}
      $\mathbb{1}$ is the unit vector, and $\mathbb{1}^T x = x^T \mathbb{1} = \sum_{i=1}^{n} {x_i}$
    \column{0.5\textwidth}
      The regression coefficients can be found by equating the RSS derivatives to zero:
      \begin{align*}
        RSS_\alpha = -2 (z - \alpha \mathbb{1} - \beta x)^T \mathbb{1} = 0\\
        RSS_\beta = -2 (z - \alpha \mathbb{1} - \beta x)^T x = 0
      \end{align*}
      The solution for $\alpha$ is:
      \begin{align*}
        \alpha = z^T \mathbb{1} - \beta x^T \mathbb{1}
      \end{align*}
      The solution for $\beta$ is:
      \begin{flalign*}
        & (z - (z^T \mathbb{1} - \beta x^T \mathbb{1}) \mathbb{1} - \beta x)^T x = 0\\
        & (z^T x - (z^T \mathbb{1} - \beta x^T \mathbb{1}) \mathbb{1}^T x - \beta x^T x) = 0\\
        & (z^T x - (z^T \mathbb{1}) (x^T \mathbb{1}) + \beta (x^T \mathbb{1})^2 - \beta x^T x) = 0\\
        & \beta = \frac {z^T x - (x^T \mathbb{1}) (z^T \mathbb{1}) } {x^T x - (\mathbb{1}^T x)^2}
      \end{flalign*}
      If the response and explanatory variables have zero mean, then $\alpha=0$ and $\beta=\frac {z^T x} {x^T x}$,
      \vskip1ex
      It's then easy to see that $\beta$ is proportional to the correlation coefficient between the response and explanatory variables,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Mean Variance Portfolio Optimization With Constraints}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A Linear Regression model with $p$ explanatory variables $\{x_j\}$, is defined by the formula:
      \begin{displaymath}
        z_i = \alpha + \sum_{j=1}^{k} {\beta_j x_{i,j}} + \varepsilon_i
      \end{displaymath}
      Or in vector notation:
      \begin{displaymath}
        z = \alpha + \beta x + \varepsilon
      \end{displaymath}
      The response variable $z$ and the $p$ explanatory variables $\{x_j\}$ each contain $n$ observations,
      \vskip1ex
      The response variable $z$ is a vector of length $n$, and the explanatory variable $x$ is a $(n,p)$-dimensional matrix,
      \vskip1ex
      $\alpha$ and $\beta$ are the unknown regression coefficients, with $\alpha$ a scalar and $\beta$ a vector of length $p$,
      \vskip1ex
      $\varepsilon_i$ are the residuals, assumed to be normally distributed, independent, and stationary, with $\varepsilon$ a vector of length $p$,
    \column{0.5\textwidth}
      The OLS estimate for $\alpha$ is given by:
      \begin{align*}
        \alpha = z^T \mathbb{1} - \beta x^T \mathbb{1}
      \end{align*}
      If the variables are de-meaned, then the OLS estimate for $\beta$ is given by equating the RSS derivative to zero:
      \begin{flalign*}
        & RSS_\beta = - 2 (z - \beta x)^T x = 0\\
        & x^T z - \beta x^T x = 0\\
        & \beta = (x^T x)^{-1} x^T z
      \end{flalign*}
      The matrix $x^T x$ is the covariance matrix of the matrix $x$,
      \vskip1ex
      The covariance matrix $x^T x$ is invertible if the columns of $x$ are linearly independent,
      \vskip1ex
      The matrix $(x^T x)^{-1} x^T$ is known as the \emph{Moore-Penrose pseudo-inverse} of the matrix $x$,
      \vskip1ex
      In the special case when the inverse matrix $x^{-1}$ does exist, then the \emph{pseudo-inverse} matrix simplifies to the inverse: $(x^T x)^{-1} x^T = x^{-1} (x^T)^{-1} x^T = x^{-1}$
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Package \protect\emph{PortfolioAnalytics} for Portfolio Optimization}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{PortfolioAnalytics}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The package \emph{PortfolioAnalytics} contains functions and data sets for portfolio optimization,
      \vskip1ex
      The function \texttt{data()} loads external data or lists data sets in a package,
      \vskip1ex
    \column{0.6\textwidth}
      \vspace{-1em}
      <<eval=FALSE>>=
library(PortfolioAnalytics)  # load package "PortfolioAnalytics"
# get documentation for package "PortfolioAnalytics"
packageDescription("PortfolioAnalytics")  # get short description

help(package="PortfolioAnalytics")  # load help page

data(package="PortfolioAnalytics")  # list all datasets in "PortfolioAnalytics"

ls("package:PortfolioAnalytics")  # list all objects in "PortfolioAnalytics"

detach("package:PortfolioAnalytics")  # remove PortfolioAnalytics from search path
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Definition}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Portfolios are defined by a named vector of asset weights, and portfolio constraints and objectives,
      \vskip1ex
      \texttt{portfolio.spec} creates a portfolio object that contains asset weights, constraints, and objectives,
      \vskip1ex
      \texttt{add.constraint} adds or updates constraints on of the portfolio object,
      \vskip1ex
      \texttt{add.objective} adds or updates risk/return objectives of the portfolio object,
      <<echo=TRUE,eval=FALSE>>=
library(PortfolioAnalytics)
# use ETF returns from package HighFreq
library(HighFreq)
portf_names <- c("VTI", "IEF", "DBC", "XLF",
        "VNQ", "XLP", "XLV", "XLU", "XLB", "XLE")
# initial portfolio to equal weights
portf_init <- rep(1/NROW(portf_names),
                  NROW(portf_names))
# named vector
names(portf_init) <- portf_names
# create portfolio object
portf_init <- portfolio.spec(
  assets=portf_init)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(PortfolioAnalytics)
# add constraints
portf_maxSR <- add.constraint(
  portfolio=portf_init,  # initial portfolio
  type="weight_sum",  # constraint sum weights
  min_sum=0.9, max_sum=1.1)
# add constraints
portf_maxSR <- add.constraint(
  portfolio=portf_maxSR,
  type="long_only")  # box constraint min=0, max=1
# add objectives
portf_maxSR <- add.objective(
  portfolio=portf_maxSR,
  type="return",  # maximize mean return
  name="mean")
# add objectives
portf_maxSR <- add.objective(
  portfolio=portf_maxSR,
  type="risk",  # minimize StdDev
  name="StdDev")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_SR_basic,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
load(file="C:/Develop/data/portf_optim.RData")
library(PortfolioAnalytics)
# perform optimization of weights
maxSR_DEOpt <- optimize.portfolio(
  R=env_etf$re_turns[, portf_names],  # specify returns
  portfolio=portf_maxSR,  # specify portfolio
  optimize_method="DEoptim", # use DEoptim
  maxSR=TRUE,  # maximize Sharpe
  trace=TRUE, traceDE=0)
# plot optimization
chart.RiskReward(maxSR_DEOpt,
  risk.col="StdDev",
  return.col="mean")
      @
    \vspace{-2em}
      <<echo=(-(1:3)),eval=FALSE,tidy=TRUE>>=
options(width=50)
library(PortfolioAnalytics)
load(file="C:/Develop/data/portf_optim.RData")
maxSR_DEOpt$weights
maxSR_DEOpt$objective_measures$mean[1]
maxSR_DEOpt$objective_measures$StdDev[[1]]
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_SR_basic-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Scatterplot}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_SR_scatter,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PortfolioAnalytics)
# plot optimization
chart.RiskReward(maxSR_DEOpt,
  risk.col="StdDev",
  return.col="mean")

# plot risk/ret points in portfolio scatterplot
risk_ret_points <- function(rets=env_etf$re_turns,
        risk=c("sd", "ETL"), sym_bols=c("VTI", "IEF")) {
  risk <- match.arg(risk)  # match to arg list
  if (risk=="ETL") {
    stopifnot(
      "package:PerformanceAnalytics" %in% search() ||
      require("PerformanceAnalytics", quietly=TRUE))
  }  # end if
  risk <- match.fun(risk)  # match to function
  risk_ret <- t(sapply(rets[, sym_bols],
     function(x_ts)
       c(ret=mean(x_ts), risk=abs(risk(x_ts)))))
  points(x=risk_ret[, "risk"], y=risk_ret[, "ret"],
         col="red", lwd=3)
  text(x=risk_ret[, "risk"], y=risk_ret[, "ret"],
       labels=rownames(risk_ret), col="red",
       lwd=2, pos=4)
}  # end risk_ret_points

risk_ret_points()
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_SR_scatter-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimized Sharpe Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-2em}
      <<optim_portf_SR_vis,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=8,fig.show='hide'>>=
library(PortfolioAnalytics)
plot_portf <- function(portfolio,
            rets_data=env_etf$re_turns) {
  weight_s <- portfolio$weights
  portf_names <- names(weight_s)
  # calculate xts of portfolio
  portf_max <- xts(
    rets_data[, portf_names] %*% weight_s,
    order.by=index(rets_data))
  colnames(portf_max) <-
    deparse(substitute(portfolio))
  graph_params <- par(oma=c(1, 0, 1, 0),
    mgp=c(2, 1, 0), mar=c(2, 1, 2, 1),
    cex.lab=0.8, cex.axis=1.0,
    cex.main=0.8, cex.sub=0.5)
  layout(matrix(c(1,2), 2),
    widths=c(1,1), heights=c(1,3))
  barplot(weight_s, names.arg=portf_names,
          las=3, ylab="", xlab="Symbol", main="")
  title(main=paste("Loadings",
                colnames(portf_max)), line=-1)
  chart.CumReturns(
    cbind(portf_max, rets_data[, c("IEF", "VTI")]),
    lwd=2, ylab="", legend.loc="topleft", main="")
  title(main=paste0(colnames(portf_max),
                    ", IEF, VTI"), line=-1)
  par(graph_params)  # restore original parameters
  invisible(portf_max)
}  # end plot_portf
maxSR_DEOpt_xts <- plot_portf(portfolio=maxSR_DEOpt)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_portf_SR_vis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Leverage Constraints}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The leverage constraint applies to the sum of absolute weights,
      \vspace{-1em}
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(PortfolioAnalytics)
# add leverage constraint abs(weight_sum)
portf_maxSRN <- add.constraint(
  portfolio=portf_init, type="leverage",
  min_sum=0.9, max_sum=1.1)
# add box constraint long/short
portf_maxSRN <- add.constraint(
  portfolio=portf_maxSRN,
  type="box", min=-0.2, max=0.2)

# add objectives
portf_maxSRN <- add.objective(
  portfolio=portf_maxSRN,
  type="return",  # maximize mean return
  name="mean")
# add objectives
portf_maxSRN <- add.objective(
  portfolio=portf_maxSRN,
  type="risk",  # minimize StdDev
  name="StdDev")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Leverage Constraint Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_SR_leverage,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
load(file="C:/Develop/data/portf_optim.RData")
library(PortfolioAnalytics)
# perform optimization of weights
maxSRN_DEOpt <- optimize.portfolio(
  R=env_etf$re_turns[, portf_names],  # specify returns
  portfolio=portf_maxSRN,  # specify portfolio
  optimize_method="DEoptim", # use DEoptim
  maxSR=TRUE,  # maximize Sharpe
  trace=TRUE, traceDE=0)
# plot optimization
chart.RiskReward(maxSRN_DEOpt,
  risk.col="StdDev",
  return.col="mean",
  xlim=c(
    maxSR_DEOpt$objective_measures$StdDev[[1]]-0.001,
    0.016))
  points(x=maxSR_DEOpt$objective_measures$StdDev[[1]],
         y=maxSR_DEOpt$objective_measures$mean[1],
         col="green", lwd=3)
  text(x=maxSR_DEOpt$objective_measures$StdDev[[1]],
         y=maxSR_DEOpt$objective_measures$mean[1],
       labels="maxSR", col="green",
       lwd=2, pos=4)
# plot risk/ret points in portfolio scatterplot
risk_ret_points()
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_SR_leverage-1}
    \vspace{-2em}
      <<echo=(-(1:2)),eval=FALSE,tidy=TRUE>>=
library(PortfolioAnalytics)
load(file="C:/Develop/data/portf_optim.RData")
maxSRN_DEOpt$weights
maxSRN_DEOpt$objective_measures$mean[1]
maxSRN_DEOpt$objective_measures$StdDev[[1]]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimized Leverage Constraint Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_portf_SRN_vis,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=8,fig.show='hide'>>=
library(PortfolioAnalytics)
maxSRN_DEOpt_xts <-
  plot_portf(portfolio=maxSRN_DEOpt)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_portf_SRN_vis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Sharpe Portfolios \texttt{CumReturns} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{chart.CumReturns()} plots the cumulative returns of a time series of returns,
      <<optim_SR_SRN_vis,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PerformanceAnalytics)
load(file="C:/Develop/data/portf_optim.RData")
chart.CumReturns(
  cbind(maxSR_DEOpt_xts, maxSRN_DEOpt_xts),
  lwd=2, ylab="",
  legend.loc="topleft", main="")
      @
    \vspace{-2em}
      <<echo=(-(1:3)),eval=FALSE>>=
options(width=50)
library(PerformanceAnalytics)
load(file="C:/Develop/data/portf_optim.RData")
rbind(maxSR_DEOpt$weights, maxSRN_DEOpt$weights)
c(maxSR_DEOpt$objective_measures$mean,
maxSRN_DEOpt$objective_measures$mean)
c(maxSR_DEOpt$objective_measures$StdDev[[1]],
maxSRN_DEOpt$objective_measures$StdDev[[1]])
      @
    \column{0.5\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_SR_SRN_vis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{STARR Portfolio Constraints}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The objective constraint applies to risk or return,
      \vspace{-1em}
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(PortfolioAnalytics)
# add constraints
portf_maxSTARR <- add.constraint(
  portfolio=portf_init,  # initial portfolio
  type="weight_sum",  # constraint sum weights
  min_sum=0.9, max_sum=1.1)
# add constraints
portf_maxSTARR <- add.constraint(
  portfolio=portf_maxSTARR,
  type="long_only")  # box constraint min=0, max=1
# add objectives
portf_maxSTARR <- add.objective(
  portfolio=portf_maxSTARR,
  type="return",  # maximize mean return
  name="mean")
# add objectives
portf_maxSTARR <- add.objective(
  portfolio=portf_maxSTARR,
  type="risk",  # minimize Expected Shortfall
  name="ES")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{STARR Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_STARR,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
load(file="C:/Develop/data/portf_optim.RData")
library(PortfolioAnalytics)
# perform optimization of weights
maxSTARR_DEOpt <- optimize.portfolio(
  R=env_etf$re_turns[, portf_names],  # specify returns
  portfolio=portf_maxSTARR,  # specify portfolio
  optimize_method="DEoptim", # use DEoptim
  maxSTARR=TRUE,  # maximize STARR
  trace=TRUE, traceDE=0)

# plot optimization
chart.RiskReward(maxSTARR_DEOpt,
  risk.col="ES",
  return.col="mean")
# plot risk/ret points in portfolio scatterplot
risk_ret_points(risk="ETL")
      @
    \vspace{-2em}
      <<echo=(-(1:3)),eval=FALSE,tidy=TRUE>>=
options(width=50)
library(PortfolioAnalytics)
load(file="C:/Develop/data/portf_optim.RData")
maxSTARR_DEOpt$weights
maxSTARR_DEOpt$objective_measures$mean[1]
maxSTARR_DEOpt$objective_measures$ES[[1]]
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_STARR-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimized STARR Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_STARR_vis,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=8,fig.show='hide'>>=
library(PortfolioAnalytics)
maxSTARR_DEOpt_xts <-
  plot_portf(portfolio=maxSTARR_DEOpt)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_STARR_vis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Sharpe STARR \texttt{CumReturns} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{chart.CumReturns()} plots the cumulative returns of a time series of returns,
      <<optim_SR_STARR_vis,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PerformanceAnalytics)
load(file="C:/Develop/data/portf_optim.RData")
chart.CumReturns(
  cbind(maxSR_DEOpt_xts, maxSTARR_DEOpt_xts),
  lwd=2, ylab="",
  legend.loc="topleft", main="")
      @
    \vspace{-2em}
      <<echo=(-(1:3)),eval=FALSE>>=
options(width=50)
library(PerformanceAnalytics)
load(file="C:/Develop/data/portf_optim.RData")
rbind(maxSR_DEOpt$weights, maxSTARR_DEOpt$weights)
c(maxSR_DEOpt$objective_measures$mean,
maxSTARR_DEOpt$objective_measures$mean)
c(maxSR_DEOpt$objective_measures$StdDev[[1]],
maxSTARR_DEOpt$objective_measures$ES[[1]])
      @
    \column{0.5\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_SR_STARR_vis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Efficient Frontier and Capital Market Line}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Efficient Frontier is the set of portfolios that have the highest returns among all portfolios with the same risk (standard deviation of returns),
      \vskip1ex
      The Capital Market Line (CML) is the line drawn from the risk-free asset to the tangency point on the Efficient Frontier,
      \vskip1ex
      The tangency point on the Efficient Frontier is considered to be the Market Portfolio,
      <<optim_eff_front,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PortfolioAnalytics)
# plot the efficient frontier
chart.EfficientFrontier(maxSR_DEOpt,
                match.col="StdDev",
                n.portfolios=15, type="l")
points(x=maxSRN_DEOpt$objective_measures$StdDev[[1]],
         y=maxSRN_DEOpt$objective_measures$mean[1],
         col="green", lwd=3)
text(x=maxSRN_DEOpt$objective_measures$StdDev[[1]],
         y=maxSRN_DEOpt$objective_measures$mean[1],
       labels="maxSRN", col="green",
       lwd=2, pos=4)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_eff_front-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{minES Portfolio Constraints}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The objective constraint applies to risk or return,
      \vspace{-1em}
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(PortfolioAnalytics)
# add constraints
portf_minES <- add.constraint(
  portfolio=portf_init,  # initial portfolio
  type="weight_sum",  # constraint sum weights
  min_sum=0.9, max_sum=1.1)
# add constraints
portf_minES <- add.constraint(
  portfolio=portf_minES,
  type="long_only")  # box constraint min=0, max=1
# add objectives
portf_minES <- add.objective(
  portfolio=portf_minES,
  type="risk",  # minimize ES
  name="ES")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{minES Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_minES,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
load(file="C:/Develop/data/portf_optim.RData")
library(PortfolioAnalytics)
# perform optimization of weights
minES_ROI <- optimize.portfolio(
  R=env_etf$re_turns[, portf_names],  # specify returns
  portfolio=portf_minES,  # specify portfolio
  optimize_method="ROI", # use ROI
  trace=TRUE, traceDE=0)

# plot optimization
chart.RiskReward(maxSTARR_DEOpt,
  risk.col="ES",
  return.col="mean")
  points(x=minES_ROI$objective_measures$ES[[1]],
         y=mean(minES_ROI_xts),
         col="green", lwd=3)
  text(x=minES_ROI$objective_measures$ES[[1]],
         y=mean(minES_ROI_xts),
       labels="minES", col="green",
       lwd=2, pos=4)
# plot risk/ret points in portfolio scatterplot
risk_ret_points(risk="ETL")
      @
    \vspace{-2em}
      <<echo=(-(1:3)),eval=FALSE,tidy=TRUE>>=
options(width=50)
library(PortfolioAnalytics)
load(file="C:/Develop/data/portf_optim.RData")
minES_ROI$weights
minES_ROI$objective_measures$ES[[1]]
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_minES-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimized minES Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_minES_vis,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=8,fig.show='hide'>>=
library(PortfolioAnalytics)
minES_ROI_xts <-
  plot_portf(portfolio=minES_ROI)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_minES_vis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Sharpe minES \texttt{CumReturns} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{chart.CumReturns()} plots the cumulative returns of a time series of returns,
      <<optim_SR_minES_vis,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PerformanceAnalytics)
load(file="C:/Develop/data/portf_optim.RData")
chart.CumReturns(
  cbind(maxSR_DEOpt_xts, minES_ROI_xts),
  lwd=2, ylab="",
  legend.loc="topleft", main="")
      @
    \vspace{-2em}
      <<echo=(-(1:3)),eval=FALSE>>=
options(width=50)
library(PerformanceAnalytics)
load(file="C:/Develop/data/portf_optim.RData")
rbind(maxSR_DEOpt$weights, minES_ROI$weights)
c(maxSR_DEOpt$objective_measures$mean,
minES_ROI$objective_measures$mean)
c(maxSR_DEOpt$objective_measures$StdDev[[1]],
minES_ROI$objective_measures$ES[[1]])
      @
    \column{0.5\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_SR_minES_vis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Out-of-sample Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_SR_1h,echo=(-(1:3)),eval=FALSE,fig.width=7,fig.height=8,fig.show='hide'>>=
load(file="C:/Develop/data/portf_optim.RData")
library(PortfolioAnalytics)
options(width=50)
# perform optimization of weights
maxSR_DEOpt <- optimize.portfolio(
  R=env_etf$re_turns["/2011", portf_names],
  portfolio=portf_maxSR,  # specify portfolio
  optimize_method="DEoptim", # use DEoptim
  maxSR=TRUE,  # maximize Sharpe
  trace=TRUE, traceDE=0)
weights_1h <- maxSR_DEOpt$weights

# plot optimization
maxSR_DEOpt_xts <-
  plot_portf(portfolio=maxSR_DEOpt)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_SR_1h-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Out-of-sample Portfolios (cont.)}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_SR_2h,echo=(-(1:3)),eval=FALSE,fig.width=7,fig.height=8,fig.show='hide'>>=
load(file="C:/Develop/data/portf_optim.RData")
library(PortfolioAnalytics)
options(width=50)
# perform optimization of weights
maxSR_DEOpt <- optimize.portfolio(
  R=env_etf$re_turns["2011/", portf_names],
  portfolio=portf_maxSR,  # specify portfolio
  optimize_method="DEoptim", # use DEoptim
  maxSR=TRUE,  # maximize Sharpe
  trace=TRUE, traceDE=0)
weights_2h <- maxSR_DEOpt$weights

# plot optimization
maxSR_DEOpt_xts <-
  plot_portf(portfolio=maxSR_DEOpt)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_SR_2h-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Out-of-sample Portfolio Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
options(width=50)
weights_1h
weights_2h
weights_1h - weights_2h
      @
    \vspace{-2em}
      <<optim_weights,echo=(-(1:2)),eval=FALSE,fig.height=5,fig.show='hide'>>=
par(oma=c(1,0,1,0), mgp=c(2,1,0), mar=c(2,1,2,1), cex.lab=0.8, cex.axis=1.0, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # set plot panels
barplot(weights_1h,
        names.arg=names(weights_1h),
        las=3, ylab="", xlab="",
        main="Portfolio Weights First Half")
barplot(weights_2h,
        names.arg=names(weights_2h),
        las=3, ylab="", xlab="",
        main="Portfolio Weights Second Half")
      @
    \column{0.5\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/optim_weights-1}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Bonds and Interest Rates}


%%%%%%%%%%%%%%%
\subsection{Downloading Treasury Bond Rates from \protect\emph{FRED}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The constant maturity Treasury rates are yields of hypothetical fixed-maturity bonds, interpolated from the market yields of actual Treasury bonds, 
      \vskip1ex
      The \emph{FRED} database contains current and historical constant maturity Treasury rates, \\
      \hskip1em\url{https://fred.stlouisfed.org/series/DGS5}
      \vskip1ex
      \texttt{getSymbols()} creates objects in the specified \emph{environment} from the input strings (names),
      \vskip1ex
      It then assigns the data to those objects, without returning them as a function value, as a \emph{side effect},
      <<echo=TRUE,eval=FALSE>>=
# symbols for constant maturity Treasury rates
sym_bols <- c("DGS1", "DGS2", "DGS5", "DGS10", "DGS20", "DGS30")
library(quantmod)  # load package quantmod
env_rates <- new.env()  # new environment for data
# download data for sym_bols into env_rates
getSymbols(sym_bols, env=env_rates, src="FRED")
ls(env_rates)  # list files in env_rates
# get class of object in env_rates
class(get(x=sym_bols[1], envir=env_rates))
# another way
class(env_rates$DGS10)
colnames(env_rates$DGS10)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/treas_10y_rate.png}\\
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
head(env_rates$DGS10, 3)
# get class of all objects in env_rates
eapply(env_rates, class)
# get class of all objects in R workspace
lapply(ls(), function(ob_ject) class(get(ob_ject)))
# plot 10-year constant maturity Treasury rate
chart_Series(env_rates$DGS10["1990/"], 
            name="10-year constant maturity Treasury rate")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Treasury Yield Curve}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{yield curve} is a vector of interest rates at different maturities, on a given date,
      \vskip1ex
      The \emph{yield curve} shape changes depending on the economic conditions: in recessions rates drop and the curve flattens, while in expansions rates rise and the curve steepens, 
      <<echo=TRUE,eval=FALSE>>=
# load constant maturity Treasury rates
load(file="C:/Develop/data/rates_data.RData")
# get end-of-year dates since 2006
date_s <- endpoints(env_rates$DGS1["2006/"], on="years")
date_s <- index(env_rates$DGS1["2006/"])[date_s]
# create time series of end-of-year rates
rate_s <- eapply(env_rates, function(ra_te) ra_te[date_s])
rate_s <- do.call(merge, rate_s)
# rename columns and rows, sort columns, and transpose into matrix
colnames(rate_s) <- substr(colnames(rate_s), start=4, stop=11)
rate_s <- rate_s[, order(as.numeric(colnames(rate_s)))]
colnames(rate_s) <- paste0(colnames(rate_s), "yr")
rate_s <- t(rate_s)
colnames(rate_s) <- substr(colnames(rate_s), start=1, stop=4)
# plot matrix using plot.zoo()
col_ors <- colorRampPalette(c("red", "blue"))(NCOL(rate_s))
plot.zoo(rate_s, main="Yield curve since 2006", lwd=3, xaxt="n", 
         plot.type="single", xlab="maturity", ylab="yield", col=col_ors)
# add x-axis
axis(1, seq_along(rownames(rate_s)), rownames(rate_s))
# add legend
legend("bottomright", legend=colnames(rate_s),
       col=col_ors, lty=1, lwd=4, inset=0.05, cex=0.8)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/yield_curve.png}\\
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# alternative plot using matplot()
matplot(rate_s, main="Yield curve since 2006", xaxt="n", lwd=3, lty=1, 
        type="l", xlab="maturity", ylab="yield", col=col_ors)
# add x-axis
axis(1, seq_along(rownames(rate_s)), rownames(rate_s))
# add legend
legend("bottomright", legend=colnames(rate_s),
       col=col_ors, lty=1, lwd=4, inset=0.05, cex=0.8)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Correlation Matrix of the Yield Curve}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The covariance matrix $\mathbf{V}$, of the data matrix $\mathbf{r}$, is given by:
      \begin{displaymath}
        \mathbf{V} = \frac{\mathbf{r}^T \, \mathbf{r}} {n-1}
      \end{displaymath}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# symbols for constant maturity Treasury rates
sym_bols <- c("DGS1", "DGS2", "DGS5", "DGS10", "DGS20")
# load constant maturity Treasury rates
load(file="C:/Develop/data/rates_data.RData")
# calculate daily percentage changes
rate_s <- na.omit(do.call(merge, 
    as.list(env_rates)[sym_bols]))
re_turns <- na.omit(diff(log(rate_s)))
# correlation matrix of Treasury rates
cor_mat <- cor(re_turns)
# reorder correlation matrix based on clusters
library(corrplot)
or_der <- corrMatOrder(cor_mat, 
              order="hclust", 
              hclust.method="complete")
cor_mat <- cor_mat[or_der, or_der]
# plot the correlation matrix
col_ors <- colorRampPalette(c("red", "white", "blue"))
corrplot(cor_mat, title="Correlation of Treasury rates", 
    tl.col="black", tl.cex=0.8, mar=c(0,0,1,0), 
    method="square", col=col_ors(8), 
    cl.offset=0.75, cl.cex=0.7, 
    cl.align.text="l", cl.ratio=0.25)
# draw rectangles on the correlation matrix plot
corrRect.hclust(cor_mat, k=NROW(cor_mat) %/% 2, 
                method="complete", col="red")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/cor_yield.png}\\
      \vspace{-1em}
            <<echo=TRUE,eval=FALSE>>=
# plot the correlation matrix
col_ors <- colorRampPalette(c("red", "white", "blue"))
corrplot(cor_mat, title="Correlation of Treasury rates", 
    tl.col="black", tl.cex=0.8, mar = c(0,0,1,0),
    method="square", col=col_ors(8), 
    cl.offset=0.75, cl.cex=0.7, 
    cl.align.text="l", cl.ratio=0.25)
# draw rectangles on the correlation matrix plot
corrRect.hclust(cor_mat, k=NROW(cor_mat) %/% 2, 
                method="complete", col="red")
      @

  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Principal Component Analysis of the Yield Curve}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{prcomp()} performs \emph{Principal Component Analysis} on a matrix of data, and returns the results as an object of class \texttt{prcomp}, 
      \vskip1ex
      The first few \emph{principal components} explain most of the volatility in the data, so \emph{PCA} is a form of \emph{dimensionality reduction}, 
      <<echo=TRUE,eval=FALSE>>=
# perform principal component analysis PCA
p_ca <- prcomp(re_turns, 
               center=TRUE, scale=TRUE)
# plot standard deviations
barplot(p_ca$sdev, 
        names.arg=colnames(p_ca$rotation), 
        las=3, xlab="", ylab="", 
        main="Volatilities of principal components 
        of Treasury rates")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth,valign=t]{figure/pca_std_dev_yield.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Yield Curve Principal Component Loadings (Weights)}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Principal component} loadings are the weights of portfolios which have mutually orthogonal returns,
      \vskip1ex
      The \emph{principal component} portfolios represent the different orthogonal modes of the data variance, 
      \vskip1ex
      The first \emph{principal component} of the \emph{yield curve} is the correlated movement of all rates up and down,
      \vskip1ex
      The second \emph{principal component} is \emph{yield curve} steepening and flattening,
      \vskip1ex
      The third \emph{principal component} is the \emph{yield curve} butterfly movement,
      <<echo=TRUE,eval=FALSE>>=
# principal component loadings (weights)
p_ca$rotation
# plot loading barplots in multiple panels
par(mfrow=c(3,2))
par(mar=c(2, 2, 2, 1), oma=c(0, 0, 0, 0))
for (or_der in 1:NCOL(p_ca$rotation)) {
  barplot(p_ca$rotation[, or_der], 
        las=3, xlab="", ylab="", main="")
  title(paste0("PC", or_der), line=-2.0, 
        col.main="red")
}  # end for
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/pca_loadings_yield.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Yield Curve Principal Component Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The time series of the \emph{principal components} can be calculated by multiplying the loadings (weights) times the original data,
      \vskip1ex
      Higher order \emph{principal components} are gradually less volatile,
      <<echo=TRUE,eval=FALSE>>=
# principal component time series
pca_ts <- xts(re_turns %*% p_ca$rotation, 
                order.by=index(re_turns))
pca_ts <- cumsum(pca_ts)
# plot principal component time series in multiple panels
par(mfrow=c(3,2))
par(mar=c(2, 2, 0, 1), oma=c(0, 0, 0, 0))
ra_nge <- range(pca_ts)
for (or_der in 1:NCOL(p_ca$rotation)) {
  plot.zoo(pca_ts[, or_der], 
           ylim=ra_nge, 
           xlab="", ylab="")
  title(paste0("PC", or_der), line=-2.0)
}  # end for
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/pca_series_yield.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Calibrating Yield Curve Using Package \protect\emph{RQuantLib}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package 
      \href{https://cran.r-project.org/web/packages/RQuantLib/index.html}{\emph{RQuantLib}} 
      is an interface to the 
      \href{http://quantlib.org/index.shtml}{\emph{QuantLib}}
      open source \texttt{C/C++} library for quantitative finance, mostly designed for pricing fixed-income instruments and options, 
      \vskip1ex
      The function \texttt{DiscountCurve()} calibrates a \emph{zero coupon yield curve} from \emph{money market} rates, \emph{Eurodollar} futures, and \emph{swap} rates, 
      \vskip1ex
      The function \texttt{DiscountCurve()} interpolates the \emph{zero coupon} rates into a vector of dates specified by the \texttt{times} argument, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(quantmod)  # load quantmod
library(RQuantLib)  # load RQuantLib
# specify curve parameters
curve_params <- list(tradeDate=as.Date("2017-01-17"),
                     settleDate=as.Date("2017-01-19"),
                     dt=0.25,
                     interpWhat="discount",
                     interpHow="loglinear")
# specify market data: prices of FI instruments
market_data <- list(d3m=0.0363,
                    fut1=96.2875,
                    fut2=96.7875,
                    fut3=96.9875,
                    fut4=96.6875,
                    s5y=0.0443,
                    s10y=0.05165,
                    s15y=0.055175)
# specify dates for calculating the zero rates
disc_dates <- seq(0, 10, 0.25)
# specify the evaluation (as of) date
setEvaluationDate(as.Date("2017-01-17"))
# calculate the zero rates
disc_curves <- DiscountCurve(params=curve_params, 
                             tsQuotes=market_data, 
                             times=disc_dates)
# plot the zero rates
x11()
plot(x=disc_curves$zerorates, t="l", main="zerorates")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Credit Portfolios}


%%%%%%%%%%%%%%%
\subsection{Simulating Single-period Defaults}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Consider a portfolio of credit assets (bonds or loans) over a single period of time,
      \vskip1ex
      At the end of the period, some of the assets default, while the rest don't,
      \vskip1ex
      The default probabilities are equal to $p_i$, 
      \vskip1ex
      Individual defaults can be simulated by comparing the probabilities $p_i$ with the uniform random numbers $u_i$, 
      \vskip1ex
      Default occurs if $u_i$ is less than the default probability $p_i$,
    \column{0.5\textwidth}
      <<echo=TRUE,eval=FALSE>>=
# calculate random default probabilities
num_assets <- 100
default_probs <- runif(num_assets, max=0.05)
# calculate number of defaults
uni_form <- runif(num_assets)
sum(uni_form < default_probs)
# simulate average number of defaults
num_simu <- 1000
de_faults <- numeric(num_simu)
for (i in 1:num_simu) {  # perform loop
  uni_form <- runif(num_assets)
  de_faults[i] <- sum(uni_form < default_probs)
}  # end for
mean(de_faults)
# average defaults using vectorized functions
uni_form <- matrix(runif(num_simu*num_assets), 
                   ncol=num_simu)
sum(uni_form < default_probs)/num_simu
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Asset Values and Default Thresholds}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Defaults can also be simulated using normally distributed variables $a_i$ called \emph{asset values}, instead of uniformly distributed variables,
      \vskip1ex
      Default occurs if the \emph{asset value} is less than the \emph{default threshold}  $t_i$: $a_i < t_i$, 
      \vskip1ex
      The default threshold is equal to $t_i = \Phi^{-1}(p_i)$, where $p_i$ is the default probability, and $\Phi()$ is the cumulative Standard Normal distribution, 
      \vskip1ex
      The asset values are mathematical variables, which can have negative values, so they are not related to actual company asset values, 
      <<echo=TRUE,eval=FALSE>>=
# plot Standard Normal distribution
x11(width=6, height=5)
curve(expr=dnorm(x),
      type="l", xlim=c(-4, 4),
      xlab="asset value", ylab="", lwd=2,
      col="blue", main="Distribution of Asset Values")
abline(v=qnorm(0.025), col="red", lwd=2)
text(x=qnorm(0.025)-0.1, y=0.15,
       labels="default threshold",
       lwd=2, srt=90, pos=3)
@
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/default_threshold.png}\\
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vasicek Model of Correlated Defaults}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Under the \emph{Vasicek} single factor model, the asset values $a_i$ are equal to the sum of a systematic factor $s$, plus an idiosyncratic factors $z_i$: 
      \begin{displaymath}
        a_i = \sqrt{\rho} s + \sqrt{1-\rho} z_i
      \end{displaymath}
      Where $\rho$ is the correlation between asset values, 
      \vskip1ex
      The variables $s$, $z_i$, and $a_i$ all follow the Standard Normal distribution $N(0, 1)$, 
      \vskip1ex
      The systematic factor $s$ may be considered to represent the state of the macro economy, with positive values representing an economic expansion, and negative values representing an economic recession, 
      \vskip1ex
      When the value of the systematic factor $s$ is positive, then the asset values will all tend to be bigger as well, which will produce fewer defaults, 
      \vskip1ex
      But when the systematic factor is negative, then the asset values will tend to be smaller, which will produce more defaults, 
      \vskip1ex
      This way the \emph{Vasicek} model introduces a correlation among defaults,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# define correlation parameters
rh_o <- 0.05
rho_sqrt <- sqrt(rh_o)
rho_sqrtm <- sqrt(1-rh_o)
# calculate default thresholds
default_thresh <- qnorm(default_probs)
# calculate vector of systematic factors
system_atic <- rnorm(num_simu)
# allocate vector of defaults
de_faults <- numeric(num_simu)
# perform loop to calculate de_faults
for (i in 1:num_simu) {
  asset_values <- 
    rho_sqrt*system_atic[i] + 
    rho_sqrtm*rnorm(num_assets)
  de_faults[i] <- 
    sum(asset_values < default_thresh)
}  # end for
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Portfolio Defaults Under Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If all the default probabilities are the same $p_i=p$, then the default threshold is equal to $t=N^{-1}(p)$, and the conditional default probability $p(s)$, given $s$, is equal to:
      \begin{displaymath}
        p(s) = N(\frac{t - \sqrt{\rho} s}{\sqrt{1-\rho}})
      \end{displaymath}
      The cumulative probability $P(x)$ for the percentage $x$ of portfolio defaults (the portfolio cumulative default distribution) is equal to:
      \begin{displaymath}
        P(x) = N(\frac{\sqrt{1-\rho} N^{-1}(x) - t}{\sqrt{\rho}})
      \end{displaymath}
      The associated probability density $f(x)$ of portfolio defaults is equal to:
      \begin{multline*}
        \hspace{-1.7em}f(x) = \frac{\sqrt{1-\rho}}{\sqrt{\rho}} \exp(-\frac{1}{2 \rho} (\sqrt{1-\rho} N^{-1}(x) - t)^2 + \\ \frac{1}{2} (N^{-1}(x))^2)
      \end{multline*}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/default_distr.png}\\
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# define default probability density function
vasi_cek <- function(x, def_thresh=-2, rh_o=0.08)
  sqrt((1-rh_o)/rh_o)*exp(-(sqrt(1-rh_o)*qnorm(x) - def_thresh)^2/(2*rh_o) + qnorm(x)^2/2)
vasi_cek(0.03, def_thresh=qnorm(0.025), rh_o=0.1)
# plot probability distribution of defaults
curve(expr=vasi_cek(x, def_thresh=qnorm(0.025), rh_o=0.02),
      type="l", xlim=c(0, 0.1), lwd=3,
      xlab="fraction of defaults", ylab="density", 
      col="green", main="Distribution of defaults")
# plot default distribution with higher correlation
curve(expr=vasi_cek(x, def_thresh=qnorm(0.025), rh_o=0.08),
      type="l", xlim=c(0, 0.1), add=TRUE,
      xlab="default fraction", ylab="", lwd=3,
      col="blue", main="")
# add legend
legend(x="topright", legend=c("high correlation", "low correlation"),
       title=NULL, inset=0.05, cex=0.8, bg="white",
       lwd=6, lty=c(1, 1), col=c("blue", "green"))
# add unconditional default probability
abline(v=0.025, col="red", lwd=3)
text(x=0.023, y=8,
       labels="default probability",
       lwd=2, srt=90, pos=3)
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Defaults Under Extreme Correlations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the correlation $\rho$ is close to \emph{zero}, then the asset values $a_i$ are independent from each other, and defaults are also independent, so that the fraction of portfolio defaults is very close to the default probability $p$,
      \vskip1ex
      In that case, the probability density of portfolio defaults is very narrow and is centered on the default probability $p$,
      \vskip1ex
      If the correlation $\rho$ is close to \emph{one}, then the asset values $a_i$ are almost the same, and defaults occur at the same time, so that the fraction of portfolio defaults is either \emph{zero} or \emph{one},
      \vskip1ex
      In that case, the probability density of portfolio defaults becomes \emph{bimodal}, with two peaks around  \emph{zero} and \emph{one},
      <<echo=TRUE,eval=FALSE>>=
# plot default distribution with low correlation
curve(expr=vasi_cek(x, def_thresh=qnorm(0.1), rh_o=0.01),
      type="l", xlab="default fraction", ylab="", 
      lwd=2, col="green", main="Distribution of defaults")
# plot default distribution with high correlation
curve(expr=vasi_cek(x, def_thresh=qnorm(0.1), rh_o=0.99),
      type="l", add=TRUE, lwd=2, n=10001, 
      xlab="fraction of defaults", ylab="density", 
      col="blue", main="")
@
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/default_distr_corr.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# add legend
legend(x="topright", legend=c("high correlation", "low correlation"),
       title=NULL, inset=0.1, cex=0.8, bg="white",
       lwd=6, lty=c(1, 1), col=c("blue", "green"))
# add unconditional default probability
abline(v=0.1, col="red", lwd=2)
text(x=0.1, y=10,
       labels="default probability",
       lwd=2, pos=4)
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Loss Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Expected Loss (\emph{EL}) of a credit portfolio is equal to the sum of default probabilities ($p_i$) multiplied by the loss given default (\emph{LGD}, also known as the loss severity): 
      \begin{displaymath}
        EL = \sum_{i=1}^{n} p_i LGD_i
      \end{displaymath}
      If the \emph{LGD} amounts are all the same, then the \emph{portfolio loss distribution} can be obtained from the \emph{default distribution}, adjusted for the \emph{LGD}:
      \begin{multline*}
        \hspace{-1.7em}f(x) = \frac{\sqrt{1-\rho}}{LGD \sqrt{\rho}} \exp(-\frac{1}{2 \rho} (\sqrt{1-\rho} N^{-1}(\frac{x}{LGD}) - t)^2 + \\ \frac{1}{2} (N^{-1}(\frac{x}{LGD}))^2)
      \end{multline*}
      <<echo=TRUE,eval=FALSE>>=
# define Vasicek loss distribution density function
portf_loss <- function(x, def_thresh=-2, rh_o=0.08, l_gd=0.4)
  sqrt((1-rh_o)/rh_o)*exp(-(sqrt(1-rh_o)*qnorm(x/l_gd) - def_thresh)^2/(2*rh_o) + qnorm(x/l_gd)^2/2)/l_gd
integrate(portf_loss, low=0, up=0.3, def_thresh=-2, rh_o=0.08, l_gd=0.4)
@
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/loss_distr.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# plot probability distribution of losses
curve(expr=portf_loss(x, def_thresh=qnorm(0.05), rh_o=0.08),
      type="l", xlim=c(0, 0.06), 
      xlab="loss fraction", ylab="density", lwd=3,
      col="orange", main="Distribution of Losses")
# add line for expected loss
abline(v=0.02, col="red", lwd=3)
text(x=0.02-0.001, y=10, labels="expected loss",
       lwd=2, srt=90, pos=3)
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Value at Risk}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A loss exceeding the \emph{EL} is called the Unexpected Loss (\emph{UL}), and can be calculated from the \emph{portfolio loss distribution}, 
      \vskip1ex
      Value at Risk (\emph{VaR}) measures extreme portfolio loss (but not the worst possible loss), defined as the quantile of the loss distribution, corresponding to a given confidence level $\alpha$, 
      <<echo=TRUE,eval=FALSE>>=
# add lines for unexpected loss
abline(v=0.04, col="blue", lwd=3)
arrows(x0=0.02, y0=35, x1=0.04, y1=35, 
       code=3, lwd=3, cex=0.5)
text(x=0.03, y=36, labels="unexpected loss", 
     lwd=2, pos=3)
# add lines for VaR
abline(v=0.055, col="red", lwd=3)
arrows(x0=0.0, y0=25, x1=0.055, y1=25, 
       code=3, lwd=3, cex=0.5)
text(x=0.03, y=26, labels="VaR", lwd=2, pos=3)
text(x=0.055-0.001, y=10, labels="VaR",
       lwd=2, srt=90, pos=3)
@
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/loss_distr_var.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Conditional Value at Risk}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Conditional Value at Risk (\emph{CVaR}) is equal to the average of the \emph{VaR} for confidence levels less than a given confidence level $\alpha$: 
      \begin{displaymath}
        \mathrm{CVaR} = \frac{1}{\alpha} \int_{0}^\alpha \mathrm{VaR}(p) \, \mathrm{d}p
      \end{displaymath}
      The Conditional Value at Risk is also called the Expected Shortfall (\emph{ES}), or Expected Tail Loss (\emph{ETL}), 
      <<echo=TRUE,eval=FALSE>>=
# plot probability distribution of losses
curve(expr=portf_loss(x, def_thresh=qnorm(0.05), rh_o=0.08),
      type="l", xlim=c(0, 0.06), 
      xlab="loss fraction", ylab="density", lwd=3,
      col="orange", main="Conditional Value at Risk")
# add line for expected loss
abline(v=0.02, col="red", lwd=3)
text(x=0.02-0.001, y=10, labels="expected loss",
       lwd=2, srt=90, pos=3)
# add lines for VaR
abline(v=0.04, col="red", lwd=3)
text(x=0.04-0.001, y=10, labels="VaR",
       lwd=2, srt=90, pos=3)
@
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/loss_distr_cvar.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# add shading for CVaR
v_ar <- 0.04; var_max <- 0.07
var_s <- seq(v_ar, var_max, length=100)
dens_ity <- sapply(var_s, 
  portf_loss, def_thresh=qnorm(0.05), rh_o=0.08)
# draw shaded polygon
polygon(c(v_ar, var_s, var_max),
        c(-1, dens_ity, -1), col="red", border=NA)
text(x=0.045, y=0, labels="CVaR", lwd=2, pos=3)
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Value at Risk Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The confidence levels of \emph{VaR} values can be calculated by integrating over the tail of the loss density function, 
      <<echo=TRUE,eval=FALSE>>=
# integrate portf_loss() over full range
integrate(portf_loss, low=0.0, up=0.3, 
          def_thresh=qnorm(0.05), rh_o=0.08, l_gd=0.4)
# calculate expected losses using portf_loss()
integrate(function(x, ...) x*portf_loss(x, ...), 
          low=0.0, up=0.3, 
          def_thresh=qnorm(0.05), rh_o=0.08, l_gd=0.4)
# calculate confidence levels corresponding to VaR values
var_s <- seq(0.04, 0.06, 0.001)
conf_levels <- sapply(var_s, function(v_ar, ...) {
  integrate(portf_loss, low=v_ar, up=0.3, ...)
}, def_thresh=qnorm(0.05), rh_o=0.08, l_gd=0.4)  # end sapply
conf_levels <- cbind(as.numeric(t(conf_levels)[, 1]), var_s)
colnames(conf_levels) <- c("conf_levels", "VaRs")
# calculate 95% confidence level VaR value
conf_levels[
  match(TRUE, conf_levels[, "conf_levels"] < 0.05), 
  "VaRs"]
plot(x=1-conf_levels[, "conf_levels"],
     y=conf_levels[, "VaRs"], 
     xlab="conf_levels", ylab="VaRs",
     t="l", main="VaR values and confidence levels")
@
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/var_conf_levels.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Conditional Value at Risk Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{CVaR} values can be calculated by integrating over the tail of the loss density function, 
      <<echo=TRUE,eval=FALSE>>=
# calculate CVaR values
cvar_s <- sapply(var_s, function(v_ar, ...) {
  integrate(function(x, ...) x*portf_loss(x, ...), 
            low=v_ar, up=0.3, ...)
}, def_thresh=qnorm(0.05), rh_o=0.08, l_gd=0.4)  # end sapply
conf_levels <- cbind(conf_levels, as.numeric(t(cvar_s)[, 1]))
colnames(conf_levels)[3] <- "CVaRs"
# divide CVaR by confidence level
conf_levels[, "CVaRs"] <- conf_levels[, "CVaRs"]/conf_levels[, "conf_levels"]
# calculate 95% confidence level CVaR value
conf_levels[
  match(TRUE, conf_levels[, "conf_levels"] < 0.05), 
  "CVaRs"]
# plot CVaRs
plot(x=1-conf_levels[, "conf_levels"],
     y=conf_levels[, "CVaRs"], 
     t="l", col="red", lwd=2, 
     ylim=range(conf_levels[, c("VaRs", "CVaRs")]), 
     xlab="conf_levels", ylab="CVaRs",
     main="CVaR values and confidence levels")
@
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/cvar_conf_levels.png}\\
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# add VaRs
lines(x=1-conf_levels[, "conf_levels"],
      y=conf_levels[, "VaRs"], lwd=2)
# add legend
legend(x="topleft", legend=c("CVaRs", "VaRs"),
       title=NULL, inset=0.05, cex=0.8, bg="white",
       lwd=6, lty=c(1, 1), col=c("red", "black"))
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating \protect\emph{VaR} Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the default probabilities $p_i$ are not all the same, then there's no formula for the \emph{portfolio loss distribution} under the Vasicek Model, 
      \vskip1ex
      In that case the portfolio losses and \emph{VaR} must be simulated,
      <<echo=TRUE,eval=FALSE>>=
# Define model parameters
num_assets <- 300
num_simu <- 1000
l_gd <- 0.4
# define correlation parameters
rh_o <- 0.08
rho_sqrt <- sqrt(rh_o)
rho_sqrtm <- sqrt(1-rh_o)
# calculate default probabilities and thresholds
set.seed(1121)
default_probs <- runif(num_assets, max=0.1)
default_thresh <- qnorm(default_probs)
# calculate vector of systematic factors
system_atic <- rnorm(num_simu)
# simulate losses under Vasicek model
asset_values <- matrix(rnorm(num_simu*num_assets), ncol=num_simu)
asset_values <- t(rho_sqrt*system_atic + t(rho_sqrtm*asset_values))
loss_es <- 
  l_gd*colSums(asset_values < default_thresh)/num_assets
@
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/var_simu.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# calculate VaRs
conf_levels <- seq(0.93, 0.99, 0.01)
var_s <- quantile(loss_es, probs=conf_levels)
names(var_s) <- round(conf_levels, 3)
plot(x=conf_levels, y=var_s, t="l",
     main="Simulated VaR and confidence levels")
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating \protect\emph{CVaR} Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{CVaR} can be calculated from the frequency of tail losses in excess of the \emph{VaR}, 
      \vskip1ex
      The function \texttt{table()} calculates the frequency distribution of categorical data,
      <<echo=TRUE,eval=FALSE>>=
# calculate frequency table of loss_es
table_losses <- table(loss_es)/num_simu
# calculate CVaRs
cvar_s <- sapply(var_s, function(v_ar) {
  tai_l <- table_losses[v_ar < names(table_losses)]
  tai_l %*% as.numeric(names(tai_l)) / sum(tai_l)
})  # end sapply
cvar_s <- cbind(cvar_s, var_s)
# plot CVaRs
plot(x=rownames(cvar_s), y=cvar_s[, "cvar_s"], 
     t="l", col="red", lwd=2, 
     ylim=range(cvar_s), 
     xlab="conf_levels", ylab="CVaRs",
     main="Simulated CVaR and confidence levels")
# add VaRs
lines(x=rownames(cvar_s), y=cvar_s[, "var_s"], lwd=2)
# add legend
legend(x="topleft", legend=c("CVaRs", "VaRs"),
       title=NULL, inset=0.05, cex=0.8, bg="white",
       lwd=6, lty=c(1, 1), col=c("red", "black"))
@
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/cvar_simu.png}\\
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Function for Simulating \protect\emph{VaR} Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The function \texttt{calc\_var()} simulates default losses under the \emph{Vasicek} model, and calculates a vector of \emph{VaR} and \emph{CVaR} values, 
      \vskip1ex
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
calc_var <- function(default_probs, 
                     l_gd=0.6, 
                     rh_o=0.08, 
                     num_simu=1000,
                     conf_levels=seq(0.93, 0.99, 0.01)) {
  # Define model parameters
  num_assets <- NROW(default_probs)
  default_thresh <- qnorm(default_probs)
  # Define correlation parameters
  rho_sqrt <- sqrt(rh_o)
  rho_sqrtm <- sqrt(1-rh_o)
  # Simulate losses under Vasicek model
  system_atic <- rnorm(num_simu)
  asset_values <- matrix(rnorm(num_simu*num_assets), ncol=num_simu)
  asset_values <- t(rho_sqrt*system_atic + t(rho_sqrtm*asset_values))
  loss_es <- l_gd*colSums(asset_values < default_thresh)/num_assets
  # Calculate VaRs
  var_s <- quantile(loss_es, probs=conf_levels)
  names(var_s) <- round(conf_levels, 3)
  # Calculate CVaRs from the frequency table 
  table_losses <- table(loss_es)/num_simu
  cvar_s <- sapply(var_s, function(v_ar) {
    tai_l <- table_losses[v_ar < names(table_losses)]
    tai_l %*% as.numeric(names(tai_l)) / sum(tai_l)
  })  # end sapply
  names(cvar_s) <- round(conf_levels, 3)
  c(var_s, cvar_s)
}  # end calc_var
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of \protect\emph{VaR} Using Bootstrap Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard errors \emph{VaR} and \emph{CVaR} can be calculated by performing an \texttt{sapply()} loop, selecting random samples from \texttt{default\_probs}, and passing them into \texttt{calc\_var()},
      <<echo=TRUE,eval=FALSE>>=
# define number of bootstrap simulations
num_boot <- 500
num_assets <- NROW(default_probs)
# perform bootstrap of calc_var
set.seed(1121)
boot_strap <- sapply(rep(l_gd, num_boot), 
  calc_var, 
  default_probs=
    default_probs[sample.int(num_assets, replace=TRUE)],
  rh_o=rh_o, num_simu=num_simu,
  conf_levels=conf_levels)  # end sapply
boot_strap <- t(boot_strap)
# calculate vectors of standard errors of VaR and CVaR from boot_strap data
std_error_var <- apply(boot_strap[, 1:7], MARGIN=2, 
    function(x) c(mean=mean(x), sd=sd(x)))
std_error_cvar <- apply(boot_strap[, 8:14], MARGIN=2, 
    function(x) c(mean=mean(x), sd=sd(x)))
# scale the standard errors of VaRs and CVaRs
std_error_var[2, ] <- std_error_var[2, ]/std_error_var[1, ]
std_error_cvar[2, ] <- std_error_cvar[2, ]/std_error_cvar[1, ]
@
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/cvar_std_error_boot.png}\\
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot the standard errors of VaRs and CVaRs
plot(x=colnames(std_error_cvar),
  y=std_error_cvar[2, ], t="l", col="red", lwd=2, 
  ylim=range(c(std_error_var[2, ], std_error_cvar[2, ])), 
  xlab="conf_levels", ylab="CVaRs",
  main="Scaled standard errors of CVaR and VaR")
lines(x=colnames(std_error_var), y=std_error_var[2, ], lwd=2)
legend(x="topleft", legend=c("CVaRs", "VaRs"),
       title=NULL, inset=0.05, cex=0.8, bg="white",
       lwd=6, lty=c(1, 1), col=c("red", "black"))
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of \protect\emph{VaR} Using Parallel Bootstrap}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # load package parallel
num_cores <- detectCores() - 1  # number of cores
clus_ter <- makeCluster(num_cores)  # initialize compute cluster
# perform bootstrap of calc_var for Windows
set.seed(1121)
boot_strap <- parLapply(clus_ter, rep(l_gd, num_boot), 
  fun=calc_var, 
  default_probs=default_probs[sample.int(num_assets, replace=TRUE)],
  rh_o=rh_o, num_simu=num_simu,
  conf_levels=conf_levels)  # end parLapply
# bootstrap under Mac-OSX or Linux
boot_strap <- mclapply(rep(l_gd, num_boot), 
  fun=calc_var, 
  default_probs=default_probs[sample.int(num_assets, replace=TRUE)],
  rh_o=rh_o, num_simu=num_simu,
  conf_levels=conf_levels)  # end mclapply
boot_strap <- do.call(rbind, boot_strap)
stopCluster(clus_ter)  # stop R processes over cluster
# calculate vectors of standard errors of VaR and CVaR from boot_strap data
std_error_var <- apply(boot_strap[, 1:7], MARGIN=2, 
    function(x) c(mean=mean(x), sd=sd(x)))
std_error_cvar <- apply(boot_strap[, 8:14], MARGIN=2, 
    function(x) c(mean=mean(x), sd=sd(x)))
# scale the standard errors of VaRs and CVaRs
std_error_var[2, ] <- std_error_var[2, ]/std_error_var[1, ]
std_error_cvar[2, ] <- std_error_cvar[2, ]/std_error_cvar[1, ]
@
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/cvar_std_error_parallel.png}\\
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot the standard errors of VaRs and CVaRs
plot(x=colnames(std_error_cvar),
  y=std_error_cvar[2, ], t="l", col="red", lwd=2, 
  ylim=range(c(std_error_var[2, ], std_error_cvar[2, ])), 
  xlab="conf_levels", ylab="CVaRs",
  main="Scaled standard errors of CVaR and VaR")
lines(x=colnames(std_error_var), y=std_error_var[2, ], lwd=2)
legend(x="topleft", legend=c("CVaRs", "VaRs"),
       title=NULL, inset=0.05, cex=0.8, bg="white",
       lwd=6, lty=c(1, 1), col=c("red", "black"))
@
  \end{columns}
\end{block}

\end{frame}


\end{document}
