% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='scriptsize', fig.width=4, fig.height=4)
options(digits=3)
options(width=60, dev='pdf')
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
% \usepackage{mathtools}
\usepackage[latin1]{inputenc}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}



% Title page setup
\title[Time Series Multivariate]{Time Series Multivariate}
\subtitle{FRE6871 \& FRE7241, Spring 2018}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{\today}



%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle


%%%%%%%%%%%%%%%
\section{Asset Pricing Models}


%%%%%%%%%%%%%%%
\subsection{Linear Regression of Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The returns of \emph{XLP} and \emph{VTI} are highly correlated because they are driven by common market factors of returns, 
      \vskip1ex
      The \emph{t}-statistic (\emph{t}-value) is the ratio of the estimated value divided by its standard error,
      \vskip1ex
      The \emph{p}-value is the probability of obtaining the observed value of the \emph{t}-statistic, or more extreme values,
      <<echo=(-(1:1)),eval=TRUE>>=
library(HighFreq)
# specify formula and perform regression
reg_formula <- XLP ~ VTI
reg_model <- lm(reg_formula, 
                data=rutils::env_etf$re_turns)
# get regression coefficients
coef(summary(reg_model))
# Durbin-Watson test of autocorrelation of residuals
lmtest::dwtest(reg_model)
      @
    \column{0.5\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/reg_rets.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# plot scatterplot of returns with aspect ratio 1
plot(reg_formula, data=rutils::env_etf$re_turns,
     xlim=c(-0.1, 0.1), ylim=c(-0.1, 0.1), 
     asp=1, main="Regression XLP ~ VTI")
# add regression line and perpendicular line
abline(reg_model, lwd=2, col="red")
abline(a=0, b=-1/coef(summary(reg_model))[2, 1], 
       lwd=2, col="blue")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Linear Regression Summary Statistics}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.55\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(HighFreq)  # load HighFreq
re_turns <- na.omit(rutils::env_etf$re_turns)
# perform regressions and collect statistics
etf_reg_stats <- sapply(colnames(re_turns)[-1], 
                        function(etf_name) {
# specify regression formula
  reg_formula <- as.formula(
    paste(etf_name, "~ VTI"))
# perform regression
  reg_model <- lm(reg_formula, data=re_turns)
# get regression summary
  reg_model_sum <- summary(reg_model)
# collect regression statistics
  etf_reg_stats <- with(reg_model_sum, 
    c(alpha=coefficients[1, 1], 
      p_alpha=coefficients[1, 4], 
      beta=coefficients[2, 1], 
      p_beta=coefficients[2, 4]))
  etf_reg_stats <- c(etf_reg_stats, 
               p_dw=lmtest::dwtest(reg_model)$p.value)
  etf_reg_stats
})  # end sapply
etf_reg_stats <- t(etf_reg_stats)
# sort by p_alpha
etf_reg_stats <- etf_reg_stats[
  order(etf_reg_stats[, "p_alpha"]), ]
      @
    \column{0.45\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
etf_reg_stats[, 1:3]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Beta Regressions Over Time}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{rollapply()} allows performing regressions over a rolling window, 
      \vskip1ex
      The function \texttt{roll\_lm()} from package \emph{roll} performs rolling regressions in \texttt{C++}, in parallel, and is therefore much faster than function \texttt{rollapply()}, 
      <<echo=TRUE,eval=FALSE>>=
library(HighFreq)
# specify regression formula
reg_formula <- XLP ~ VTI
# perform rolling beta regressions every month
beta_s <- rollapply(rutils::env_etf$re_turns, width=252, 
  FUN=function(de_sign) 
  coef(lm(reg_formula, data=de_sign))[2],
  by=22, by.column=FALSE, align="right")
beta_s <- na.omit(beta_s)
# plot beta_s in x11() window
x11(width=(wid_th <- 6), height=(hei_ght <- 4))
chart_Series(x=beta_s[, "VTI"], 
  name=paste("rolling betas", format(reg_formula)))
# perform daily rolling beta regressions in parallel
library(roll)
beta_s <- roll_lm(x=rutils::env_etf$re_turns[, "VTI"], 
                  y=rutils::env_etf$re_turns[, "XLP"],
                  width=252)$coefficients
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/rolling_betas.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# compare speed of rollapply() versus roll_lm()
library(microbenchmark)
da_ta <- rutils::env_etf$re_turns["2012", c("VTI", "XLP")]
summary(microbenchmark(
  rollapply=rollapply(da_ta, width=22, 
      FUN=function(de_sign) 
      coef(lm(reg_formula, data=de_sign))[2],
        by.column=FALSE, align="right"), 
  roll_lm=roll_lm(x=da_ta[, "VTI"], 
                  y=da_ta[, "XLP"],
                  width=22)$coefficients, 
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Capital Asset Pricing Model (\protect\emph{CAPM})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Capital Asset Pricing Model} decomposes asset returns into \emph{systematic} returns (proportional to the market returns) and \emph{idiosyncratic} returns (uncorrelated to market returns):
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + {\varepsilon}
      \end{displaymath}
      Where $R_m$ are the market returns, and $R_f$ are the risk-free returns,
      \vskip1ex
      The \emph{systematic} risk and returns are proportional to $\beta$, 
      \vskip1ex
      $\beta$ can be obtained from linear regression, and is proportional to the correlation of returns between the asset and the market:
      \begin{displaymath}
        \beta = \frac{\sum_{i=1}^n (R_i-\bar{R}) (R_{i,m}-\bar{R_m})} {\sum_{i=1}^n (R_{i,m}-\bar{R_m})^2}
      \end{displaymath}
      The \emph{CAPM} model states that if an asset has higher $\beta$ risk, then it should earn higher \emph{systematic} returns,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(PerformanceAnalytics)
CAPM.beta(Ra=re_turns[, "XLP"], 
          Rb=re_turns[, "VTI"])
CAPM.beta.bull(Ra=re_turns[, "XLP"], 
  Rb=re_turns[, "VTI"])
CAPM.beta.bear(Ra=re_turns[, "XLP"], 
  Rb=re_turns[, "VTI"])
CAPM.alpha(Ra=re_turns[, "XLP"], 
           Rb=re_turns[, "VTI"])
      @
      The \emph{idiosyncratic} returns are equal to the sum of $\alpha$ plus $\varepsilon$,
      \vskip1ex
      \emph{Alpha} ($\alpha$) are the returns in excess of \emph{systematic} returns, that can be attributed to portfolio selection or active manager performance,
      \vskip1ex
      The \emph{idiosyncratic} risk (equal to $\varepsilon$) is uncorrelated to the \emph{systematic} risk, and can be reduced through portfolio diversification,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Security Market Line}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      According to the \emph{CAPM} model, assets should earn a \emph{systematic} return proportional to their \emph{systematic} risk ($\beta$),
      \vskip1ex
      The \emph{Security Market Line} (SML) represents the linear relationship between \emph{systematic} risk ($\beta$) and return, for different stocks, 
      <<capm_scatter,echo=(-1),eval=FALSE,fig.width=5,fig.height=5,fig.show='hide'>>=
library(PerformanceAnalytics)
etf_betas <- sapply(
  re_turns[, colnames(re_turns)!="VXX"], 
  CAPM.beta, Rb=re_turns[, "VTI"])
etf_annrets <- sapply(
  re_turns[, colnames(re_turns)!="VXX"], 
  Return.annualized)
# plot scatterplot
plot(etf_annrets ~ etf_betas, xlab="betas", 
            ylab="ann. rets", xlim=c(-0.25, 1.6))
points(x=1, y=etf_annrets["VTI"], col="red", 
       lwd=3, pch=21)
abline(a=0, b=etf_annrets["VTI"])
label_names <- rownames(etf_reg_stats)[1:13]
# add labels
text(x=1, y=etf_annrets["VTI"], labels="VTI", 
     pos=2)
text(x=etf_betas[label_names], 
     y=etf_annrets[label_names], 
     labels=label_names, pos=2, cex=0.8)
      @
    \column{0.5\textwidth}
    \vspace{-3em}
      \includegraphics[width=0.5\paperwidth]{figure/capm_scatter-1}\\
    \vspace{-2em}
      A scatterplot of asset returns versus their $\beta$ shows which assets earn a positive $\alpha$, and which don't,
      \vskip1ex
      If an asset lies on the \emph{SML}, then its returns are mostly \emph{systematic}, and its $\alpha$ is equal to zero,
      \vskip1ex
      Assets above the \emph{SML} have a positive $\alpha$), and those below have a negative $\alpha$),
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk-adjusted Performance Measurement}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Treynor} ratio measures the excess returns per unit of \emph{systematic} risk ($\beta$), and is equal to the excess returns (over a risk-free return) divided by the $\beta$:
      \begin{displaymath}
        T_r=\frac{E[R-R_f]}{\beta}
      \end{displaymath}
      The \emph{Treynor} ratio is similar to the \emph{Sharpe} ratio, with the difference that its denominator represents only \emph{systematic} risk, not total risk,
      \vskip1ex
      The \emph{Information} ratio is equal to the excess returns (over a benchmark) divided by the \emph{tracking error} (standard deviation of excess returns):
      \begin{displaymath}
        I_r = \frac{E[R-R_b]} {\sqrt{\sum_{i=1}^n (R_i-R_{i,b})^2}}
      \end{displaymath}
      The \emph{Information} ratio measures the amount of outperformance versus the benchmark, and the consistency of outperformance,
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(PerformanceAnalytics)
TreynorRatio(Ra=re_turns[, "XLP"], 
           Rb=re_turns[, "VTI"])

InformationRatio(Ra=re_turns[, "XLP"], 
           Rb=re_turns[, "VTI"])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{CAPM} Summary Statistics}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.55\textwidth}
      \vspace{-1em}
      <<echo=(-1),eval=FALSE>>=
library(PerformanceAnalytics)
table.CAPM(Ra=re_turns[, c("XLP", "XLF")], 
           Rb=re_turns[, "VTI"], scale=252)
      @
      \vspace{-2em}
      <<eval=FALSE,echo=(-1)>>=
library(PerformanceAnalytics)
capm_stats <- table.CAPM(Ra=re_turns[, colnames(re_turns)!="VTI"], 
              Rb=re_turns[, "VTI"], scale=252)
colnames(capm_stats) <- 
  sapply(colnames(capm_stats), 
  function (str) {strsplit(str, split=" ")[[1]][1]})
capm_stats <- as.matrix(capm_stats)
capm_stats <- t(capm_stats)
capm_stats <- capm_stats[
  order(capm_stats[, "Annualized Alpha"], 
        decreasing=TRUE), ]
# copy capm_stats into env_etf and save to .RData file
assign("capm_stats", capm_stats, envir=env_etf)
save(env_etf, file='etf_data.RData')
      @
    \column{0.45\textwidth}
      \vspace{-1em}
      <<eval=FALSE,echo=TRUE>>=
capm_stats[, c("Information Ratio", "Annualized Alpha")]
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Principal Component Analysis}


%%%%%%%%%%%%%%%
\subsection{draft: Principal Components for two assets}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The scaled returns of \emph{XLP} and \emph{VTI} can be expressed as linear combinations of two orthogonal principal components: \\
      The first principal component can be 
      returns of \emph{XLP} and \emph{VTI} are highly correlated because they both share a common factor of market returns, 
      <<echo=TRUE,eval=FALSE>>=
re_turns <- 
  scale(na.omit(rutils::env_etf$re_turns
                [, as.character(reg_formula)[-1]]))
(t(re_turns) %*% re_turns) / NROW(re_turns)
w_1 <- sqrt(0.5); w_2 <- w_1
foo <- matrix(c(w_1, w_2, -w_2, w_1), nc=2)
t(foo) %*% foo
# bar <- re_turns %*% t(solve(foo))
(t(bar) %*% bar) / NROW(bar)

cov_mat <- function(re_turns, an_gle=0) {
  w_1 <- cos(an_gle)
  w_2 <- sin(an_gle)
  mat_rix <- matrix(c(w_1, -w_2, w_2, w_1), nc=2)
  compo_nents <- re_turns %*% t(mat_rix)
  (t(compo_nents) %*% compo_nents) / NROW(compo_nents)
}  # end cov_mat

bar <- cov_mat(re_turns, an_gle=pi/4)
(t(re_turns) %*% re_turns) / NROW(re_turns)
(t(bar) %*% bar) / NROW(bar)

angle_s <- seq(0, pi/2, by=pi/24)
cov_mat <- sapply(angle_s, function(an_gle) 
  cov_mat(re_turns, an_gle=an_gle)[1, 1])
plot(x=angle_s, y=cov_mat, t="l")

op_tim <- optimize(
  f=function(an_gle) 
    -cov_mat(re_turns, an_gle=an_gle)[1, 1], 
  interval=range(angle_s))
an_gle <- op_tim$minimum
bar <- cov_mat(re_turns, an_gle=an_gle)
tan(an_gle)

w_1 <- cos(an_gle)
w_2 <- sin(an_gle)
mat_rix <- matrix(c(w_1, -w_2, w_2, w_1), nc=2)
compo_nents <- re_turns %*% t(mat_rix)
(t(compo_nents) %*% compo_nents) / NROW(compo_nents)

plot(x=compo_nents[, 1], y=compo_nents[, 2],
     xlim=c(-10, 10), ylim=c(-10, 10))

reg_model <- lm(reg_formula, data=re_turns)
# get regression coefficients
coef(summary(reg_model))

foo <- cbind(rnorm(1000, sd=0.2), rnorm(1000)) %*% t(mat_rix)
(t(foo) %*% foo) / NROW(foo)
plot(x=foo[, 1], y=foo[, 2])
summary(lm(foo[, 1] ~ foo[, 2]))

op_tim <- optimize(
  f=function(an_gle) 
    -cov_mat(foo, an_gle=an_gle)[1, 1], 
  interval=range(angle_s))
an_gle <- op_tim$minimum
tan(an_gle)

###

w_1 <- cos(0.5)
s_1 <- 1
s_2 <- 2
foo <- matrix(c(s_1^2, s_1*s_2*w_1, s_1*s_2*w_1, s_2^2), nc=2)
eigen(foo)

# Durbin-Watson test of autocorrelation of residuals
lmtest::dwtest(reg_model)
      @
    \column{0.5\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/returns_regress-1}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# plot scatterplot of returns
plot(reg_formula, data=rutils::env_etf$re_turns,
     main="Regression XLP ~ VTI")
# add regression line
abline(reg_model, lwd=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Covariance Matrix of ETF Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The covariance matrix $\mathbb{C}$, of the return matrix $r$, is given by:
      \begin{displaymath}
        \mathbb{C} = \frac{r^T r} {n-1}
      \end{displaymath}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(HighFreq)
# Select ETF symbols
sym_bols <- c("IEF", "DBC", "XLU", "XLF", "XLP", "XLI")
# calculate ETF prices and simple returns (not percentage)
price_s <- rutils::env_etf$price_s[, sym_bols]
price_s <- zoo::na.locf(price_s)
price_s <- zoo::na.locf(price_s, fromLast=TRUE)
date_s <- index(price_s)
re_turns <- rutils::diff_it(price_s)
# de-mean (center) and scale the returns
re_turns <- t(t(re_turns) - colMeans(re_turns))
re_turns <- t(t(re_turns) / sqrt(colSums(re_turns^2)/(NROW(re_turns)-1)))
re_turns <- xts(re_turns, date_s)
# alternative de-mean (center) and scale the returns
# re_turns <- scale(re_turns, center=TRUE, scale=TRUE)
# re_turns <- xts(re_turns, date_s)
# or
# re_turns <- lapply(re_turns, function(x) {x - sum(x)/NROW(re_turns)})
# re_turns <- rutils::do_call(cbind, re_turns)
# re_turns <- apply(re_turns, 2, scale)
# covariance matrix and variance vector of returns
cov_mat <- cov(re_turns)
vari_ance <- diag(cov_mat)
cor_mat <- cor(re_turns)
# cov_mat <- (t(re_turns) %*% re_turns) / (NROW(re_turns)-1)
# cor_mat <- cov_mat / sqrt(vari_ance)
# cor_mat <- t(t(cor_mat) / sqrt(vari_ance))
# reorder correlation matrix based on clusters
library(corrplot)
or_der <- corrMatOrder(cor_mat, 
              order="hclust", 
              hclust.method="complete")
cor_mat <- cor_mat[or_der, or_der]
# plot the correlation matrix
col_ors <- colorRampPalette(c("red", "white", "blue"))
corrplot(cor_mat, title="ETF Correlation Matrix", 
    tl.col="black", tl.cex=0.8, mar=c(0,0,1,0), 
    method="square", col=col_ors(8), 
    cl.offset=0.75, cl.cex=0.7, 
    cl.align.text="l", cl.ratio=0.25)
# draw rectangles on the correlation matrix plot
corrRect.hclust(cor_mat, k=NROW(cor_mat) %/% 2, 
                method="complete", col="red")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/corr_etf.png}
      \vspace{-1em}
            <<echo=TRUE,eval=FALSE>>=
# plot the correlation matrix
col_ors <- colorRampPalette(c("red", "white", "blue"))
corrplot(cor_mat, title="Correlation Matrix", 
    tl.col="black", tl.cex=0.8, mar = c(0,0,1,0),
    method="square", col=col_ors(NCOL(cor_mat)), 
    cl.offset=0.75, cl.cex=0.7, 
    cl.align.text="l", cl.ratio=0.25)
# draw rectangles on the correlation matrix plot
corrRect.hclust(cor_mat, k=NCOL(cor_mat) %/% 2, 
                method="complete", col="red")
      @

  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Principal Component Vectors}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Principal components} are linear combinations of the \texttt{k} return vectors $\mathbf{r}_i$:
      \begin{displaymath}
        \mathbf{pc}_j = \sum_{i=1}^k {w_{ij} \, \mathbf{r}_i}
      \end{displaymath}
      Where $\mathbf{w}_j$ is a vector of weights (loadings) of the \emph{principal component} \texttt{j}, with $\mathbf{w}_j^T \mathbf{w}_j = 1$,
      \vskip1ex
      The weights $\mathbf{w}_j$ are chosen to maximize the variance of the \emph{principal components}, under the condition that they are orthogonal:
      \begin{align*}
        \mathbf{w}_j = {\operatorname{\arg \, \max}} \, \left\{ \mathbf{pc}_j^T \, \mathbf{pc}_j \right\} \\
        \mathbf{pc}_i^T \, \mathbf{pc}_j = 0 \> (i \neq j)
      \end{align*}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# create initial vector of portfolio weights
n_weights <- NROW(sym_bols)
weight_s <- rep(1/sqrt(n_weights), n_weights)
names(weight_s) <- sym_bols
# objective function equal to minus portfolio variance
object_ive <- function(weight_s, re_turns) {
  portf_rets <- re_turns %*% weight_s
  -sum(portf_rets^2) + 
    1e7*(1 - sum(weight_s^2))^2
}  # end object_ive
# objective for equal weight portfolio
object_ive(weight_s, re_turns)
# compare speed of vector multiplication methods
summary(microbenchmark(
  trans_pose=(t(re_turns[, 1]) %*% re_turns[, 1]),
  s_um=sum(re_turns[, 1]^2),
  times=10))[, c(1, 4, 5)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/pca_etf_load1.png}
      \vspace{-3em}
      <<echo=TRUE,eval=FALSE>>=
# find weights with maximum variance
optim_run <- optim(par=weight_s,
  fn=object_ive,
  re_turns=re_turns,
  method="L-BFGS-B",
  upper=rep(10.0, n_weights),
  lower=rep(-10.0, n_weights))
# optimal weights and maximum variance
weight_s <- optim_run$par
-object_ive(weight_s, re_turns)
# plot first principal component weights
barplot(weight_s, names.arg=names(weight_s), 
        xlab="", ylab="", 
        main="First Principal Component Weights")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Higher Order Principal Components}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The second \emph{principal component} can be calculated by maximizing its variance, under the constraint that it must be orthogonal to the first \emph{principal component}, 
      \vskip1ex
      Similarly, higher order \emph{principal components} can be calculated by maximizing their variances, under the constraint that they must be orthogonal to all the previous \emph{principal components}, 
      <<echo=TRUE,eval=FALSE>>=
# pc1 weights and returns
weights_1 <- weight_s
pc_1 <- re_turns %*% weights_1
# redefine objective function
object_ive <- function(weight_s, re_turns) {
  portf_rets <- re_turns %*% weight_s
  -sum(portf_rets^2) + 
    1e7*(1 - sum(weight_s^2))^2 + 
    1e7*(sum(weights_1*weight_s))^2
}  # end object_ive
# find second PC weights using parallel DEoptim
optim_run <- DEoptim::DEoptim(fn=object_ive,
  upper=rep(10, NCOL(re_turns)),
  lower=rep(-10, NCOL(re_turns)),
  re_turns=re_turns, control=list(parVar="weights_1", 
    trace=FALSE, itermax=1000, parallelType=1))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/pca_etf_load2.png}
      \vspace{-3em}
      <<echo=TRUE,eval=FALSE>>=
# pc2 weights and returns
weights_2 <- optim_run$optim$bestmem
names(weights_2) <- colnames(re_turns)
sum(weights_2^2)
sum(weights_1*weights_2)
# plot second principal component loadings
barplot(weights_2, names.arg=names(weights_2), 
        xlab="", ylab="", 
        main="Second Principal Component Loadings")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Eigenvalues of the Covariance Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The portfolio variance: $w^T \, \mathbb{C} \, w$ can be maximized under the constraint $w^T w = 1$, by maximizing the \emph{Lagrangian}:
      \begin{displaymath}
        \mathcal{L} = w^T \, \mathbb{C} \, w \, - \, \lambda \, (w^T w - 1)
      \end{displaymath}
      Where $\lambda$ is a \emph{Lagrange multiplier}, 
      \vskip1ex
      The weights corresponding to the maximum portfolio variance can be found by differentiating $\mathcal{L}$ with respect to $w$ and setting it to zero:
      \begin{displaymath}
        \mathbb{C} \, w = \lambda \, w
      \end{displaymath}
      The above is the \emph{eigenvalue} equation of the covariance matrix $\mathbb{C}$,
      \vskip1ex
      The optimal weights $w$ form an \emph{eigenvector}, and $\lambda$ is the \emph{eigenvalue} corresponding to the \emph{eigenvector} $w$, 
      \vskip1ex
      The \emph{eigenvalues} are the variances of the \emph{eigenvectors}, and their sum is equal to the sum of the return variances:
      \begin{displaymath}
        \sum_{i=1}^k \lambda_i = \sum_{i=1}^k r_i^T r_i
      \end{displaymath}
      The number of \emph{eigenvalues} is equal to the dimension of the covariance matrix,
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/pca_etf_eigenvalues.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# calculate eigenvectors and eigenvalues
ei_gen <- eigen(cov_mat)
ei_gen$vectors
weights_1
weights_2
ei_gen$values[1]
var(pc_1)
(cov_mat %*% weights_1) / weights_1
ei_gen$values[2]
var(pc_2)
(cov_mat %*% weights_2) / weights_2
sum(vari_ance)
sum(ei_gen$values)
barplot(ei_gen$values, # plot eigenvalues
  names.arg=paste0("PC", 1:n_weights), 
  las=3, xlab="", ylab="", main="Principal Component Variances")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Minimum Variance Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The highest order \emph{principal component}, with the smallest eigenvalue, has the lowest possible variance, under the constraint that the sum of the squared weights is equal to \texttt{1}: $\mathbf{w}^T \mathbf{w} = 1$,
      \vskip1ex
      So the highest order \emph{principal component} is equal to the \emph{Minimum Variance Portfolio},
      <<echo=TRUE,eval=FALSE>>=
# redefine objective function to minimize variance
object_ive <- function(weight_s, re_turns) {
  portf_rets <- re_turns %*% weight_s
  sum(portf_rets^2) + 
    1e7*(1 - sum(weight_s^2))^2
}  # end object_ive
# find highest order PC weights using parallel DEoptim
optim_run <- DEoptim::DEoptim(fn=object_ive,
  upper=rep(10, NCOL(re_turns)),
  lower=rep(-10, NCOL(re_turns)),
  re_turns=re_turns, control=list(trace=FALSE, 
    itermax=1000, parallelType=1))
# pc6 weights and returns
weights_6 <- optim_run$optim$bestmem
names(weights_6) <- colnames(re_turns)
sum(weights_6^2)
sum(weights_1*weights_6)
# calculate objective function
object_ive(weights_6, re_turns)
object_ive(ei_gen$vectors[, 6], re_turns)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/pca_etf_load6.png}
      \vspace{-3em}
      <<echo=TRUE,eval=FALSE>>=
# plot highest order principal component loadings
barplot(weights_6, names.arg=names(weights_2), 
        xlab="", ylab="", 
        main="Highest Order Principal Component Loadings")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Principal Component Analysis} of ETF Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Principal Component Analysis} (\emph{PCA}) is a \emph{dimensionality reduction} technique, that explains the returns of a large number of correlated time series as linear combinations of a smaller number of principal component time series,
      \vskip1ex
      The input time series are often scaled by their standard deviations, to improve the accuracy of \emph{PCA dimensionality reduction}, so that more information is retained by the first few \emph{principal component} time series,
      \vskip1ex
      If the input time series are not scaled, then \emph{PCA} analysis is equvalent to the \emph{eigen decomposition} of the covariance matrix, and if they are scaled, then \emph{PCA} analysis is equvalent to the \emph{eigen decomposition} of the correlation matrix,
      \vskip1ex
      The function \texttt{prcomp()} performs \emph{Principal Component Analysis} on a matrix of data (with the time series as columns), and returns the results as an object of class \texttt{prcomp}, 
      \vskip1ex
      The \texttt{prcomp()} argument \texttt{scale=TRUE} specifies that the input time series should be scaled by their standard deviations,
      \vskip1ex
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_etf_scree.png}
      A \emph{scree plot} is a bar plot of the volatilities of the \emph{principal components}, 
      <<echo=TRUE,eval=FALSE>>=
# perform principal component analysis PCA
pc_a <- prcomp(re_turns, scale=TRUE)
# plot standard deviations of principal components
barplot(pc_a$sdev, 
        names.arg=colnames(pc_a$rotation), 
        las=3, xlab="", ylab="", 
        main="Scree Plot: Volatilities of Principal Components 
  of Stock Returns")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Principal Component} Loadings (Weights)}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Principal component} loadings are the weights of portfolios which have mutually orthogonal returns,
      \vskip1ex
      The \emph{principal component} (\emph{PC}) portfolios represent the different orthogonal modes of the return variance,
      \vskip1ex
      The \emph{PC} portfolios typically consist of long or short positions of highly correlated groups of assets (clusters), so that they represent relative value portfolios,
      <<echo=TRUE,eval=FALSE>>=
# principal component loadings (weights)
pc_a$rotation
# Plot barplots with PCA weights in multiple panels
par(mfrow=c(n_weights/2, 2))
par(mar=c(2, 2, 2, 1), oma=c(0, 0, 0, 0))
for (or_der in 1:n_weights) {
  barplot(pc_a$rotation[, or_der], 
        las=3, xlab="", ylab="", main="")
  title(paste0("PC", or_der), line=-2.0, 
        col.main="red")
}  # end for
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_etf_loadings.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Principal Component} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The time series of the \emph{principal components} can be calculated by multiplying the loadings (weights) times the original data,
      \vskip1ex
      The higher order \emph{PCs} are gradually less volatile,
      <<echo=TRUE,eval=FALSE>>=
# principal component time series
pca_rets <- xts(re_turns %*% pc_a$rotation, 
                order.by=date_s)
round(cov(pca_rets), 3)
all.equal(unname(coredata(pca_rets)), unname(pc_a$x))
pca_ts <- xts:::cumsum.xts(pca_rets)
# plot principal component time series in multiple panels
par(mfrow=c(n_weights/2, 2))
par(mar=c(2, 2, 0, 1), oma=c(0, 0, 0, 0))
ra_nge <- range(pca_ts)
for (or_der in 1:n_weights) {
  plot.zoo(pca_ts[, or_der], 
           ylim=ra_nge, 
           xlab="", ylab="")
  title(paste0("PC", or_der), line=-2.0)
}  # end for
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_etf_series.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Time Series from the \protect\emph{Principal Components}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The original time series of returns can be calculated exactly from the time series of all the \emph{principal components}, by inverting the loadings matrix, 
      \vskip1ex
      The original time series of returns can be calculated approximately from just the first few \emph{principal components}, which demonstrates that \emph{PCA} is a form of \emph{dimensionality reduction}, 
      \vskip1ex
      The \emph{Kaiser-Guttman} rule uses only \emph{principal components} with \emph{variance} greater than \texttt{1}, 
      \vskip1ex
      Another rule is to use the \emph{principal components} with the largest standard deviations which sum up to \texttt{80\%} of the total variance of returns,
      \vskip1ex
      The function \texttt{solve()} solves systems of linear equations, and also inverts square matrices, 
      <<echo=(-(1:2)),eval=FALSE>>=
par(mfrow=c(n_weights/2, 2))
par(mar=c(2, 2, 0, 1), oma=c(0, 0, 0, 0))
# invert all the principal component time series
pca_rets <- re_turns %*% pc_a$rotation
sol_ved <- pca_rets %*% solve(pc_a$rotation)
all.equal(re_turns, sol_ved)
# invert first 3 principal component time series
sol_ved <- pca_rets[, 1:3] %*% solve(pc_a$rotation)[1:3, ]
sol_ved <- xts::xts(sol_ved, date_s)
sol_ved <- xts:::cumsum.xts(sol_ved)
cum_returns <- xts:::cumsum.xts(re_turns)
# plot the solved returns
for (sym_bol in sym_bols) {
  plot.zoo(
    cbind(cum_returns[, sym_bol], sol_ved[, sym_bol]), 
    plot.type="single", col=c("black", "blue"), xlab="", ylab="")
  legend(x="topleft", bty="n",
         legend=paste0(sym_bol, c("", " solved")),
         title=NULL, inset=0.0, cex=1.0, lwd=6,
         lty=1, col=c("black", "blue"))
}  # end for
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_etf_series_solved.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Principal Component Analysis} Versus \protect\emph{Eigen Decomposition}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Principal Component Analysis} (\emph{PCA}) is equivalent to the \emph{eigen decomposition} of either the covariance or the correlation matrix,
      \vskip1ex
      If the input time series \emph{are not} scaled, then \emph{PCA} is equivalent to the \emph{eigen decomposition} of the covariance matrix,
      \vskip1ex
      If the input time series \emph{are} scaled, then \emph{PCA} is equivalent to the \emph{eigen decomposition} of the correlation matrix,
      \vskip1ex
      Scaling the input time series improves the accuracy of the \emph{PCA dimensionality reduction}, allowing a smaller number of \emph{principal components} to more accurately capture the data contained in the input time series,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# eigen decomposition of covariance matrix
re_turns <- rutils::diff_it(price_s)
cov_mat <- cov(re_turns)
ei_gen <- eigen(cov_mat)
# perform PCA without scaling
pc_a <- prcomp(re_turns, scale=FALSE)
# compare outputs
all.equal(ei_gen$values, pc_a$sdev^2)
all.equal(abs(unname(ei_gen$vectors)), 
          abs(unname(pc_a$rotation)))
# eigen decomposition of correlation matrix
cor_mat <- cor(re_turns)
ei_gen <- eigen(cor_mat)
# perform PCA with scaling
pc_a <- prcomp(re_turns, scale=TRUE)
# compare outputs
all.equal(ei_gen$values, pc_a$sdev^2)
all.equal(abs(unname(ei_gen$vectors)), 
          abs(unname(pc_a$rotation)))
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Factor Analysis}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Principal Components} of \protect\emph{S\&P500} Stock Constituents}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{PCA} standard deviations are the volatilities of the \emph{principal component} time series, 
      \vskip1ex
      The original time series of returns can be calculated approximately from the first few \emph{principal components} with the largest standard deviations,
      \vskip1ex
      The \emph{Kaiser-Guttman} rule uses only \emph{principal components} with \emph{variance} greater than \texttt{1}, 
      \vskip1ex
      Another rule of thumb is to use the \emph{principal components} with the largest standard deviations which sum up to \texttt{80\%} of the total variance of returns,
      <<echo=TRUE,eval=FALSE>>=
# load S&P500 constituent stock prices
load("C:/Develop/R/lecture_slides/data/sp500_prices.RData")
date_s <- index(price_s)
# calculate simple returns (not percentage)
re_turns <- rutils::diff_it(price_s)
# de-mean (center) and scale the returns
re_turns <- t(t(re_turns) - colMeans(re_turns))
re_turns <- t(t(re_turns) / sqrt(colSums(re_turns^2)/(NROW(re_turns)-1)))
re_turns <- xts(re_turns, date_s)
# perform principal component analysis PCA
pc_a <- prcomp(re_turns, scale=TRUE)
# find number of components with variance greater than 2
num_comp <- which(pc_a$sdev^2 < 2)[1]
# plot standard deviations of principal components
barplot(pc_a$sdev[1:num_comp], 
        names.arg=colnames(pc_a$rotation[, 1:num_comp]), 
        las=3, xlab="", ylab="", 
        main="Volatilities of S&P500 Principal Components")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_sp500_scree.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} \protect\emph{Principal Component} Loadings (Weights)}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Principal component} loadings are the weights of \emph{principal component} portfolios,
      \vskip1ex
      The \emph{principal component} portfolios have mutually orthogonal returns
      represent the different orthogonal modes of the return variance, 
      <<echo=TRUE,eval=FALSE>>=
# Principal component loadings (weights)
# Plot barplots with PCA weights in multiple panels
n_comps <- 6
par(mfrow=c(n_comps/2, 2))
par(mar=c(4, 2, 2, 1), oma=c(0, 0, 0, 0))
# First principal component weights
weight_s <- sort(pc_a$rotation[, 1], decreasing=TRUE)
barplot(weight_s[1:6], 
        las=3, xlab="", ylab="", main="")
title(paste0("PC", 1), line=-2.0, 
      col.main="red")
for (or_der in 2:n_comps) {
  weight_s <- sort(pc_a$rotation[, or_der], decreasing=TRUE)
  barplot(weight_s[c(1:3, 498:500)], 
        las=3, xlab="", ylab="", main="")
  title(paste0("PC", or_der), line=-2.0, 
        col.main="red")
}  # end for
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_sp500_loadings.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} \protect\emph{Principal Component} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The time series of the \emph{principal components} can be calculated by multiplying the loadings (weights) times the original data,
      \vskip1ex
      Higher order \emph{principal components} are gradually less volatile,
      <<echo=TRUE,eval=FALSE>>=
# principal component time series
pca_rets <- xts(re_turns %*% pc_a$rotation[, 1:n_comps], 
                order.by=date_s)
round(cov(pca_rets), 3)
pca_ts <- xts:::cumsum.xts(pca_rets)
# plot principal component time series in multiple panels
par(mfrow=c(n_comps/2, 2))
par(mar=c(2, 2, 0, 1), oma=c(0, 0, 0, 0))
ra_nge <- range(pca_ts)
for (or_der in 1:n_comps) {
  plot.zoo(pca_ts[, or_der], 
           ylim=ra_nge, 
           xlab="", ylab="")
  title(paste0("PC", or_der), line=-2.0)
}  # end for
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_sp500_series.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} Factor Model From \protect\emph{Principal Components}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      By inverting the \emph{PCA} analysis, the \emph{S\&P500} constituent returns can be calculated from the first \texttt{k} \emph{principal components} under a \emph{factor model}: 
      \begin{displaymath}
        \mathbf{r}_i = \alpha_i + \sum_{j=1}^k {\beta_{ji} \, \mathbf{F}_j} + \varepsilon_i
      \end{displaymath}
      The \emph{principal components} are interpreted as \emph{market factors}: $\mathbf{F}_j = \mathbf{pc}_j$, 
      \vskip1ex
      The \emph{market betas} are the inverse of the \emph{principal component loadings}: $\beta_{ji} = w_{ij}$, 
      \vskip1ex
      The $\varepsilon_i$ are the \emph{idiosyncratic} returns, which should be mutually independent and uncorrelated to the \emph{market factor} returns,
      <<echo=(-(1:2)),eval=FALSE>>=
par(mfrow=c(n_comps/2, 2))
par(mar=c(2, 2, 0, 1), oma=c(0, 0, 0, 0))
# invert principal component time series
inv_rotation <- solve(pc_a$rotation)
all.equal(inv_rotation, t(pc_a$rotation))
sol_ved <- pca_rets %*% inv_rotation[1:n_comps, ]
sol_ved <- xts::xts(sol_ved, date_s)
sol_ved <- xts:::cumsum.xts(sol_ved)
cum_returns <- xts:::cumsum.xts(re_turns)
# plot the solved returns
sym_bols <- c("MSFT", "XOM", "JPM", "AAPL", "BRK_B", "JNJ")
for (sym_bol in sym_bols) {
  plot.zoo(
    cbind(cum_returns[, sym_bol], sol_ved[, sym_bol]), 
    plot.type="single", col=c("black", "blue"), xlab="", ylab="")
  legend(x="topleft", bty="n",
         legend=paste0(sym_bol, c("", " solved")),
         title=NULL, inset=0.05, cex=1.0, lwd=6,
         lty=1, col=c("black", "blue"))
}  # end for
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_sp500_series_solved.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} \protect\emph{Factor Model} Residuals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The original time series of returns can be calculated exactly from the time series of all the \emph{principal components}, by inverting the loadings matrix, 
      \vskip1ex
      The original time series of returns can be calculated approximately from just the first few \emph{principal components}, which demonstrates that \emph{PCA} is a form of \emph{dimensionality reduction}, 
      \vskip1ex
      The function \texttt{solve()} solves systems of linear equations, and also inverts square matrices, 
      <<echo=(-(1:2)),eval=FALSE>>=
par(mfrow=c(n_comps/2, 2))
par(mar=c(2, 2, 0, 1), oma=c(0, 0, 0, 0))
# perform ADF unit-root tests on original series and residuals
sapply(sym_bols, function(sym_bol) {
  c(series=tseries::adf.test(cum_returns[, sym_bol])$p.value,
    resid=tseries::adf.test(cum_returns[, sym_bol] - sol_ved[, sym_bol])$p.value)
})  # end sapply
# plot the residuals
for (sym_bol in sym_bols) {
  plot.zoo(cum_returns[, sym_bol] - sol_ved[, sym_bol], 
    plot.type="single", col="blue", xlab="", ylab="")
  legend(x="topleft", bty="n",
         legend=paste0(sym_bol, " residuals"),
         title=NULL, inset=0.05, cex=1.0, lwd=6,
         lty=1, col="blue")
}  # end for
# perform ADF unit-root test on principal component time series
pca_rets <- xts(re_turns %*% pc_a$rotation, 
                order.by=date_s)
pca_ts <- xts:::cumsum.xts(pca_rets)
adf_pvalues <- sapply(1:NCOL(pca_ts), function(or_der)
  tseries::adf.test(pca_ts[, or_der])$p.value)
# ADF unit-root test on stationary time series
tseries::adf.test(rnorm(1e5))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_sp500_residuals.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\subsection{Correlation and Factor Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<corr_plot,echo=(-(1:1)),eval=FALSE,fig.show='hide'>>=
library(quantmod)
### perform pair-wise correlation analysis
# calculate correlation matrix
cor_mat <- cor(re_turns)
colnames(cor_mat) <- colnames(re_turns)
rownames(cor_mat) <- colnames(re_turns)
# reorder correlation matrix based on clusters
# calculate permutation vector
library(corrplot)
or_der <- corrMatOrder(cor_mat, 
              order="hclust", 
              hclust.method="complete")
# apply permutation vector
cor_mat <- cor_mat[or_der, or_der]
# plot the correlation matrix
col_ors <- colorRampPalette(c("red", "white", "blue"))
corrplot(cor_mat, 
    tl.col="black", tl.cex=0.8, 
    method="square", col=col_ors(8), 
    cl.offset=0.75, cl.cex=0.7, 
    cl.align.text="l", cl.ratio=0.25)
# draw rectangles on the correlation matrix plot
corrRect.hclust(cor_mat, k=NROW(cor_mat) %/% 2, 
                method="complete", col="red")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/corr_plot-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Hierarchical Clustering Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{as.dist()} converts a matrix representing the \emph{distance} (dissimilarity) between elements, into an object of class \texttt{"dist"},
      \vskip1ex
      For example, \texttt{as.dist()} converts \texttt{(1-correlation)} to distance,
      \vskip1ex
      The function \texttt{hclust()} recursively combines elements into clusters based on their mutual \emph{distance},
      \vskip1ex
      First \texttt{hclust()} combines individual elements that are closest to each other,
      \vskip1ex
      Then it combines elements to the closest clusters, then clusters with other clusters, until all elements are combined into one cluster,
      \vskip1ex
      This process of recursive clustering can be represented as a \emph{dendrogram} (tree diagram), 
      \vskip1ex
      Branches of a \emph{dendrogram} represent clusters,
      \vskip1ex
      Neighboring branches contain elements that are close to each other (have small distance),
      \vskip1ex
      Neighboring branches combine into larger branches, that then combine with their closest branches, etc.
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/cluster_plot-1}
      \vspace{-4em}
      <<cluster_plot,echo=TRUE,eval=FALSE,fig.width=6,fig.height=6,fig.show='hide'>>=
# convert correlation matrix into distance object
dis_tance <- as.dist(1-cor_mat)
# perform hierarchical clustering analysis
clus_ter <- hclust(dis_tance)
plot(clus_ter, ann=FALSE, xlab="", ylab="")
title("Dendrogram representing hierarchical clustering
      \nwith dissimilarity = 1-correlation", line=-0.5)
      @
  \end{columns}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%
\subsection{depr: \protect\emph{Principal Component} Returns Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
\vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(PerformanceAnalytics)  # load package "PerformanceAnalytics"
# PC returns from rotation and scaled re_turns
re_turns_scaled <- apply(re_turns, 2, scale)
pca_rets <- re_turns_scaled %*% pc_a$rotation
# "x" matrix contains time series of PC returns
dim(pc_a$x)
class(pc_a$x)
head(pc_a$x[, 1:3], 3)
# convert PC matrix to xts and rescale to decimals
pca_rets <- xts(pc_a$x/100, 
    order.by=index(re_turns))
      @
\vspace{-1em}
      <<pca_cum_returns,echo=(-1),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PerformanceAnalytics)  # load package "PerformanceAnalytics"
chart.CumReturns(
  pca_rets[, 1:3], lwd=2, ylab="", 
  legend.loc="topright", main="")
# add title
title(main="ETF cumulative returns", line=-1)
      @
    \column{0.5\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/pca_cum_returns-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: \protect\emph{Principal Component} Returns Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-1),eval=FALSE>>=
library(PerformanceAnalytics)
# calculate PC correlation matrix
cor_mat <- cor(pca_rets)
colnames(cor_mat) <- colnames(pca_rets)
rownames(cor_mat) <- colnames(pca_rets)
cor_mat[1:3, 1:3]
table.CAPM(Ra=pca_rets[, 1:3], 
    Rb=re_turns[, "VTI"], scale=252)
      @
    \column{0.5\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Principal Component Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<pca_plot,echo=(-(1:2)),fig.height=5,fig.show='hide',eval=FALSE>>=
par(oma=c(1,0,1,0), mgp=c(2,1,0), mar=c(2,1,2,1), cex.lab=0.8, cex.axis=1.0, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # set plot panels
### Perform principal component analysis PCA
re_turns <- na.omit(rutils::env_etf$re_turns)
pc_a <- prcomp(re_turns, center=TRUE, scale=TRUE)
barplot(pc_a$sdev[1:10], 
        names.arg=colnames(pc_a$rotation)[1:10], 
        las=3, ylab="STDEV", xlab="PCVec", 
        main="PCA Explain VAR")
# Show first three principal component loadings
head(pc_a$rotation[,1:3], 3)
# Permute second principal component loadings by size
pca_vec2 <- as.matrix(
  pc_a$rotation[order(pc_a$rotation[, 2], 
  decreasing=TRUE), 2])
colnames(pca_vec2) <- "pca2"
head(pca_vec2, 3)
# The option las=3 rotates the names.arg labels
barplot(as.vector(pca_vec2), 
        names.arg=rownames(pca_vec2), 
        las=3, ylab="Loadings", 
        xlab="Symbol", main="Loadings pca2")
      @
    \column{0.5\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/pca_plot-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Principal Component Vectors}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<pca_vec,echo=(-(1:2)),eval=FALSE,fig.height=5,fig.show='hide'>>=
par(oma=c(1,0,1,0), mgp=c(2,1,0), mar=c(2,1,2,1), cex.lab=0.8, cex.axis=1.0, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(3,1))  # set plot panels
# get list of principal component vectors
pca_vecs <- lapply(1:3, function(or_der) {
  pca_vec <- as.matrix(
    pc_a$rotation[
    order(pc_a$rotation[, or_der], 
    decreasing=TRUE), or_der])
  colnames(pca_vec) <- paste0("pca", or_der)
  pca_vec
})  # end lapply
names(pca_vecs) <- c("pca1", "pca2", "pca3")
# The option las=3 rotates the names.arg labels
for (or_der in 1:3) {
  barplot(as.vector(pca_vecs[[or_der]]), 
        names.arg=rownames(pca_vecs[[or_der]]), 
        las=3, xlab="", ylab="", 
        main=paste("Loadings", 
          colnames(pca_vecs[[or_der]])))
}  # end for
      @
    \column{0.5\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/pca_vec-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Package \protect\emph{factorAnalytics}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The package \emph{factorAnalytics} performs estimation and risk analysis of linear factor models for portfolio asset returns,
    \column{0.6\textwidth}
      \vspace{-1em}
      <<eval=FALSE>>=
library(factorAnalytics)  # load package "factorAnalytics"
# get documentation for package "factorAnalytics"
packageDescription("factorAnalytics")  # get short description
help(package="factorAnalytics")  # load help page
      @
      \vspace{-1em}
      <<eval=FALSE,echo=(-(1:2)),tidy=TRUE>>=
options(width=50)
library(factorAnalytics)  # load package "factorAnalytics"
# list all objects in "factorAnalytics"
ls("package:factorAnalytics")

# list all datasets in "factorAnalytics"
# data(package="factorAnalytics")

# remove factorAnalytics from search path
detach("package:factorAnalytics")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Fitting Factor Models Using PCA}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(factorAnalytics)
# fit a three-factor model using PCA
factor_pca <- fitSfm(rutils::env_etf$re_turns, k=3)
head(factor_pca$loadings, 3)  # factor loadings
# factor realizations (time series)
head(factor_pca$factors)
# residuals from regression
factor_pca$residuals[1:3, 1:3]
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(factorAnalytics)
factor_pca$alpha  # estimated alphas
factor_pca$r2  # R-squared regression
# covariance matrix estimated by factor model
factor_pca$Omega[1:3, 4:6]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Factor Loadings}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      <<fact_load,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=10,fig.show='hide'>>=
library(factorAnalytics)
# load(file="C:/Develop/R/lecture_slides/data/portf_optim.RData")
plot(factor_pca, which.plot.group=3, 
     n.max=30, loop=FALSE)
# ?plot.sfm
      @
    \column{0.6\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/fact_load-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Time Series of Factors}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      \vspace{-1em}
      <<fact_tsplot,eval=FALSE,echo=TRUE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PortfolioAnalytics)
# plot factor cumulative returns
chart.CumReturns(factor_pca$factors, 
    lwd=2, ylab="", legend.loc="topleft", 
    main="")

# plot time series of factor returns
# plot(factor_pca, which.plot.group=2, 
#   loop=FALSE)
      @
    \column{0.6\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/fact_tsplot-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Asset Correlations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      \vspace{-1em}
      <<fact_corr_plot,eval=FALSE,echo=TRUE,fig.width=7,fig.height=7,fig.show='hide'>>=
# asset correlations "hclust" hierarchical clustering order
plot(factor_pca, which.plot.group=7, 
     loop=FALSE, order="hclust", 
     method="ellipse")
      @
    \column{0.6\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/fact_corr_plot-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Time Series of Residuals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      \vspace{-1em}
      <<fact_resid_plot,eval=FALSE,echo=TRUE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PortfolioAnalytics)
# plot residual cumulative returns
chart.CumReturns(
  factor_pca$residuals[, c("IEF", 
                  "DBC", "XLF")], 
  lwd=2, ylab="", legend.loc="topleft", 
  main="")
      @
    \column{0.6\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/fact_resid_plot-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Residual Returns Histogram}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      \vspace{-1em}
      <<fact_resid_hist,eval=FALSE,echo=TRUE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PortfolioAnalytics)
# plot residual histogram with normal curve
plot(factor_pca, asset.name="VTI", 
     which.plot.single=8, 
     plot.single=TRUE, loop=FALSE, 
     xlim=c(-0.007, 0.007))
      @
    \column{0.6\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/fact_resid_hist-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Residual Returns \texttt{Q-Q} Plot}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      \vspace{-1em}
      <<fact_resid_qq,eval=FALSE,echo=(-1),fig.width=7,fig.height=6,fig.show='hide'>>=
library(PortfolioAnalytics)
# residual Q-Q plot
plot(factor_pca, asset.name="VTI", 
     which.plot.single=9, 
     plot.single=TRUE, loop=FALSE)
      @
    \column{0.6\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/fact_resid_qq-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Autocorrelation of Residuals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      \vspace{-1em}
      <<fact_resid_autocorr,eval=FALSE,echo=TRUE,fig.width=7,fig.height=9,fig.show='hide'>>=
# SACF and PACF of residuals
plot(factor_pca, asset.name="VTI", 
     which.plot.single=5, 
     plot.single=TRUE, loop=FALSE)
      @
    \column{0.6\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/fact_resid_autocorr-1}
  \end{columns}
\end{block}

\end{frame}



\end{document}
