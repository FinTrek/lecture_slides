% FRE6871_Lecture_2
% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='scriptsize', fig.width=4, fig.height=4)
options(width=60, dev='pdf')
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
% \usepackage{mathtools}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
\usepackage{listings}
\usepackage{xcolor}
\definecolor{anti_flashwhite}{rgb}{0.95, 0.95, 0.96}
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[FRE6871 Lecture\#2]{FRE6871 \texttt{R} in Finance}
\subtitle{Lecture\#2, Spring 2018}

\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{January 29, 2018}
% \date{\today}
% \pgfdeclareimage[height=0.5cm]{university-logo}{engineering_long_white}
% \logo{\pgfuseimage{engineering_long_white}}


%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Atomic Data Objects}


%%%%%%%%%%%%%%%
\subsection{Factors}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Factors are similar to vectors, but their elements can only take values from a set of \emph{levels},
      \vskip1ex
      Factors are designed for categorical data which can only take certain values,
      \vskip1ex
      The function \texttt{factor()} converts a vector into a factor,
      \vskip1ex
      Factors have two attributes: \emph{class} (equal to \texttt{"factor"}) and \texttt{levels} (the allowed values),
      \vskip1ex
      Although factors aren't vectors, the data underlying a factor is an integer vector, called an \emph{encoding vector},
      \vskip1ex
      The function \texttt{as.numeric()} extracts the encoding vector (indices) of a factor,
      \vskip1ex
      The function \texttt{as.vector()} coerces a factor to a character vector,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# create factor vector
fac_tor <- factor(c('b', 'c', 'd', 'a', 'c', 'b'))
fac_tor
fac_tor[3]
attributes(fac_tor)  # get factor attributes
levels(fac_tor)  # get allowed values
as.numeric(fac_tor)  # get encoding vector
is.vector(fac_tor)
as.factor(1:5)  # coerce vector to factor
# coerce factor to character vector
as.vector(as.factor(1:5))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Tables of Categorical Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{table()} calculates the frequency distribution of categorical data,
      \vskip1ex
      A contingency table is a matrix that contains the frequency distribution of variables (factors) contained in a set of data,
      \vskip1ex
      The function \texttt{unique()} extracts unique elements of an object,
      \vskip1ex
      \texttt{sapply()} applies a function to a vector or a list of objects and returns a vector or a list,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
fac_tor
levels(fac_tor)  # get allowed values
unique(fac_tor)  # get unique elements
# get contingency (frequency) table
table(fac_tor)
# get contingency table using sapply
sapply(levels(fac_tor),
       function(le_vel) {
         sum(fac_tor==le_vel)
       })  # end sapply
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Classifying Continuous Numeric Data Into Categories}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Numeric data that represents a \emph{magnitude}, \emph{intensity}, or \emph{score} can be classified into categorical data, given a vector of \emph{breakpoints},
      \vskip1ex
      The \emph{breakpoints} create intervals that correspond to different \emph{categories},
      \vskip1ex
      The \emph{categories} combine elements that have a similar numeric \emph{magnitude},
      \vskip1ex
      \texttt{findInterval()} returns the indices of the intervals specified by \texttt{"vec"} that contain the elements of \texttt{"x"},
      \vskip1ex
      If there's an exact match, then \texttt{findInterval()} returns the same index as function \texttt{match()},
      \vskip1ex
      If there's no exact match, then \texttt{findInterval()} finds the element of \texttt{"vec"} that is closest to, but not greater than, the element of \texttt{"x"},
      \vskip1ex
      If all the elements of \texttt{"vec"} are greater than the element of \texttt{"x"}, then \texttt{findInterval()} returns \texttt{zero},
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(microbenchmark)
str(findInterval)
# get index of the element of "vec" that matches 5
findInterval(x=5, vec=c(3, 5, 7))
match(5, c(3, 5, 7))
# no exact match
findInterval(x=6, vec=c(3, 5, 7))
match(6, c(3, 5, 7))
# indices of "vec" that match elements of "x"
findInterval(x=1:8, vec=c(3, 5, 7))
# return only indices of inside intervals
findInterval(x=1:8, vec=c(3, 5, 7),
             all.inside=TRUE)
# make rightmost interval inclusive
findInterval(x=1:8, vec=c(3, 5, 7),
             rightmost.closed=TRUE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Classifying Numeric Data Into Categories Example}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Temperature can be categorized into "cold", "warm", "hot", etc.
      \vskip1ex
      A named numeric vector of \emph{breakpoints} can be used to convert a temperature into one of the \emph{categories},
      \vskip1ex
      \emph{Breakpoints} correspond to \emph{categories} of the data,
      \vskip1ex
      The first \emph{breakpoint} should correspond to the lowest \emph{category}, and should have a value less than any of the data,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# named numeric vector of breakpoints
brea_ks <- c(freezing=0, very_cold=30,
             cold=50, pleasant=60,
             warm=80, hot=90)
brea_ks
tempe_ratures <- runif(10, min=10, max=100)
feels_like <- names(
  brea_ks[findInterval(x=tempe_ratures,
                       vec=brea_ks)])
names(tempe_ratures) <- feels_like
tempe_ratures
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Converting Numeric Data Into Factors Using \texttt{cut()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{cut()} converts a numeric vector into a vector of factors, representing the intervals to which the numeric values belong,
      \vskip1ex
      \texttt{cut()} divides the range of values into intervals, based on a vector of breaks,
      \vskip1ex
      \texttt{cut()} then assigns factors to the numeric values, representing the intervals to which the numeric values belong,
      \vskip1ex
      The argument \texttt{"breaks"} is a numeric vector of break points that divide the range of values into intervals,
      \vskip1ex
      The argument \texttt{"labels"} is a vector of labels for the intervals,
      \vskip1ex
      The argument \texttt{"right"} is a Boolean indicating if the intervals should be closed on the right (and open on the left), or vice versa,
      \vskip1ex
      \texttt{cut()} can produce the same classification as \texttt{findInterval()}, but \texttt{findInterval()} is faster than \texttt{cut()}, because it's a compiled function,
      \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(microbenchmark)
foo <- sample(0:6) + 0.1
foo
cut(x=foo, breaks=c(2, 4, 6, 8))
rbind(foo, cut(x=foo, breaks=c(2, 4, 6, 8)))
# cut() replicates findInterval()
cut(x=1:8, breaks=c(3, 5, 7), labels=1:2,
    right=FALSE)
findInterval(x=1:8, vec=c(3, 5, 7))
# findInterval() is a compiled function, so it's faster than cut()
vec_tor <- rnorm(1000)
summary(microbenchmark(
  find_interval=
    findInterval(x=vec_tor, vec=c(3, 5, 7)),
  cuut=
    cut(x=vec_tor, breaks=c(3, 5, 7)),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting Histograms of Frequency Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{hist()} calculates and plots a histogram, and returns its data invisibly,
      \vskip1ex
      The parameter \texttt{breaks} is the number of cells of the histogram,
      \vskip1ex
      If the argument \texttt{freq} is \texttt{TRUE} then the frequencies (counts) are plotted, and if it's \texttt{FALSE} then the probability density is plotted (with total area equal to \texttt{1}),
      \vskip1ex
      The function \texttt{density()} calculates a kernel estimate of the probability density,
      \vskip1ex
      The function \texttt{lines()} draws a line through specified points,
      <<echo=TRUE,eval=FALSE>>=
# calculate DAX percentage returns
dax_rets <- diff(log(EuStockMarkets[, 1]))
# plot histogram
par(mar=c(1, 1, 1, 1), oma=c(2, 2, 2, 0))
histo_gram <- hist(dax_rets, breaks=30, 
  main="", ylim=c(0, 60), xlim=c(-0.04, 0.04), 
  xlab="", ylab="", freq=FALSE)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
    \includegraphics[width=0.5\paperwidth]{figure/hist_dax.png}
    \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# draw kernel density of histogram
lines(density(dax_rets), col='red', lwd=2)
# add density of normal distribution
curve(expr=dnorm(x, mean=mean(dax_rets), 
  sd=sd(dax_rets)), add=TRUE, type="l", 
  lwd=2, col="blue")
title(main="DAX return distribution", line=0)
# add legend
legend("topright", inset=0.05, cex=0.8, title=NULL,
  leg=c(colnames(EuStockMarkets)[1], "Normal"),
  lwd=6, bg="white", col=c("red", "blue"))
# total area under histogram
diff(histo_gram$breaks) %*% histo_gram$density
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Functions}


%%%%%%%%%%%%%%%
\subsection{\secname \hskip0.4em That Return \texttt{invisible}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      If a return value is wrapped in the function \texttt{invisible()} then the return value isn't printed,
      \vskip1ex
      But if the function is assigned to a variable, then its return value is assigned to that variable,
      \vskip1ex
      \texttt{invisible()} allows creating functions whose return values can be assigned, but which do not print when they're not assigned,
      \vskip1ex
      The function \texttt{load()} reads data from \texttt{*.RData} files, and \emph{invisibly} returns a vector of names of objects created in the workspace,
    \column{0.6\textwidth}
      \vspace{-1em}
        <<echo=(-(1:1))>>=
setwd("C:/Develop/R/lecture_slides/data")
# define a function that returns invisibly
return_invisible <- function(in_put) {
  invisible(in_put)
}  # end return_invisible

return_invisible(2)

glob_var <- return_invisible(2)
glob_var

rm(list=ls())  # remove all objects
# load objects from file
loaded <- load(file="C:/Develop/data/my_data.RData")
loaded  # vector of loaded objects
ls()  # list objects
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Exploring Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      If a function name is called alone without arguments, then \texttt{R} displays the function code (but it must be on the search path),
      \vskip1ex
      Non-visible objects can't be viewed by calling their name,
      \vskip1ex
      The function \texttt{getAnywhere()} displays information about \texttt{R} objects, including non-visible objects,
      \vskip1ex
      The function \texttt{getAnywhere()} also displays \texttt{R} objects that aren't on the search path,
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# show the function code
plot.default
# display function
getAnywhere(plot.default)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The dots \texttt{"..."} Function Argument}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The dots \texttt{"..."} function argument is a formal argument without a name, as opposed to the other formal arguments which all have names,
      \vskip1ex
      The dots \texttt{"..."} bind with any number of additional arguments, that aren't already bound by name or position to the named arguments,
      \vskip1ex
      The dots \texttt{"..."} are used when the number of arguments isn't known in advance, and allows functions to accept an indefinite number of arguments,
      \vskip1ex
      The dots \texttt{"..."} are sometimes placed \emph{after} the named arguments, to allow passing of additional parameters into a function,
      \vskip1ex
      \emph{Functionals} often place the dots \texttt{"..."} argument \emph{after} the named arguments, to allow passing the dots \texttt{"..."} to the function being called by the \emph{functional},
    \column{0.6\textwidth}
      \vspace{-1em}
        <<echo=TRUE>>=
str(plot)  # dots for additional plot parameters
bind_dots <- function(in_put, ...) {
  paste0("in_put=", in_put,
         ", dots=", paste(..., sep=", "))
}  # end bind_dots
bind_dots(1, 2, 3)  # "in_put" bound by position
bind_dots(2, in_put=1, 3)  # "in_put" bound by name
bind_dots(1, 2, 3, foo=10)  # named argument bound to dots
bind_dots <- function(arg1, arg2, ...) {
  arg1 + 2*arg2 + sum(...)
}  # end bind_dots
bind_dots(3, 2)  # bind arguments by position
bind_dots(3, 2, 5, 8)  # extra arguments bound to dots
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Argument Binding With dots \texttt{"..."} Argument}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The dots \texttt{"..."} argument is sometimes placed \emph{before} the named arguments, so that a function can accept an indefinite number of arguments, without binding them by position with the named arguments,
      \vskip1ex
      When the dots \texttt{"..."} are placed \emph{before} the named arguments, the named arguments are often assigned default values, so they don't have to be bound to a value in the call,
      \vskip1ex
      Arguments that appear after the dots \texttt{"..."} must be \emph{bound} by their full name, and can't be partially \emph{bound},
    \column{0.6\textwidth}
      \vspace{-1em}
        <<echo=TRUE>>=
str(sum)  # dots before other arguments
sum(1, 2, 3)  # dots bind before other arguments
sum(1, 2, NA, 3, na.rm=TRUE)
bind_dots <- function(..., in_put) {
  paste0("in_put=", in_put,
         ", dots=", paste(..., sep=", "))
}  # end bind_dots
# arguments after dots must be bound by full name
bind_dots(1, 2, 3, in_put=10)
bind_dots(1, 2, 3, in_put=10, foo=4)  # dots bound
bind_dots(1, 2, 3)  # "in_put" not bound
bind_dots <- function(..., in_put=10) {
  paste0("in_put=", in_put,
         ", dots=", paste(..., sep=", "))
}  # end bind_dots
bind_dots(1, 2, 3)  # "in_put" not bound, but has default
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Wrapper \secname \hskip0.4em With dots \texttt{"..."} Argument}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      \emph{Wrapper} functions provide a convenient user interface to functions, by assigning default argument values, validating data, and formatting the output,
      \vskip1ex
      \emph{Wrapper} functions are designed to perform the actions of other functions, while reducing their complexity,
      \vskip1ex
      The dots \texttt{"..."} argument of the \emph{wrapper} function allows passing additional arguments on to the wrapped function,
      \vskip1ex
      \emph{Wrapper} functions should be used with caution, since wrapping a function creates extra code (overhead), which slows down \texttt{R},
    \column{0.6\textwidth}
      \vspace{-1em}
        <<>>=
# wrapper for mean() with default na.rm=TRUE
my_mean <- function(x, na.rm=TRUE, ...) {
  mean(x=x, na.rm=na.rm, ...)
}  # end my_mean
foo <- sample(c(1:10, NA, rep(0.1, t=5)))
mean(c(foo, NA))
mean(c(foo, NA), na.rm=TRUE)
my_mean(c(foo, NA))
my_mean(c(foo, NA), trim=0.4)  # pass extra argument
# wrapper for saving data into default directory
save_data <- function(...,
                      file=stop("error: no file name"),
                      my_dir="C:/Develop/data") {
# create file path
  file <- file.path(my_dir, file)
  save(..., file=file)
}  # end save_data
foo <- 1:10
save_data(foo, file="scratch.RData")
save_data(foo, file="scratch.RData", my_dir="C:/Develop")
# wrapper for testing negative arguments
stop_if_neg <- function(in_put) {
  if (!is.numeric(in_put) || in_put<0)
    stop("argument not numeric or negative")
}  # end stop_if_neg
# wrapper for sqrt()
my_sqrt <- function(in_put) {
  stop_if_neg(in_put)
  sqrt(in_put)
}  # end my_sqrt
my_sqrt(2)
my_sqrt(-2)
my_sqrt(NA)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Side effects Using the Super-assignment Operator \texttt{"<<-"}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Function \emph{side effects} are operations on objects outside a function's \emph{evaluation} environment,
      \vskip1ex
      The functions \texttt{plot()} and \texttt{load()} are examples of functions that produce \emph{side effects},
      \vskip1ex
      \texttt{load()} reads data from an \texttt{*.RData} file, and creates objects in the workspace that are contained in the \texttt{*.RData} file,
      \vskip1ex
      The super-assignment operator \texttt{"<<-"} allows creating functions that produce \emph{side effects},
      \vskip1ex
      The super-assignment operator \texttt{"<<-"} modifies or creates variables in the \emph{enclosing} environment in which a function was \emph{defined} (\emph{lexical} scoping),
      \vskip1ex
      If a function was \emph{defined} in the \emph{global} environment then that's the function's \emph{enclosing} environment, and the \texttt{"<<-"} operator operates on variables in the \emph{global} environment,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-(1:1))>>=
setwd("C:/Develop/R/lecture_slides/data")
rm(list=ls())  # remove all objects
ls()  # list objects
# load objects from file (side effect)
load(file="my_data.RData")
ls()  # list objects
glob_var <- 1  # define a global variable
# explore function scope and side effects
side_effect <- function() {
  cat("global glob_var:\t", glob_var, "\n")
# define local "glob_var" variable
  glob_var <- 10
# re-define the global "glob_var"
  glob_var <<- 2
  cat("local glob_var:\t", glob_var, "\n")
}  # end side_effect
side_effect()
# global variable was modified as side effect
glob_var
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Higher-order Functions}


%%%%%%%%%%%%%%%
\subsection{Functions as First Class Objects}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      Functions in \texttt{R} are \emph{first class objects}, which means they can be treated like any other \texttt{R} object:
      \begin{itemize}
        \item Functions can be passed as arguments to other functions,
        \item Functions can be nested (defined inside other functions),
        \item Functions can return functions as their return value,
      \end{itemize}
      \emph{Higher-order} functions are \texttt{R} functions that either accept a function as their argument (input) or return a function as their value (output),
    \column{0.6\textwidth}
      \vspace{-1em}
        <<>>=
# create functional that accepts a function as input argument
func_tional <- function(func_name) {
# calculates statistic on random numbers
  set.seed(1)
  func_name(runif(1e4))  # apply the function name
}  # end func_tional
func_tional(mean)
func_tional(sd)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Functionals}


%%%%%%%%%%%%%%%
\subsection{Functionals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      \emph{Functionals} are functions that accept a function or a function name (string) as one of their input arguments,
      \vskip1ex
      \emph{Functionals} are able to execute function calls using the function names,
      \vskip1ex
      The function \texttt{match.fun()} returns a function name that is specified by a string,
      \vskip1ex
      \emph{Functionals} that call \texttt{match.fun()} are able to accept a string as a function name, because \texttt{match.fun()} converts it to a function,
      \vskip1ex
      \texttt{match.fun()} produces an error condition if it fails to find a function with the specified name,
    \column{0.6\textwidth}
      \vspace{-1em}
        <<>>=
# func_tional accepts function name and additional argument
func_tional <- function(func_name, in_put) {
# produce function name from argument
  func_name <- match.fun(func_name)
# execute function call
  func_name(in_put)
}  # end func_tional
func_tional(sqrt, 4)
# string also works because match.fun() converts it to a function
func_tional("sqrt", 4)
str(sum)  # sum() accepts multiple arguments
# func_tional can't accept indefinite number of arguments
func_tional(sum, 1, 2, 3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Functionals with dots \texttt{"..."} Argument}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The dots \texttt{"..."} argument in \emph{functionals} can be used to pass additional arguments to the function being called by the \emph{functional},
      \vskip1ex
      If named values are passed to the dots \texttt{"..."} argument, then the \emph{functional} can bind them to the correct formal arguments of the function being called by the \emph{functional},
    \column{0.6\textwidth}
      \vspace{-1em}
        <<>>=
# func_tional accepts function name and dots '...' argument
func_tional <- function(func_name, ...) {
  func_name <- match.fun(func_name)
  func_name(...)  # execute function call
}  # end func_tional
func_tional(sum, 1, 2, 3)
func_tional(sum, 1, 2, NA, 4, 5)
func_tional(sum, 1, 2, NA, 4, 5, na.rm=TRUE)
# function with three arguments and dots '...' arguments
my_func <- function(in_put, param1, param2, ...) {
  c(input=in_put, param1=param1, param2=param2,
        dots=c(...))
}  # end my_func
my_func(1, 2, 3, param2=4, param1=5)
func_tional(my_func, 1, 2, 3, param2=4, param1=5)
func_tional(my_func, 1, 2, 3, 4, 5)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Anonymous Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      \texttt{R} allows defining functions without assigning a name to them,
      \vskip1ex
      \emph{Anonymous} functions are functions that are not assigned to a name,
      \vskip1ex
      \emph{Anonymous} functions can be passed as arguments to \emph{functionals},
    \column{0.6\textwidth}
      \vspace{-1em}
        <<>>=
# simple anonymous function
(function(x) (x + 3)) (10)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Functionals with Anonymous Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      \emph{Anonymous} functions can be passed as arguments to \emph{functionals},
      \vskip1ex
      \emph{Anonymous} functions can also be used as default values for function arguments,
    \column{0.6\textwidth}
      \vspace{-1em}
        <<>>=
# anonymous function passed to func_tional
func_tional(func_name=(function(x) (x + 3)), 5)
# anonymous function is default value
func_tional <-
  function(..., func_name=function(x, y, z) {x+y+z}) {
    func_name <- match.fun(func_name)
    func_name(...)  # execute function call
}  # end func_tional
func_tional(2, 3, 4)  # use default func_name
func_tional(2, 3, 4, 5)
# func_name bound by name
func_tional(func_name=sum, 2, 3, 4, 5)
# pass anonymous function to func_name
func_tional(func_name=function(x, y, z) {x*y*z},
            2, 3, 4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Executing Function Calls Using the \texttt{do.call()} Functional}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The functional \texttt{do.call()} executes a function call using a function name and a list of arguments,
      \vskip1ex
      \texttt{do.call()} allows calling a function on arguments that are elements of a list,
      \vskip1ex
      \texttt{do.call()} passes the list elements individually, instead of passing the whole list as one argument:\\
      \texttt{do.call(fun, list)=
      fun(list[[1]], list[[2]], \ldots)}
      \vskip1ex
      \texttt{do.call()} can be called inside other \emph{functionals} to allow them to execute function calls,
      \vskip1ex
      The function \texttt{str()} displays the structure of an \texttt{R} object, for example a function name and its formal arguments,
    \column{0.6\textwidth}
      \vspace{-1em}
        <<>>=
str(sum)  # sum() accepts multiple arguments
# sum() can't accept list of arguments
sum(list(1, 2, 3))
str(do.call)  # "what" argument is a function
# do.call passes list elements into "sum" individually
do.call(sum, list(1, 2, 3))
do.call(sum, list(1, 2, NA, 3))
do.call(sum, list(1, 2, NA, 3, na.rm=TRUE))
# func_tional() accepts list with function name and arguments
func_tional <- function(list_arg) {
# produce function name from argument
  func_name <- match.fun(list_arg[[1]])
# execute function call uing do.call()
  do.call(func_name, list_arg[-1])
}  # end func_tional
arg_list <- list("sum", 1, 2, 3)
func_tional(arg_list)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Loops Using the \texttt{apply()} \secname}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      An important example of \emph{functionals} are the \texttt{apply()} functionals,
      \vskip1ex
      The functional \texttt{apply()} returns the result of applying a function to the rows or columns of an array or matrix,
      \vskip1ex
      If \texttt{MARGIN=1} then the function will be applied over the matrix \emph{rows,}
      \vskip1ex
      If \texttt{MARGIN=2} then the function will be applied over the matrix \emph{columns},
      \vskip1ex
      \texttt{apply()} performs a loop over the list of objects, and can replace \texttt{"for"} loops in \texttt{R},
    \column{0.6\textwidth}
      \vspace{-1em}
        <<echo=(-1)>>=
rm(list=ls())
str(apply)  # get list of arguments
# create a matrix
mat_rix <- matrix(6:1, nrow=2, ncol=3)
mat_rix
# sum the rows and columns
row_sums <- apply(mat_rix, 1, sum)
col_sums <- apply(mat_rix, 2, sum)
mat_rix <- cbind(c(sum(row_sums), row_sums),
                  rbind(col_sums, mat_rix))
dimnames(mat_rix) <- list(c("col_sums", "row1", "row2"),
                         c("row_sums", "col1", "col2", "col3"))
mat_rix
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \texttt{apply()} Functional with dots \texttt{"..."} Argument}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The dots \texttt{"..."} argument in \texttt{apply()} is designed to pass additional arguments to the function being called by \texttt{apply()},
      \vskip1ex
      The additional arguments to \texttt{apply()} must be \emph{bound} by their full (complete) names,
      <<>>=
str(apply)  # get list of arguments
mat_rix <- matrix(sample(12), nrow=3, ncol=4)  # create a matrix
mat_rix
apply(mat_rix, 2, sort)  # sort matrix columns
apply(mat_rix, 2, sort, decreasing=TRUE)  # sort decreasing order
      @
    \column{0.5\textwidth}
        <<>>=
mat_rix[2, 2] <- NA  # introduce NA value
mat_rix
# calculate median of columns
apply(mat_rix, 2, median)
# calculate median of columns with na.rm=TRUE
apply(mat_rix, 2, median, na.rm=TRUE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \texttt{apply()} Functional with Anonymous Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The \texttt{apply()} functional combined with \emph{anonymous} functions can be used to loop over function parameters,
      \vskip1ex
      The dots \texttt{"..."} argument in \texttt{apply()} is designed to pass additional arguments to the function being called by \texttt{apply()},
      \vskip1ex
      The additional arguments to \texttt{apply()} must be \emph{bound} by their full (complete) names,
    \column{0.6\textwidth}
      \vspace{-1em}
        <<echo=(-1)>>=
rm(list=ls())
# DAX percent returns
dax_rets <- 100*diff(log(EuStockMarkets[, 1]))
library(moments)  # load package moments
str(moment)  # get list of arguments
# apply moment function
moment(x=dax_rets, order=3)
# 4x1 matrix of moment orders
moment_orders <- as.matrix(1:4)
# anonymous function allows looping over function parameters
apply(X=moment_orders, MARGIN=1,
      FUN=function(moment_order) {
          moment(x=dax_rets, order=moment_order)
        }  # end anonymous function
      )  # end apply

# another way of passing parameters into moment() function
apply(X=moment_orders, MARGIN=1, FUN=moment,
      x=dax_rets)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{apply()} Calling Functions with Multiple Arguments}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      When \texttt{apply()} calls a function with multiple arguments, then care must be taken for proper argument binding,
      \vskip1ex
      The dots \texttt{"..."} argument in \texttt{apply()} allows passing additional arguments to the function being called by \texttt{apply()},
      \vskip1ex
      The additional arguments to \texttt{apply()} must be \emph{bound} by their full (complete) names,
      \vskip1ex
      The values of the \texttt{"X"} argument in \texttt{apply()} are \emph{bound} by position to the first unused argument in the function being called by \texttt{apply()},
    \column{0.6\textwidth}
      \vspace{-1em}
        <<echo=TRUE>>=
# function with three arguments
my_func <- function(arg1, arg2, arg3) {
  c(arg1=arg1, arg2=arg2, arg3=arg3)
}  # end my_func
my_func(1, 2, 3)
da_ta <- as.matrix(1:4)
# pass da_ta to arg1
apply(X=da_ta, MAR=1, FUN=my_func, arg2=2, arg3=3)
# pass da_ta to arg2
apply(X=da_ta, MAR=1, FUN=my_func, arg1=1, arg3=3)
# pass da_ta to arg3
apply(X=da_ta, MAR=1, FUN=my_func, arg1=1, arg2=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \texttt{lapply()} Functional}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{lapply()} functional is a specialized version of the \texttt{apply()} functional,
      \vskip1ex
      \texttt{lapply()} applies a function to a list of objects and returns a list,
      \vskip1ex
      The function \texttt{unlist()} flattens a list into a vector that contains the atomic elements of the list,
      \vskip1ex
      {\color{red}{Rule of Thumb}}\\
      It's often better to use \texttt{lapply()}, since \texttt{apply()} and \texttt{sapply()} attempt to coerce their output into a vector or matrix, which may cause them to fail,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<>>=
# vector of means of numeric columns
sapply(iris[, -5], mean)
# list of means of numeric columns
lapply(iris[, -5], mean)
# lapply using anonymous function
unlist(lapply(iris,
              function(col_umn) {
                if (is.numeric(col_umn)) mean(col_umn)
              }  # end anonymous function
              )  # end lapply
       )  # end unlist
unlist(sapply(iris, function(col_umn) {
  if (is.numeric(col_umn)) mean(col_umn)}))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \texttt{sapply()} Functional}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The \texttt{sapply()} functional is a specialized version of the \texttt{apply()} functional,
      \vskip1ex
      \texttt{sapply()} applies a function to a vector or a list of objects and returns a vector or a list,
      \vskip1ex
      \texttt{sapply()} tries to return a vector, but if the elements can't be combined into a vector, then it returns a list,
      \vskip1ex
      When \texttt{sapply()} is given a data frame, it interprets it as a list, and applies the function to each element (column) of the data frame,
    \column{0.6\textwidth}
      \vspace{-1em}
        <<>>=
sapply(6:10, sqrt)  # sapply on vector
sapply(list(6, 7, 8, 9, 10), sqrt)  # sapply on list

# calculate means of iris data frame columns
sapply(iris, mean)  # returns NA for Species

# create a matrix
mat_rix <- matrix(sample(100), ncol=4)
# calculate column means using apply
apply(mat_rix, 2, mean)

# calculate column means using sapply, with anonymous function
sapply(1:NCOL(mat_rix),
       function(col_index) {  # anonymous function
         mean(mat_rix[, col_index])
  }  # end anonymous function
)  # end sapply
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{sapply()} Returning Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      If the function called by \texttt{sapply()} returns a vector, then \texttt{sapply()} returns a matrix, if possible,
      \vskip1ex
      The vectors returned by the function are arranged to form columns of the matrix returned by \texttt{sapply()},
      \vskip1ex
      But if the function returns vectors of different lengths, then \texttt{sapply()} cannot return a matrix, and returns a list instead,
      \vskip1ex
      This behavior of \texttt{sapply()} can cause run-time errors,
      \vskip1ex
      The function \texttt{vapply()} is similar to \texttt{sapply()}, but it always attempts to simplify its output to a matrix, and if it can't then it produces an error,
      \vskip1ex
      \texttt{vapply()} requires the argument \texttt{FUN.VALUE} that specifes the output format of the function called by \texttt{vapply()},
    \column{0.6\textwidth}
      \vspace{-1em}
        <<>>=
# vectors form columns of matrix returned by sapply
sapply(2:4, function(num) c(el1=num, el2=2*num))
# vectors of different lengths returned as list
sapply(2:4, function(num) 1:num)
# vapply is similar to sapply
vapply(2:4, function(num) c(el1=num, el2=2*num),
       FUN.VALUE=c(row1=0, row2=0))
# vapply produces an error if it can't simplify
vapply(2:4, function(num) 1:num,
       FUN.VALUE=c(row1=0, row2=0))
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Writing Fast \texttt{R} Code Using Vectorized Operations}


%%%%%%%%%%%%%%%
\subsection{Vectorized Functions for Vector Computations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Vectorized} functions accept \texttt{vectors} as their arguments, and return a vector of the same length as their value,
      \vskip1ex
      Many \emph{vectorized} functions are also \emph{compiled} (they pass their data to compiled \texttt{C++} code), which makes them very fast,
      \vskip1ex
      The following \emph{vectorized compiled} functions calculate cumulative values over large vectors:
      \begin{itemize}
        \item \texttt{cummax()}
        \item \texttt{cummin()}
        \item \texttt{cumsum()}
        \item \texttt{cumprod()}
      \end{itemize}
      Standard arithmetic operations (\texttt{"+", "-"}, etc.) can be applied to \texttt{vectors}, and are implemented as \emph{vectorized compiled} functions,
      \vskip1ex
      \texttt{ifelse()} and \texttt{which()} are \emph{vectorized compiled} functions for logical operations,
      \vskip1ex
      But many \emph{vectorized} functions perform their calculations in \texttt{R} code, and are therefore slow, but convenient to use,
    \column{0.5\textwidth}
        <<eval=FALSE>>=
vec_tor1 <- rnorm(1000000)
vec_tor2 <- rnorm(1000000)
big_vector <- numeric(1000000)
# sum two vectors in two different ways
summary(microbenchmark(
  # sum vectors using "for" loop
  r_loop=(for (i in 1:NROW(vec_tor1)) {
    big_vector[i] <- vec_tor1[i] + vec_tor2[i]
  }),
  # sum vectors using vectorized "+"
  vec_torized=(vec_tor1 + vec_tor2),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
# allocate memory for cumulative sum
cum_sum <- numeric(NROW(big_vector))
cum_sum[1] <- big_vector[1]
# calculate cumulative sum in two different ways
summary(microbenchmark(
# cumulative sum using "for" loop
  r_loop=(for (i in 2:NROW(big_vector)) {
    cum_sum[i] <- cum_sum[i-1] + big_vector[i]
  }),
# cumulative sum using "cumsum"
  vec_torized=cumsum(big_vector),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vectorized Functions for Matrix Computations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{apply()} loops are very inefficient for calculating statistics over rows and columns of very large matrices,
      \vskip1ex
      \texttt{R} has very fast \emph{vectorized compiled} functions for calculating sums and means of rows and columns:
      \begin{itemize}
        \item \texttt{rowSums()}
        \item \texttt{colSums()}
        \item \texttt{rowMeans()}
        \item \texttt{colMeans()}
      \end{itemize}
      These \emph{vectorized} functions are also \emph{compiled} functions, so they're very fast because they pass their data to compiled \texttt{C++} code, which performs the loop calculations,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<eval=FALSE>>=
# calculate row sums two different ways
summary(microbenchmark(
  row_sums=rowSums(big_matrix),
  ap_ply=apply(big_matrix, 1, sum),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fast \texttt{R} Code for Matrix Computations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functions \texttt{pmax()} and \texttt{pmin()} calculate the "parallel" maxima (minima) of multiple vector arguments,
      \vskip1ex
      \texttt{pmax()} and \texttt{pmin()} return a vector, whose \emph{n}-th element is equal to the maximum (minimum) of the \emph{n}-th elements of the arguments, with shorter vectors recycled if necessary,
      \vskip1ex
      \texttt{pmax.int()} and \texttt{pmin.int()} are methods of generic functions \texttt{pmax()} and \texttt{pmin()}, designed for atomic vectors,
      \vskip1ex
      \texttt{pmax()} can be used to quickly calculate the maximum values of rows of a matrix, by first converting the matrix columns into a list, and then passing them to \texttt{pmax()},
      \vskip1ex
      \texttt{pmax.int()} and \texttt{pmin.int()} are very fast because they are \emph{compiled} functions (compiled from \texttt{C++} code),
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(microbenchmark)
str(pmax)
# calculate row maximums two different ways
summary(microbenchmark(
  p_max=
    do.call(pmax.int,
      lapply(seq_along(big_matrix[1, ]),
        function(in_dex) big_matrix[, in_dex])),
  l_apply=unlist(
    lapply(seq_along(big_matrix[, 1]),
        function(in_dex) max(big_matrix[in_dex, ]))),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vectorized \texttt{if-else} Statements Using Function \texttt{ifelse()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{ifelse()} performs \emph{vectorized} \texttt{if-else} statements on vectors,
      \vskip1ex
      \texttt{ifelse()} is much faster than performing an element-wise loop in \texttt{R},
        <<func_ifelse,eval=FALSE,echo=TRUE,fig.show='hide'>>=
# create two numeric vectors
vec_tor1 <- sin(0.25*pi*1:10)
vec_tor2 <- cos(0.25*pi*1:10)
# create third vector using 'ifelse'
vec_tor3 <- ifelse(vec_tor1 > vec_tor2,
                  vec_tor1, vec_tor2)
# cbind all three together
vec_tor4 <- cbind(vec_tor1, vec_tor2, vec_tor3)

# set plotting parameters
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0),
    cex.lab=0.8, cex.axis=0.8, cex.main=0.8,
    cex.sub=0.5)
# plot matrix
matplot(vec_tor4, type="l", lty="solid",
        col=c("green", "blue", "red"),
        lwd=c(2, 2, 2), xlab="", ylab="")
# add legend
legend(x="bottomright", legend=colnames(vec_tor4),
       title="", inset=0.05, cex=0.8, lwd=2,
       lty=c(1, 1, 1), col=c("green", "blue", "red"))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/func_ifelse-1}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Parallel Computing}


%%%%%%%%%%%%%%%
\subsection{Parallel Computing in \texttt{R}}
\begin{frame}[t]{\subsecname}
\vspace{-1em}

\begin{block}{Parallel Computing in \texttt{R}}
      Parallel computing means splitting a computing task into separate sub-tasks, and then simultaneously computing the sub-tasks on several computers or CPU cores,
      \vskip1ex
      There are many different packages that allow parallel computing in \texttt{R}, most importantly package \emph{parallel}, and packages \texttt{foreach}, \texttt{doParallel}, and related packages:\\
      \hskip1em\url{http://cran.r-project.org/web/views/HighPerformanceComputing.html}\\
      \hskip1em\url{http://blog.revolutionanalytics.com/high-performance-computing/}\\
      \hskip1em\url{http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/}\\
\end{block}

\begin{block}{\texttt{R} Base Package \emph{parallel}}
  The package \emph{parallel} provides functions for parallel computing using multiple cores of CPUs,
  \vskip1ex
  The package \emph{parallel} is part of the standard \texttt{R} distribution, so it doesn't need to be installed.\\
  \hskip1em\url{http://adv-r.had.co.nz/Profiling.html\#parallelise}\\
  \hskip1em\url{https://github.com/tobigithub/R-parallel/wiki/R-parallel-package-overview}\\
\end{block}

\begin{block}{Packages \texttt{foreach}, \texttt{doParallel}, and Related Packages}
      \hskip1em\url{http://blog.revolutionanalytics.com/2015/10/updates-to-the-foreach-package-and-its-friends.html}\\
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Parallel Computing Using Package \protect\emph{parallel}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{parallel} provides functions for parallel computing using multiple cores of CPUs,
      \vskip1ex
      The package \emph{parallel} is part of the standard \texttt{R} distribution, so it doesn't need to be installed,
      \vskip1ex
      Different functions from package \emph{parallel} need to be called depending on the operating system (\emph{Windows}, \emph{Mac-OSX}, or \emph{Linux}),
    \column{0.5\textwidth}
      \vspace{-1em}
      <<eval=FALSE>>=
library(parallel)  # load package parallel
# get short description
packageDescription("parallel")
# load help page
help(package="parallel")
# list all objects in "parallel"
ls("package:parallel")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Parallel Loops Using Package \protect\emph{parallel}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Some computing tasks naturally lend themselves to parallel computing, like for example performing loops,
      \vskip1ex
      Different functions from package \emph{parallel} need to be called depending on the operating system (\emph{Windows}, \emph{Mac-OSX}, or \emph{Linux}),
      \vskip1ex
      The function \texttt{mclapply()} performs apply loops (similar to \texttt{lapply()}) using parallel computing on several CPU cores under \emph{Mac-OSX} or \emph{Linux},
      \vskip1ex
      Under \emph{Windows}, a cluster of \texttt{R} processes (one per each CPU core) need to be started first, by calling the function \texttt{makeCluster()},
      \vskip1ex
      \emph{Mac-OSX} and \emph{Linux} don't require calling the function \texttt{makeCluster()},
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs apply loops under \emph{Windows}, using parallel computing on several CPU cores,
      \vskip1ex
      The function \texttt{stopCluster()} stops the \texttt{R} processes running on several CPU cores,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # load package parallel
# calculate number of available cores
num_cores <- detectCores() - 1
# define function that pauses execution
paws <- function(x, sleep_time) {
  Sys.sleep(sleep_time)
  x
}  # end paws
# perform parallel loop under Mac-OSX or Linux
paw_s <- mclapply(1:10, paws, mc.cores=num_cores,
                sleep_time=0.01)
# initialize compute cluster under Windows
clus_ter <- makeCluster(num_cores)
# perform parallel loop under Windows
paw_s <- parLapply(clus_ter, 1:10, paws,
                 sleep_time=0.01)
library(microbenchmark)  # load package microbenchmark
# compare speed of lapply versus parallel computing
summary(microbenchmark(
  l_apply=lapply(1:10, paws, sleep_time=0.01),
  parl_apply=
    parLapply(clus_ter, 1:10, paws, sleep_time=0.01),
  times=10)
)[, c(1, 4, 5)]
# stop R processes over cluster under Windows
stopCluster(clus_ter)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Computing Overhead of Parallel Computing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Parallel computing requires additional resources and time for distributing the computing tasks and collecting the output, which produces a computing overhead,
      \vskip1ex
      Therefore parallel computing can actually be slower for small computations, or for computations that can't be naturally separated into sub-tasks,
      <<echo=(-(1:10)),eval=FALSE>>=
library(parallel)  # load package parallel
# calculate number of available cores
num_cores <- detectCores() - 1
# initialize compute cluster under Windows
clus_ter <- makeCluster(num_cores)
# define function that pauses execution
paws <- function(x, sleep_time) {
  Sys.sleep(sleep_time)
  x
}  # end paws
# compare speed of lapply with parallel computing
iter_ations <- 3:10
compute_times <- sapply(iter_ations,
  function(max_iterations, sleep_time) {
    out_put <- summary(microbenchmark(
      lapply=lapply(1:max_iterations, paws,
                    sleep_time=sleep_time),
      parallel=parLapply(clus_ter, 1:max_iterations,
              paws, sleep_time=sleep_time),
      times=10))[, c(1, 4)]
    structure(out_put[, 2],
              names=as.vector(out_put[, 1]))
    }, sleep_time=0.01)
compute_times <- t(compute_times)
rownames(compute_times) <- iter_ations
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/parallel_plot.png}\\
      \vspace{-1em}
      <<parallel_plot,echo=(-(1:1)),eval=FALSE>>=
library(parallel)  # load package parallel
plot(x=rownames(compute_times),
     y=compute_times[, "lapply"],
     type="l", lwd=2, col="blue",
     main="Compute times",
     xlab="number of iterations in loop", ylab="",
     ylim=c(0, max(compute_times[, "lapply"])))
lines(x=rownames(compute_times),
      y=compute_times[, "parallel"], lwd=2, col="green")
legend(x="topleft", legend=colnames(compute_times),
       inset=0.1, cex=1.0, bg="white",
       lwd=2, lty=c(1, 1), col=c("blue", "green"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Parallel Computing Over Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Very often we need to perform time consuming calculations over columns of matrices,
      \vskip1ex
      The function \texttt{parCapply()} performs an apply loop over columns of matrices using parallel computing on several CPU cores,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:5)),eval=FALSE>>=
library(parallel)  # load package parallel
# calculate number of available cores
num_cores <- detectCores() - 1
# initialize compute cluster under Windows
clus_ter <- makeCluster(num_cores)
# define large matrix
mat_rix <- matrix(rnorm(7*10^5), ncol=7)
# define aggregation function over column of matrix
agg_regate <- function(col_umn) {
  out_put <- 0
  for (in_dex in 1:NROW(col_umn))
    out_put <- out_put + col_umn[in_dex]
  out_put
}  # end agg_regate
# perform parallel aggregations over columns of matrix
agg_regations <-
  parCapply(clus_ter, mat_rix, agg_regate)
# compare speed of apply with parallel computing
summary(microbenchmark(
  ap_ply=apply(mat_rix, MARGIN=2, agg_regate),
  parl_apply=
    parCapply(clus_ter, mat_rix, agg_regate),
  times=10)
)[, c(1, 4, 5)]
# stop R processes over cluster under Windows
stopCluster(clus_ter)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Initializing Parallel Clusters Under \protect\emph{Windows}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Under \emph{Windows} the child processes in the parallel compute cluster don't inherit data and objects from their parent process,
      \vskip1ex
      Therefore the required data must be either passed into \texttt{parLapply()} via the dots \texttt{"..."} argument, or by calling the function \texttt{clusterExport()},
      \vskip1ex
      Objects from packages must be either referenced using the double-colon operator \texttt{"::"}, or the packages must be loaded in the child processes,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:5)),eval=FALSE>>=
library(parallel)  # load package parallel
# calculate number of available cores
num_cores <- detectCores() - 1
# initialize compute cluster under Windows
clus_ter <- makeCluster(num_cores)
ba_se <- 2
# fails because child processes don't know ba_se:
parLapply(clus_ter, 2:4,
          function(exponent) ba_se^exponent)
# ba_se passed to child via dots ... argument:
parLapply(clus_ter, 2:4,
          function(exponent, ba_se) ba_se^exponent,
          ba_se=ba_se)
# ba_se passed to child via clusterExport:
clusterExport(clus_ter, "ba_se")
parLapply(clus_ter, 2:4,
          function(exponent) ba_se^exponent)
# fails because child processes don't know zoo::index():
parSapply(clus_ter, c("VTI", "IEF", "DBC"),
          function(sym_bol)
            NROW(index(get(sym_bol, envir=rutils::env_etf))))
# zoo function referenced using "::" in child process:
parSapply(clus_ter, c("VTI", "IEF", "DBC"),
          function(sym_bol)
            NROW(zoo::index(get(sym_bol, envir=rutils::env_etf))))
# package zoo loaded in child process:
parSapply(clus_ter, c("VTI", "IEF", "DBC"),
          function(sym_bol) {
            stopifnot("package:zoo" %in% search() || require("zoo", quietly=TRUE))
            NROW(index(get(sym_bol, envir=rutils::env_etf)))
          })  # end parSapply
# stop R processes over cluster under Windows
stopCluster(clus_ter)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reproducible Parallel Simulations Under \protect\emph{Windows}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Simulations use pseudo-random number generators, and in order to perform reproducible results, they must set the \emph{seed} value, so that the number generators produce the same sequence of pseudo-random numbers,
      \vskip1ex
      The function \texttt{set.seed()} initializes the random number generator by specifying the \emph{seed} value, so that the number generator produces the same sequence of numbers for a given \emph{seed} value,
      \vskip1ex
      But under \emph{Windows} \texttt{set.seed()} doesn't initialize the random number generators of child processes, and they don't produce the same sequence of numbers,
      \vskip1ex
      The function \texttt{clusterSetRNGStream()} initializes the random number generators of child processes under \emph{Windows},
      \vskip1ex
      The function \texttt{set.seed()} does initialize the random number generators of child processes under \emph{Mac-OSX} and \emph{Linux},
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # load package parallel
# calculate number of available cores
num_cores <- detectCores() - 1
# initialize compute cluster under Windows
clus_ter <- makeCluster(num_cores)
# set seed for cluster under Windows
# doesn't work: set.seed(1121)
clusterSetRNGStream(clus_ter, 1121)
# perform parallel loop under Windows
out_put <- parLapply(clus_ter, 1:70, rnorm, n=100)
sum(unlist(out_put))
# stop R processes over cluster under Windows
stopCluster(clus_ter)
# perform parallel loop under Mac-OSX or Linux
out_put <- mclapply(1:10, rnorm, mc.cores=num_cores, n=100)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Credit Portfolio Models}


%%%%%%%%%%%%%%%
\subsection{Simulating Single-period Defaults}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Consider a portfolio of credit assets (bonds or loans) over a single period of time,
      \vskip1ex
      At the end of the period, some of the assets default, while the rest don't,
      \vskip1ex
      The default probabilities are equal to $p_i$, 
      \vskip1ex
      Individual defaults can be simulated by comparing the probabilities $p_i$ with the uniform random numbers $u_i$, 
      \vskip1ex
      Default occurs if $u_i$ is less than the default probability $p_i$:
      \begin{displaymath}
        u_i < p_i
      \end{displaymath}
      Simulations in \texttt{R} can be accelerated by pre-computing a vector of random numbers, instead of generatng them one at a time in a loop,
      \vskip1ex
      Vectors of random numbers allow using \emph{vectorized} functions, instead of inefficient (slow) \texttt{for()} loops,
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# calculate random default probabilities
num_assets <- 100
default_probs <- runif(num_assets, max=0.2)
mean(default_probs)
# calculate number of defaults
uni_form <- runif(num_assets)
sum(uni_form < default_probs)
# simulate average number of defaults
num_simu <- 1000
de_faults <- numeric(num_simu)
# simulate using for() loop (inefficient way)
for (i in 1:num_simu) {  # perform loop
  uni_form <- runif(num_assets)
  de_faults[i] <- sum(uni_form < default_probs)
}  # end for
# calculate average number of defaults
mean(de_faults)
# simulate using vectorized functions  (efficient way)
uni_form <- matrix(runif(num_simu*num_assets), 
                   ncol=num_simu)
sum(uni_form < default_probs)/num_simu
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Asset Values and Default Thresholds}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Defaults can also be simulated using normally distributed variables $a_i$ called \emph{asset values}, instead of uniformly distributed variables,
      \vskip1ex
      These asset values are mathematical variables, which can have negative values, so they are not related to actual company asset values, but may be thought of as related to the balance of company assets minus its liabilities,
      \vskip1ex
      Default occurs if the \emph{asset value} $a_i$ is less than the \emph{default threshold} $t_i$:
      \begin{displaymath}
        a_i < t_i
      \end{displaymath}
      The default threshold is equal to $t_i = \Phi^{-1}(p_i)$, where $p_i$ is the default probability, and $\Phi()$ is the cumulative Standard Normal distribution, 
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/default_threshold.png}
    \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot Standard Normal distribution
curve(expr=dnorm(x),
      type="l", xlim=c(-4, 4),
      xlab="asset value", ylab="", lwd=2,
      col="blue", main="Distribution of Asset Values")
abline(v=qnorm(0.025), col="red", lwd=2)
text(x=qnorm(0.025)-0.1, y=0.15,
       labels="default threshold",
       lwd=2, srt=90, pos=3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vasicek Model of Correlated Asset Values}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Under the \emph{Vasicek} single factor model, the asset value $a_i$ is equal to the sum of a \emph{systematic} factor $s$, plus an \emph{idiosyncratic} factor $z_i$: 
      \begin{displaymath}
        a_i = \sqrt{\rho} s + \sqrt{1-\rho} z_i
      \end{displaymath}
      Where $\rho$ is the correlation between asset values, 
      \vskip1ex
      The variables $s$, $z_i$, and $a_i$ all follow the Standard Normal distribution $N(0, 1)$, 
      \vskip1ex
      The \emph{Vasicek} model resembles the \emph{CAPM} model, with the asset value (not the asset returns) equal to the sum of a \emph{systematic} factor plus an \emph{idiosyncratic} factor,
      \vskip1ex
      The Bank for International Settlements (BIS) uses the \emph{Vasicek} model as part of its regulatory capital requirements for credit risk:\\
\hskip1em\url{http://bis2information.org/content/Vasicek_model}\\
\hskip1em\url{https://www.bis.org/bcbs/basel3.htm}\\
\hskip1em\url{https://www.bis.org/bcbs/irbriskweight.pdf}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# define correlation parameters
rh_o <- 0.2
rho_sqrt <- sqrt(rh_o) ; rho_sqrtm <- sqrt(1-rh_o)
num_assets <- 5 ; num_simu <- 10000
# calculate vector of systematic factors
system_atic <- rnorm(num_simu)
# simulate asset values using vectorized functions (efficient way)
asset_values <- rho_sqrt*system_atic + 
  rho_sqrtm*rnorm(num_simu*num_assets)
dim(asset_values) <- c(num_simu, num_assets)
# calculate correlations between asset values
cor(asset_values)
# simulate asset values using for() loop (inefficient way)
# allocate matrix of assets
asset_values <- matrix(nr=num_simu, nc=num_assets)
# simulate asset values using for() loop
for (i in 1:num_simu) {  # perform loop
  asset_values[i, ] <- 
    rho_sqrt*system_atic[i] + 
    rho_sqrtm*rnorm(num_assets)
}  # end for
cor(asset_values)
# benchmark the speed of the two methods
library(microbenchmark)
summary(microbenchmark(
  for_loop={for (i in 1:num_simu) {
    rho_sqrt*system_atic[i] + 
    rho_sqrtm*rnorm(num_assets)}},
  vector_ized={rho_sqrt*system_atic + 
              rho_sqrtm*rnorm(num_simu*num_assets)},
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vasicek Model of Correlated Defaults}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Under the \emph{Vasicek} model, default occurs if the \emph{asset value} $a_i$ is less than the \emph{default threshold} $t_i$:
      \begin{align*}
        a_i = \sqrt{\rho} s + \sqrt{1-\rho} z_i \\
        a_i < t_i
      \end{align*}
      The \emph{systematic} factor $s$ may be considered to represent the state of the macro economy, with positive values representing an economic expansion, and negative values representing an economic recession, 
      \vskip1ex
      When the value of the \emph{systematic} factor $s$ is positive, then the asset values will all tend to be bigger as well, which will produce fewer defaults, 
      \vskip1ex
      But when the \emph{systematic} factor is negative, then the asset values will tend to be smaller, which will produce more defaults, 
      \vskip1ex
      This way the \emph{Vasicek} model introduces a correlation among defaults,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# calculate random default probabilities
num_assets <- 5
default_probs <- runif(num_assets, max=0.2)
mean(default_probs)
# calculate default thresholds
default_thresh <- qnorm(default_probs)
# calculate number of defaults using vectorized functions (efficient way)
# calculate vector of number of defaults
de_faults <- 
  colSums(t(t(asset_values) < default_thresh))
de_faults / num_simu
default_probs
# calculate number of defaults using for() loop (inefficient way)
# allocate matrix of de_faults
de_faults <- matrix(nr=num_simu, nc=num_assets)
# simulate asset values using for() loop
for (i in 1:num_simu) {  # perform loop
  de_faults[i, ] <- 
    (asset_values[i, ] < default_thresh)
}  # end for
colSums(de_faults) / num_simu
default_probs
# calculate correlations between defaults
cor(de_faults)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Asset Correlation and Default Correlation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Default correlation is defined as the correlation between the \emph{Boolean} vectors of default events, 
      \vskip1ex
      The \emph{Vasicek} model introduces correlation among default events, through the correlation of \emph{asset values},
      \vskip1ex
      If \emph{asset values} have a positive correlation, then the defaults among credits are clustered together, and if one credit defaults then the other credits are more likely to default as well,
      \vskip1ex
      Empirical studies have found that the asset correlation $\rho$ can vary between \texttt{5\%} to \texttt{20\%}, depending on the default risk, 
      \vskip1ex
      Credits with higher default risk tend to also have higher asset correlation, since they are more  sensitive to the economic conditions,
      \vskip1ex
      Default correlations are usually much lower than the corresponding asset correlations,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# define default probabilities
num_assets <- 2
default_prob <- 0.2
default_thresh <- qnorm(default_prob)
# define correlation parameters
rh_o <- 0.2
rho_sqrt <- sqrt(rh_o) ; rho_sqrtm <- sqrt(1-rh_o)
# calculate vector of systematic factors
num_simu <- 1000
system_atic <- rnorm(num_simu)
# simulate asset values using vectorized functions
asset_values <- rho_sqrt*system_atic + 
  rho_sqrtm*rnorm(num_simu*num_assets)
dim(asset_values) <- c(num_simu, num_assets)
# calculate number of defaults using vectorized functions
de_faults <- t(t(asset_values) < default_thresh)
# calculate correlations between defaults
cor(de_faults)
# calculate averaage number of defaults and compare to default_prob
colSums(de_faults) / num_simu
default_prob
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Cumulative Defaults Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If all the default probabilities are the same $p_i=p$, then the default threshold is equal to $t=N^{-1}(p)$, and the conditional default probability $p(s)$, given the systematic factor $s$, is equal to:
      \begin{displaymath}
        p(s) = N(\frac{t - \sqrt{\rho} s}{\sqrt{1-\rho}})
      \end{displaymath}
      The cumulative probability $P(x)$ for the percentage \texttt{x} of portfolio defaults (the portfolio cumulative default distribution) is equal to:
      \begin{displaymath}
        P(x) = N(\frac{{\sqrt{1-\rho}} N^{-1}(x) - t}{\sqrt{\rho}})
      \end{displaymath}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# define cumulative default probability function
def_prob <- function(x, def_thresh=qnorm(0.1), rh_o=0.1)
  pnorm((sqrt(1-rh_o)*qnorm(x) - def_thresh)/sqrt(rh_o))
def_prob(x=0.2, def_thresh=qnorm(0.2), rh_o=0.2)
# plot cumulative default probability function
curve(expr=def_prob(x, def_thresh=qnorm(0.4), rh_o=0.05),
      xlim=c(0, 0.999), lwd=3,
      xlab="percent default", ylab="probability", 
      col="green", main="Cumulative Default Probabilities")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vasicek_cum_def.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot default distribution with higher correlation
curve(expr=def_prob(x, def_thresh=qnorm(0.4), rh_o=0.2),
      xlim=c(0, 0.999), add=TRUE, lwd=3,
      col="blue", main="")
# add legend
legend(x="topleft", 
       legend=c("high correlation", "low correlation"),
       title=NULL, inset=0.05, cex=0.8, bg="white", 
       bty="n", lwd=6, lty=c(1, 1), col=c("blue", "green"))
# add unconditional default probability
abline(v=0.4, col="red", lwd=3)
text(x=0.4, y=0.0,
       labels="default probability",
       lwd=2, srt=90, pos=4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Defaults Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The probability density $f(x)$ of portfolio defaults is equal to the derivative of the cumulative default distribution:
      \begin{multline*}
        \hspace{-1.7em}f(x) = \frac{\sqrt{1-\rho}}{\sqrt{\rho}} \exp(-\frac{1}{2 \rho} ({\sqrt{1-\rho}} N^{-1}(x) - t)^2 + \\ \frac{1}{2} {N^{-1}(x)^2)}
      \end{multline*}
      <<echo=TRUE,eval=FALSE>>=
# define default probability density function
vasi_cek <- function(x, def_thresh=-2, rh_o=0.1)
  sqrt((1-rh_o)/rh_o)*exp(-(sqrt(1-rh_o)*qnorm(x) - 
  def_thresh)^2/(2*rh_o) + qnorm(x)^2/2)
vasi_cek(0.03, def_thresh=qnorm(0.025), rh_o=0.1)
# plot probability distribution of defaults
curve(expr=vasi_cek(x, def_thresh=qnorm(0.025), rh_o=0.02),
      xlim=c(0, 0.1), lwd=3,
      xlab="percentage of defaults", ylab="density", 
      col="green", main="Distribution of Defaults")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vasicek_distr_def.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot default distribution with higher correlation
curve(expr=vasi_cek(x, def_thresh=qnorm(0.025), rh_o=0.1),
      xlab="default percentage", ylab="", 
      add=TRUE, lwd=3, col="blue", main="")
# add legend
legend(x="topright", 
       legend=c("high correlation", "low correlation"),
       title=NULL, inset=0.05, cex=0.8, bg="white",
       bty="n", lwd=6, lty=c(1, 1), col=c("blue", "green"))
# add unconditional default probability
abline(v=0.025, col="red", lwd=3)
text(x=0.023, y=8,
       labels="default probability",
       lwd=2, srt=90, pos=3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Defaults Under Extreme Correlations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the correlation $\rho$ is close to \emph{zero}, then the asset values $a_i$ are independent from each other, and defaults are also independent, so that the percentage of portfolio defaults is very close to the default probability $p$,
      \vskip1ex
      In that case, the probability density of portfolio defaults is very narrow and is centered on the default probability $p$,
      \vskip1ex
      If the correlation $\rho$ is close to \emph{one}, then the asset values $a_i$ are almost the same, and defaults occur at the same time, so that the percentage of portfolio defaults is either \emph{zero} or \emph{one},
      \vskip1ex
      In that case, the probability density of portfolio defaults becomes \emph{bimodal}, with two peaks around  \emph{zero} and \emph{one},
      <<echo=TRUE,eval=FALSE>>=
# plot default distribution with low correlation
curve(expr=vasi_cek(x, def_thresh=qnorm(0.1), rh_o=0.01),
      xlab="default percentage", ylab="", lwd=2, 
      col="green", main="Distribution of Defaults")
# plot default distribution with high correlation
curve(expr=vasi_cek(x, def_thresh=qnorm(0.1), rh_o=0.99),
      xlab="percentage of defaults", ylab="density", 
      add=TRUE, lwd=2, n=10001, col="blue", main="")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vasicek_high_corr.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# add legend
legend(x="top", 
       legend=c("high correlation", "low correlation"),
       title=NULL, inset=0.1, cex=0.8, bg="white",
       bty="n", lwd=6, lty=c(1, 1), col=c("blue", "green"))
# add unconditional default probability
abline(v=0.1, col="red", lwd=2)
text(x=0.1, y=10, lwd=2, pos=4,
       labels="default probability")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Loss Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Expected Loss (\emph{EL}) of a credit portfolio is equal to the sum of default probabilities ($p_i$) multiplied by the loss given default (\emph{LGD}, also known as the loss severity): 
      \begin{displaymath}
        EL = \sum_{i=1}^{n} p_i LGD_i
      \end{displaymath}
      If the \emph{LGD} amounts are all the same, then the \emph{portfolio loss distribution} can be obtained from the \emph{default distribution}, adjusted for the \emph{LGD}:
      \begin{multline*}
        \hspace{-1.7em}f(x) = \frac{\sqrt{1-\rho}}{LGD \sqrt{\rho}} \exp(-\frac{1}{2 \rho} ({\sqrt{1-\rho}} N^{-1}(\frac{x}{LGD}) - t)^2 + \\ \frac{1}{2} {N^{-1}(\frac{x}{LGD}))^2}
      \end{multline*}
      <<echo=TRUE,eval=FALSE>>=
# define Vasicek loss distribution density function
portf_loss <- function(x, def_thresh=-2, rh_o=0.1, l_gd=0.4)
  sqrt((1-rh_o)/rh_o)*exp(-(sqrt(1-rh_o)*qnorm(x/l_gd) - def_thresh)^2/(2*rh_o) + qnorm(x/l_gd)^2/2)/l_gd
integrate(portf_loss, low=0, up=0.3, 
  def_thresh=-2, rh_o=0.1, l_gd=0.4)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vasicek_loss_distr.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot probability distribution of losses
curve(expr=portf_loss(x, def_thresh=qnorm(0.06), rh_o=0.1),
      type="l", xlim=c(0, 0.06), 
      xlab="loss percentage", ylab="density", lwd=3,
      col="orange", main="Distribution of Losses")
# add line for expected loss
abline(v=0.02, col="red", lwd=3)
text(x=0.02-0.001, y=10, labels="expected loss",
       lwd=2, srt=90, pos=3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Value at Risk}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Value at Risk (\emph{VaR}) measures extreme portfolio loss (but not the worst possible loss), defined as the \emph{quantile} of the loss distribution, corresponding to a given confidence level $\alpha$, 
      \vskip1ex
      A loss exceeding the \emph{EL} is called the Unexpected Loss (\emph{UL}), and can be calculated from the \emph{portfolio loss distribution}, 
      <<echo=TRUE,eval=FALSE>>=
# add lines for unexpected loss
abline(v=0.04, col="blue", lwd=3)
arrows(x0=0.02, y0=35, x1=0.04, y1=35, 
       code=3, lwd=3, cex=0.5)
text(x=0.03, y=36, labels="unexpected loss", 
     lwd=2, pos=3)
# add lines for VaR
abline(v=0.055, col="red", lwd=3)
arrows(x0=0.0, y0=25, x1=0.055, y1=25, 
       code=3, lwd=3, cex=0.5)
text(x=0.03, y=26, labels="VaR", lwd=2, pos=3)
text(x=0.055-0.001, y=10, labels="VaR",
       lwd=2, srt=90, pos=3)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/vasicek_distr_var.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Conditional Value at Risk}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Conditional Value at Risk (\emph{CVaR}) is equal to the average of the \emph{VaR} for confidence levels less than a given confidence level $\alpha$: 
      \begin{displaymath}
        \mathrm{CVaR} = \frac{1}{\alpha} \int_{0}^\alpha \mathrm{VaR}(p) \, \mathrm{d}p
      \end{displaymath}
      The Conditional Value at Risk is also called the Expected Shortfall (\emph{ES}), or Expected Tail Loss (\emph{ETL}), 
      <<echo=TRUE,eval=FALSE>>=
# plot probability distribution of losses
curve(expr=portf_loss(x, def_thresh=qnorm(0.1), rh_o=0.1),
      type="l", xlim=c(0, 0.06), 
      xlab="loss percentage", ylab="density", lwd=3,
      col="orange", main="Conditional Value at Risk")
# add line for expected loss
abline(v=0.02, col="red", lwd=3)
text(x=0.02-0.001, y=10, labels="expected loss",
       lwd=2, srt=90, pos=3)
# add lines for VaR
abline(v=0.04, col="red", lwd=3)
text(x=0.04-0.001, y=10, labels="VaR",
       lwd=2, srt=90, pos=3)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vasicek_distr_cvar.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# add shading for CVaR
va_r <- 0.04; var_max <- 0.07
var_s <- seq(va_r, var_max, length=100)
dens_ity <- sapply(var_s, portf_loss, 
  def_thresh=qnorm(0.1), rh_o=0.1)
# draw shaded polygon
polygon(c(va_r, var_s, var_max),
  c(-1, dens_ity, -1), col="red", border=NA)
text(x=0.045, y=0, labels="CVaR", lwd=2, pos=3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Value at Risk Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Value at Risk (\emph{VaR}) measures extreme portfolio loss (but not the worst possible loss), defined as the \emph{quantile} of the loss distribution, corresponding to a given confidence level $\alpha$, 
      \vskip1ex
      The \emph{quantile} of the loss distribution (the \emph{VaR}), for a given a confidence level $\alpha$, is given by the inverse of the cumulative loss distribution:
      \begin{displaymath}
        VaR(\alpha) = LGD \cdot N(\frac{{\sqrt{\rho}} N^{-1}(\alpha) + t}{\sqrt{1-\rho}})
      \end{displaymath}
      <<echo=TRUE,eval=FALSE>>=
# VaR (quantile of the loss distribution)
var_func <- function(x, def_thresh=qnorm(0.1), rh_o=0.1, l_gd=0.4)
  l_gd*pnorm((sqrt(rh_o)*qnorm(x) + def_thresh)/sqrt(1-rh_o))
var_func(x=0.99, def_thresh=qnorm(0.1), rh_o=0.2, l_gd=0.4)
# plot VaR
curve(expr=var_func(x, def_thresh=qnorm(0.1), rh_o=0.1, l_gd=0.4),
      type="l", xlim=c(0, 0.999), 
      xlab="confidence level", ylab="VaR", lwd=3,
      col="orange", main="VaR versus Confidence Level")
# add line for expected loss
abline(h=0.04, col="red", lwd=3)
text(x=0.2, y=0.04, labels="expected loss",
     lwd=2, pos=3)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/vasicek_var.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Value at Risk and Confidence Levels}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The confidence levels of \emph{VaR} values can also be calculated by integrating over the tail of the loss density function, 
      <<echo=TRUE,eval=FALSE>>=
# integrate portf_loss() over full range
integrate(portf_loss, low=0.0, up=0.3, 
          def_thresh=qnorm(0.1), rh_o=0.1, l_gd=0.4)
# calculate expected losses using portf_loss()
integrate(function(x, ...) x*portf_loss(x, ...), 
          low=0.0, up=0.3, 
          def_thresh=qnorm(0.1), rh_o=0.1, l_gd=0.4)
# calculate confidence levels corresponding to VaR values
var_s <- seq(0.07, 0.12, 0.001)
conf_levels <- sapply(var_s, function(va_r, ...) {
  integrate(portf_loss, low=va_r, up=0.3, ...)
}, def_thresh=qnorm(0.1), rh_o=0.1, l_gd=0.4)  # end sapply
conf_levels <- cbind(as.numeric(t(conf_levels)[, 1]), var_s)
colnames(conf_levels) <- c("conf_levels", "VaRs")
# calculate 95% confidence level VaR value
conf_levels[
  match(TRUE, conf_levels[, "conf_levels"] < 0.05), "VaRs"]
plot(x=1-conf_levels[, "conf_levels"],
     y=conf_levels[, "VaRs"], lwd=2,
     xlab="conf_levels", ylab="VaRs",
     t="l", main="VaR values and confidence levels")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/vasicek_var_conf.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Conditional Value at Risk Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{CVaR} values can be calculated by integrating over the tail of the loss density function, 
      <<echo=TRUE,eval=FALSE>>=
# calculate CVaR values
cvar_s <- sapply(var_s, function(va_r, ...) {
  integrate(function(x, ...) x*portf_loss(x, ...), 
            low=va_r, up=0.3, ...)
}, def_thresh=qnorm(0.1), rh_o=0.1, l_gd=0.4)  # end sapply
conf_levels <- cbind(conf_levels, as.numeric(t(cvar_s)[, 1]))
colnames(conf_levels)[3] <- "CVaRs"
# divide CVaR by confidence level
conf_levels[, "CVaRs"] <- 
  conf_levels[, "CVaRs"]/conf_levels[, "conf_levels"]
# calculate 95% confidence level CVaR value
conf_levels[match(TRUE, 
  conf_levels[, "conf_levels"] < 0.05), "CVaRs"]
# plot CVaRs
plot(x=1-conf_levels[, "conf_levels"],
     y=conf_levels[, "CVaRs"], 
     t="l", col="red", lwd=2, 
     ylim=range(conf_levels[, c("VaRs", "CVaRs")]), 
     xlab="conf_levels", ylab="CVaRs",
     main="CVaR values and confidence levels")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/cvar_conf_levels.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# add VaRs
lines(x=1-conf_levels[, "conf_levels"],
      y=conf_levels[, "VaRs"], lwd=2)
# add legend
legend(x="topleft", legend=c("CVaRs", "VaRs"),
       title="default probability = 10%
correlation = 10%
loss given default = 40%", 
       inset=0.1, cex=0.8, bg="white", bty="n",
       lwd=6, lty=c(1, 1), col=c("red", "black"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating \protect\emph{VaR} Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the default probabilities $p_i$ are not all the same, then there's no formula for the \emph{portfolio loss distribution} under the Vasicek Model, 
      \vskip1ex
      In that case the portfolio losses and \emph{VaR} must be simulated,
      <<echo=TRUE,eval=FALSE>>=
# Define model parameters
num_assets <- 300
num_simu <- 1000
l_gd <- 0.4
# define correlation parameters
rh_o <- 0.2
rho_sqrt <- sqrt(rh_o)
rho_sqrtm <- sqrt(1-rh_o)
# calculate default probabilities and thresholds
set.seed(1121)
default_probs <- runif(num_assets, max=0.2)
default_thresh <- qnorm(default_probs)
# calculate vector of systematic factors
system_atic <- rnorm(num_simu)
# simulate losses under Vasicek model
asset_values <- matrix(rnorm(num_simu*num_assets), ncol=num_simu)
asset_values <- t(rho_sqrt*system_atic + t(rho_sqrtm*asset_values))
loss_es <- 
  l_gd*colSums(asset_values < default_thresh)/num_assets
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/vasicek_var_simu.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# calculate VaRs
conf_levels <- seq(0.93, 0.99, 0.01)
var_s <- quantile(loss_es, probs=conf_levels)
plot(x=conf_levels, y=var_s, t="l", lwd=2,
     main="Simulated VaR and confidence levels")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating \protect\emph{CVaR} Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{CVaR} can be calculated from the frequency of tail losses in excess of the \emph{VaR}, 
      \vskip1ex
      The function \texttt{table()} calculates the frequency distribution of categorical data,
      <<echo=TRUE,eval=FALSE>>=
# calculate CVaRs
cvar_s <- sapply(var_s, function(va_r) {
  mean(loss_es[loss_es>va_r])
})  # end sapply
cvar_s <- cbind(cvar_s, var_s)
# alternative CVaR calculation using frequency table
# first calculate frequency table of loss_es
table_losses <- table(loss_es)/num_simu
# calculate CVaRs from frequency table
cvar_s <- sapply(var_s, function(va_r) {
  tai_l <- table_losses[names(table_losses) > va_r]
  tai_l %*% as.numeric(names(tai_l)) / sum(tai_l)
})  # end sapply
# plot CVaRs
plot(x=rownames(cvar_s), y=cvar_s[, "cvar_s"], 
     t="l", col="red", lwd=2, 
     ylim=range(cvar_s), 
     xlab="conf_levels", ylab="CVaRs",
     main="Simulated CVaR and confidence levels")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vasicek_cvar_simu.png}
    \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# add VaRs
lines(x=rownames(cvar_s), y=cvar_s[, "var_s"], lwd=2)
# add legend
legend(x="topleft", legend=c("CVaRs", "VaRs"), bty="n",
       title=NULL, inset=0.05, cex=0.8, bg="white",
       lwd=6, lty=c(1, 1), col=c("red", "black"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Function for Simulating \protect\emph{VaR} Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The function \texttt{calc\_var()} simulates default losses under the \emph{Vasicek} model, and calculates a vector of \emph{VaR} and \emph{CVaR} values, 
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
calc_var <- function(default_thresh, 
                     l_gd=0.6, 
                     rho_sqrt,
                     rho_sqrtm,
                     num_simu=1000,
                     conf_levels=seq(0.93, 0.99, 0.01)) {
  # Define model parameters
  num_assets <- NROW(default_thresh)
  # Simulate losses under Vasicek model
  system_atic <- rnorm(num_simu)
  asset_values <- matrix(rnorm(num_simu*num_assets), ncol=num_simu)
  asset_values <- t(rho_sqrt*system_atic + t(rho_sqrtm*asset_values))
  loss_es <- l_gd*colSums(asset_values < default_thresh)/num_assets
  # Calculate VaRs and CVaRs
  var_s <- quantile(loss_es, probs=conf_levels)
  cvar_s <- sapply(var_s, function(va_r) {
    mean(loss_es[loss_es>va_r])
  })  # end sapply
  names(cvar_s) <- names(var_s)
  c(var_s, cvar_s)
}  # end calc_var
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of \protect\emph{VaR} Using Bootstrap Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{scaled} standard errors of \emph{VaR} and \emph{CVaR} increase with the confidence level, making them much less reliable at very high confidence levels,
      <<echo=TRUE,eval=FALSE>>=
# define number of bootstrap simulations
num_boot <- 500
num_assets <- NROW(default_probs)
# perform bootstrap of calc_var
set.seed(1121)
boot_strap <- sapply(rep(l_gd, num_boot), 
  calc_var, 
  default_thresh=qnorm(default_probs),
  rho_sqrt=rho_sqrt,
  rho_sqrtm=rho_sqrtm,
  num_simu=num_simu,
  conf_levels=conf_levels)  # end sapply
boot_strap <- t(boot_strap)
# calculate vectors of standard errors of VaR and CVaR from boot_strap data
std_error_var <- apply(boot_strap[, 1:7], MARGIN=2, 
    function(x) c(mean=mean(x), sd=sd(x)))
std_error_cvar <- apply(boot_strap[, 8:14], MARGIN=2, 
    function(x) c(mean=mean(x), sd=sd(x)))
# scale the standard errors of VaRs and CVaRs
std_error_var[2, ] <- std_error_var[2, ]/std_error_var[1, ]
std_error_cvar[2, ] <- std_error_cvar[2, ]/std_error_cvar[1, ]
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vasicek_cvar_stderror.png}
    \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot the standard errors of VaRs and CVaRs
plot(x=colnames(std_error_cvar),
  y=std_error_cvar[2, ], t="l", col="red", lwd=2, 
  ylim=range(c(std_error_var[2, ], std_error_cvar[2, ])), 
  xlab="conf_levels", ylab="CVaRs",
  main="Scaled standard errors of CVaR and VaR")
lines(x=colnames(std_error_var), y=std_error_var[2, ], lwd=2)
legend(x="topleft", legend=c("CVaRs", "VaRs"), bty="n",
       title=NULL, inset=0.05, cex=0.8, bg="white",
       lwd=6, lty=c(1, 1), col=c("red", "black"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of \protect\emph{VaR} Using Parallel Bootstrap}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{scaled} standard errors of \emph{VaR} and \emph{CVaR} increase with the confidence level, making them much less reliable at very high confidence levels,
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # load package parallel
num_cores <- detectCores() - 1  # number of cores
clus_ter <- makeCluster(num_cores)  # initialize compute cluster
# perform bootstrap of calc_var for Windows
set.seed(1121)
boot_strap <- parLapply(clus_ter, rep(l_gd, num_boot), 
  fun=calc_var, default_probs=default_probs,
  rh_o=rh_o, num_simu=num_simu,
  conf_levels=conf_levels)  # end parLapply
# bootstrap under Mac-OSX or Linux
boot_strap <- mclapply(rep(l_gd, num_boot), 
  FUN=calc_var, default_probs=default_probs,
  rh_o=rh_o, num_simu=num_simu,
  conf_levels=conf_levels)  # end mclapply
boot_strap <- rutils::do_call(rbind, boot_strap)
stopCluster(clus_ter)  # stop R processes over cluster
# calculate vectors of standard errors of VaR and CVaR from boot_strap data
std_error_var <- apply(boot_strap[, 1:7], MARGIN=2, 
    function(x) c(mean=mean(x), sd=sd(x)))
std_error_cvar <- apply(boot_strap[, 8:14], MARGIN=2, 
    function(x) c(mean=mean(x), sd=sd(x)))
# scale the standard errors of VaRs and CVaRs
std_error_var[2, ] <- std_error_var[2, ]/std_error_var[1, ]
std_error_cvar[2, ] <- std_error_cvar[2, ]/std_error_cvar[1, ]
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vasicek_cvar_stderror_parallel.png}
    \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot the standard errors of VaRs and CVaRs
plot(x=colnames(std_error_cvar),
  y=std_error_cvar[2, ], t="l", col="red", lwd=2, 
  ylim=range(c(std_error_var[2, ], std_error_cvar[2, ])), 
  xlab="conf_levels", ylab="CVaRs",
  main="Scaled standard errors of CVaR and VaR")
lines(x=colnames(std_error_var), y=std_error_var[2, ], lwd=2)
legend(x="topleft", legend=c("CVaRs", "VaRs"), bty="n",
       title=NULL, inset=0.05, cex=0.8, bg="white",
       lwd=6, lty=c(1, 1), col=c("red", "black"))
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Homework Assignment}


%%%%%%%%%%%%%%%
\subsection{Homework Assignment}
\begin{frame}[t]{\secname}
\vspace{-1em}
\begin{block}{Required}
  \begin{itemize}[]
    \item Read all the lecture slides in \emph{FRE6871\_Lecture\_2.pdf}, and run all the code in \emph{FRE6871\_Lecture\_2.R}
  \end{itemize}
\end{block}
\begin{block}{Recommended}
  \begin{itemize}[]
    \item Read about the \emph{Vasicek} single factor model in \emph{Vasicek Portfolio Default Distribution.pdf}
  \end{itemize}
\end{block}

\end{frame}


\end{document}
