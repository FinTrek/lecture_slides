% FRE7241_Lecture_3

% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='scriptsize', fig.width=4, fig.height=4)
options(width=60, dev='pdf')
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
% \usepackage{mathtools}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape,bg=red,fg=red}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[FRE7241 Lecture\#3]{FRE7241 Algorithmic Portfolio Management}
\subtitle{Lecture\#3, Fall 2016}
% \subject{Getting Started With R}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{September 20, 2016}
% \date{\today}
% \pgfdeclareimage[height=0.5cm]{university-logo}{engineering_long_white}
% \logo{\pgfuseimage{engineering_long_white}}


%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Probability and Statistics}


%%%%%%%%%%%%%%%
\subsection{Generating Pseudo-Random Numbers}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      Random number generators produce the same deterministic sequence of numbers after their \texttt{seed} value is reset,
      \vskip1ex
      The function \texttt{set.seed()} initializes the random number generator by specifying the \texttt{seed} value,
      \vskip1ex
      The function \texttt{runif()} produces random numbers from the \emph{uniform} distribution,
      \vskip1ex
      The function \texttt{rnorm()} produces random numbers from the \emph{normal} distribution,
      \vskip1ex
      The function \texttt{pnorm()} calculates the cumulative \emph{normal} distribution,
      \vskip1ex
      The function \texttt{qnorm()} calculates the inverse cumulative \emph{normal} distribution,
    \column{0.6\textwidth}
      \vspace{-1em}
        <<>>=
set.seed(1121)  # reset random number generator
runif(3)  # three random numbers from the uniform distribution
runif(3)  # produce another three numbers
set.seed(1121)  # reset random number generator
runif(3)  # produce another three numbers

# produce random number from standard normal distribution
rnorm(1)
# produce five random numbers from standard normal distribution
rnorm(5)
# produce five random numbers from the normal distribution
rnorm(n=5, mean=1, sd=2)  # match arguments by name
# calculate cumulative standard normal distribution
c(pnorm(-2), pnorm(2))
# calculate inverse cumulative standard normal distribution
c(qnorm(0.75), qnorm(0.25))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generating Binomial Random Numbers}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      A \emph{binomial} trial is a coin flip, that results in either a success or failure,
      \vskip1ex
      The \emph{binomial} distribution specifies the probability of obtaining a certain number of successes in a sequence of independent \emph{binomial} trials,
      \vskip1ex
      Let $p$ be the probability of obtaining a success in a \emph{binomial} trial, and let $(1-p)$ be the probability of failure,
      \vskip1ex
      $p = 0.5$ corresponds to flipping an unbiased coin,
      \vskip1ex
      The probability of obtaining $k$ successes in $n$ independent \emph{binomial} trials is equal to:
      \begin{displaymath}
        {n \choose k} p^k (1-p)^{(n-k)}
      \end{displaymath}
      The function \texttt{rbinom()} produces random numbers from the \emph{binomial} distribution,
    \column{0.6\textwidth}
      \vspace{-1em}
        <<>>=
set.seed(1121)  # reset random number generator
# flip unbiased coin once, 20 times
rbinom(n=20, size=1, 0.5)
# number of heads after flipping twice, 20 times
rbinom(n=20, size=2, 0.5)
# number of heads after flipping thrice, 20 times
rbinom(n=20, size=3, 0.5)
# number of heads after flipping biased coin thrice, 20 times
rbinom(n=20, size=3, 0.8)
# number of heads after flipping biased coin thrice, 20 times
rbinom(n=20, size=3, 0.2)
# flip unbiased coin once, 20 times
sample(x=0:1, size=20, replace=TRUE)  # fast
as.numeric(runif(20) < 0.5)  # slower
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generating Random Samples and Permutations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{sample} is a subset of elements taken from a set of data elements,
      \vskip1ex
      The function \texttt{sample()} produces a random sample form a vector of data elements,
      \vskip1ex
      By default the \emph{size} of the sample (the \texttt{size} argument) is equal to the number of elements in the data vector,
      \vskip1ex
      So the call \texttt{sample(da\_ta)} produces a random permutation of all the elements of \texttt{da\_ta},
      \vskip1ex
      If \texttt{replace=TRUE}, then \texttt{sample()} produces samples with replacement,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<>>=
# permutation of five numbers
sample(x=5)
# permutation of four strings
sample(x=c("apple", "grape", "orange", "peach"))
# sample of size three
sample(x=5, size=3)
# sample with replacement
sample(x=5, replace=TRUE)
sample(  # sample of strings
  x=c("apple", "grape", "orange", "peach"),
  size=12,
  replace=TRUE)
# binomial sample: flip coin once, 20 times
sample(x=0:1, size=20, replace=TRUE)
# flip unbiased coin once, 20 times
as.numeric(runif(20) > 0.5)  # slower
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Statistical Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A data \emph{sample} is a set of observations of a \emph{random variable},
      \vskip1ex
      Let $\{x_1,\ldots ,x_n\}$ be a data \emph{sample} of a \emph{random variable} \texttt{x},
      \vskip1ex
      Let \texttt{x} follow a probability distribution with population mean equal to $\mu$ and population standard deviation equal to $\sigma$,
      \vskip1ex
      A \emph{statistic} is a function of a data \emph{sample}:  $f( x_1,\ldots ,x_n )$,
      \vskip1ex
      A \emph{statistic} is itself a \emph{random variable},
      \vskip1ex
      A statistical \emph{estimator} is a \emph{statistic} that provides an estimate of a \emph{distribution} parameter,
      \vskip1ex
      For example:
      \begin{displaymath}
        \bar{x}=\frac{1}{n}{\sum_{i=1}^{n}x_i}
      \end{displaymath}
      Is an \emph{estimator} of the \emph{mean} of the \emph{distribution},
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:2))>>=
rm(list=ls())
set.seed(1121)  # reset random number generator
# sample from Standard Normal Distribution
sam_ple <- rnorm(1000)

mean(sam_ple)  # sample mean

median(sam_ple)  # sample median

sd(sam_ple)  # sample standard deviation
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimators of Higher Moments}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimators of moments of a probability distribution, based on a \emph{sample} of data, are given by:
      \vskip1ex
      Sample mean: $\hat\mu=\frac{1}{n} \sum_{i=1}^{n} x_i$
      \vskip1ex
      Sample variance: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^{n} (x_i-\hat\mu)^2$
      \vskip1ex
      With their expected values equal to the population mean and standard deviation:\\
      $\mathbb{E[\hat\mu]=\mu}$ \hskip0.5em and \hskip0.5em $\mathbb{E[\hat\sigma]=\sigma}$
      \vskip1ex
      The sample skewness (third moment) is equal to:
      \begin{displaymath}
        \hat{s}=\frac{n}{(n-1)(n-2)} \sum_{i=1}^{n} (\frac{x_i-\hat\mu}{\hat\sigma})^3
      \end{displaymath}
      The sample kurtosis (fourth moment) is equal to
      \begin{displaymath}
        \hat{k}=\frac{n(n+1)}{(n-1)(n-2)(n-3)} \sum_{i=1}^{n} (\frac{x_i-\hat\mu}{\hat\sigma})^4
      \end{displaymath}
      The normal distribution has zero skewness and kurtosis equal to 3,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-(1:1))>>=
rm(list=ls())
# DAX returns
ts_rets <- diff(log(EuStockMarkets[, 1]))
# number of observations
len_rets <- length(ts_rets)
# mean of DAX returns
mean_rets <- mean(ts_rets)
# standard deviation of DAX returns
sd_rets <- sd(ts_rets)
# skew of DAX returns
len_rets/((len_rets-1)*(len_rets-2))*
  sum(((ts_rets - mean_rets)/sd_rets)^3)
# kurtosis of DAX returns
len_rets*(len_rets+1)/((len_rets-1)^3)*
  sum(((ts_rets - mean_rets)/sd_rets)^4)
# random normal returns
ts_rets <- rnorm(len_rets, sd=2)
# mean and standard deviation of random normal returns
mean_rets <- mean(ts_rets)
sd_rets <- sd(ts_rets)
# skew of random normal returns
len_rets/((len_rets-1)*(len_rets-2))*
  sum(((ts_rets - mean_rets)/sd_rets)^3)
# kurtosis of random normal returns
len_rets*(len_rets+1)/((len_rets-1)^3)*
  sum(((ts_rets - mean_rets)/sd_rets)^4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Statistical estimators are themselves \emph{random variables},
      \vskip1ex
      The standard deviation of the estimator of the mean is equal to:
      \begin{displaymath}
        \sigma_{\mu} = \frac{\sigma}{\sqrt{n}}
      \end{displaymath}
      Where $\sigma$ is the population standard deviation,
      \vskip1ex
      The standard error is the estimator of the standard deviation, and for the mean estimator it is equal to:
      \begin{displaymath}
        SE_{\mu} = \frac{\hat\sigma}{\sqrt{n}}
      \end{displaymath}
      where: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^{n} (x_i-\hat\mu)^2$ is the sample standard deviation (the estimator of the population standard deviation),
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
set.seed(1121)  # reset random number generator
# sample from Standard Normal Distribution
sample_length <- 1000
sam_ple <- rnorm(sample_length)
# sample mean
mean(sam_ple)
# sample standard deviation
sd(sam_ple)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Hypothesis Testing}


%%%%%%%%%%%%%%%
\subsection{Hypothesis Testing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Hypothesis Testing} is designed to test the validity of a \emph{null hypothesis},
      \vskip1ex
      A \emph{Hypothesis Test} consists of:
      \begin{itemize}
        \item \emph{null hypothesis},
        \item test \emph{statistic} (from sample),
        \item \emph{significance level} $\alpha$, determining whether to accept or reject the \emph{null hypothesis},
        \item \emph{p}-value (probability of observing the test statistic value, assuming the \emph{null hypothesis} is \texttt{TRUE}),
      \end{itemize}
      If the \emph{p}-value is less than the \emph{significance level} $\alpha$, then the \emph{null hypothesis} is rejected,
      \vskip1ex
      The objective of \emph{Hypothesis Testing} is to invalidate the \emph{null hypothesis},
      \vskip1ex
      In statistics we cannot \emph{prove} that a hypothesis is \texttt{TRUE}; we can only conclude that it's very unlikely to be \texttt{FALSE} (given the data),
    \column{0.5\textwidth}
      \vspace{-1em}
      <<>>=
### Perform two-tailed test that sample is
### from Standard Normal Distribution (mean=0, SD=1)
# generate vector of samples and store in data frame
test_frame <- data.frame(samples=rnorm(1000))

# significance level, two-tailed test, critical value=2*SD
signif_level <- 2*(1-pnorm(2))
signif_level
# get p-values for all the samples
test_frame$p_values <-
  sapply(test_frame$samples, pnorm)
test_frame$p_values <-
  2*(0.5-abs(test_frame$p_values-0.5))
# compare p_values to significance level
test_frame$result <-
  test_frame$p_values > signif_level
# number of null rejections
sum(!test_frame$result)
# show null rejections
head(test_frame[!test_frame$result, ])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Visualizing Hypothesis Testing Using Package \texttt{ggplot2}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      <<hyp_test_ggp2,echo=(-(1:2)),eval=FALSE,fig.show='hide'>>=
rm(list=ls())
par(oma=c(1, 1, 1, 1), mgp=c(2, 0.5, 0), mar=c(5, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
library(ggplot2)  # load ggplot2

qplot(  # simple ggplot2
    main="Standard Normal Distribution",
    c(-4, 4),
    stat="function",
    fun=dnorm,
    geom="line",
    xlab=NULL, ylab=NULL
    ) +  # end qplot

theme(  # modify plot theme
    plot.title=element_text(vjust=-1.0),
    plot.background=element_blank()
    ) +  # end theme

geom_vline(  # add vertical line
  aes(xintercept=c(-2.0, 2.0)),
  colour="red",
  linetype="dashed"
  )  # end geom_vline
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/hyp_test_ggp2-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Visualizing Hypothesis Testing Using \texttt{ggplot2} (cont.)}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      <<hyp_test_ggp2_2,echo=(-(1:2)),eval=FALSE,fig.show='hide'>>=
rm(list=ls())
par(oma=c(1, 1, 1, 1), mgp=c(2, 0.5, 0), mar=c(5, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
### create ggplot2 with shaded area
x_var <- -400:400/100
norm_frame <- data.frame(x_var=x_var,
                       d.norm=dnorm(x_var))
norm_frame$shade <- ifelse(
                  abs(norm_frame$x_var) >= 2,
                  norm_frame$d.norm, NA)
ggplot(  # main function
  data=norm_frame,
  mapping=aes(x=x_var, y=d.norm)
  ) +  # end ggplot
# plot line
  geom_line() +
# plot shaded area
  geom_ribbon(aes(ymin=0, ymax=shade), fill="red") +
# no axis labels
  xlab("") + ylab("") +
# add title
  ggtitle("Standard Normal Distribution") +
# modify plot theme
  theme(
        plot.title=element_text(vjust=-1.0),
        plot.background=element_blank()
  )  # end theme
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/hyp_test_ggp2_2-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\section{Univariate Statistical Analysis}


%%%%%%%%%%%%%%%
\subsection{Shapiro-Wilk Test of Normality}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Shapiro-Wilk} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1,\ldots ,x_n\}$ is from a normally distributed population,
      \vskip1ex
      The test statistic is:
      \begin{displaymath}
        W= \frac {(\sum_{i=1}^{n} a_i x_{(i)})^2} {\sum_{i=1}^{n} (x_i-\bar{x})^2}
      \end{displaymath}
      Where the: $\{a_1,\ldots ,a_n\}$ are proportional to the \emph{order statistics} of random variables from the normal distribution,
      \vskip1ex
      $x_{(k)}$ is the \emph{k}-th \emph{order statistic}, and is equal to the \emph{k}-th smallest value in the sample: $\{x_1,\ldots ,x_n\}$,
      \vskip1ex
      The \emph{Shapiro-Wilk} statistic follows its own distribution, and is less than or equal to one,
      \vskip1ex
      The \emph{Shapiro-Wilk} statistic is close to one for samples from normal distributions,
      \vskip1ex
      The \emph{p}-value for DAX returns is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and the DAX returns are not from a normally distributed population,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<>>=
# calculate DAX percentage returns
dax_rets <- diff(log(EuStockMarkets[, 1]))

# Shapiro-Wilk test for normal distribution
shapiro.test(rnorm(length(dax_rets)))

# Shapiro-Wilk test for DAX returns
shapiro.test(dax_rets)

# Shapiro-Wilk test for uniform distribution
shapiro.test(runif(length(dax_rets)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Jarque-Bera Test of Normality}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Jarque-Bera} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1,\ldots ,x_n\}$ is from a normally distributed population,
      \vskip1ex
      The test statistic is:
      \begin{displaymath}
        JB= \frac{n}{6} (\hat{s}^2 + \frac{1}{4} (\hat{k} - 3)^2)
      \end{displaymath}
      Where the skewness and kurtosis are defined as:
      \begin{align*}
        \hat{s} = \frac{1}{n} \sum_{i=1}^{n} (\frac{x_i-\bar{x}}{\hat\sigma})^3
      &&
        \hat{k} = \frac{1}{n} \sum_{i=1}^{n} (\frac{x_i-\bar{x}}{\hat\sigma})^4
      \end{align*}
      The \emph{Jarque-Bera} statistic asymptotically follows the \emph{chi-squared} distribution with two degrees of freedom,
      \vskip1ex
      The \emph{Jarque-Bera} statistic is small for samples from normal distributions,
      \vskip1ex
      The \emph{p}-value for DAX returns is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and the DAX returns are not from a normally distributed population,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1))>>=
dax_rets <- diff(log(EuStockMarkets[, 1]))
library(tseries)  # load package tseries

# Jarque-Bera test for normal distribution
jarque.bera.test(rnorm(length(dax_rets)))

# Jarque-Bera test for DAX returns
jarque.bera.test(dax_rets)

# Jarque-Bera test for uniform distribution
jarque.bera.test(runif(length(dax_rets)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Autocorrelation Function} is the correlation coefficient of a time series with its lagged values:
      \begin{displaymath}
        \rho_k = \frac{1}{(n-k)\sigma^2} {\sum_{i=k+1}^{n} (x_i-\bar{x})(x_{i-k}-\bar{x})}
      \end{displaymath}
      \vskip1ex
      The function \texttt{acf()} from the base package \texttt{stats} calculates and plots the autocorrelation function for a univariate time series,
      \vskip1ex
      \texttt{acf()} returns the \texttt{acf} data invisibly - the return value isn't automatically printed to the console,
      \vskip1ex
      The \texttt{acf()} return data can be assigned to a variable, and then printed,
        <<acf_func,eval=FALSE,echo=(-(1:1)),fig.width=4,fig.height=3.5,fig.show='hide'>>=
par(mar=c(5,0,1,2), oma=c(1,2,1,0), mgp=c(2,1,0), cex.lab=0.8, cex.axis=1.0, cex.main=0.8, cex.sub=0.5)
library(zoo)  # load package zoo
# autocorrelation from "stats"
acf(coredata(dax_rets), lag=10, main="")
title(main="acf of DAX returns", line=-1)
      @
      The package \texttt{zoo} is designed for managing \emph{time series} and ordered objects,
      \vskip1ex
      \texttt{coredata} extracts the core underlying data from a complex object,
    \column{0.5\textwidth}
      \vspace{-1.0em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/acf_func-1}
      \vspace{-3.0em}
      The horizontal dashed lines are confidence intervals of the autocorrelation estimator (at 95\% significance level),
      \vskip1ex
      The DAX time series of returns does not appear to have statistically significant autocorrelations,
      \vskip1ex
      The function \texttt{acf()} has the drawback that it plots the lag-zero autocorrelation (which is simply \texttt{1}),
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Improved Autocorrelation Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Inspection of the data returned by \texttt{acf()} shows how to omit the lag-zero autocorrelation,
      <<echo=(-(1:1))>>=
library(zoo)  # load package zoo
dax_acf <- acf(coredata(dax_rets), plot=FALSE)
summary(dax_acf)  # get the structure of the "acf" object
# print(dax_acf)  # print acf data
dim(dax_acf$acf)
dim(dax_acf$lag)
head(dax_acf$acf)
      @
    \column{0.5\textwidth}
      The below wrapper function for \texttt{acf()} omits the lag-zero autocorrelation,
      <<eval=FALSE>>=
acf_plus <- function (ts_data, plot=TRUE,
                      xlab="Lag", ylab="",
                      main="", ...) {
  acf_data <- acf(x=ts_data, plot=FALSE, ...)
# remove first element of acf data
  acf_data$acf <-  array(data=acf_data$acf[-1],
          dim=c((dim(acf_data$acf)[1]-1), 1, 1))
  acf_data$lag <-  array(data=acf_data$lag[-1],
          dim=c((dim(acf_data$lag)[1]-1), 1, 1))
  if (plot) {
    ci <- qnorm((1+0.95)/2)*sqrt(1/length(ts_data))
    ylim <- c(min(-ci, range(acf_data$acf[-1])),
              max(ci, range(acf_data$acf[-1])))
    plot(acf_data, xlab=xlab, ylab=ylab,
         ylim=ylim, main=main, ci=0)
    abline(h=c(-ci, ci), col="blue", lty=2)
  }
  invisible(acf_data)  # return invisibly
}  # end acf_plus
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of DAX Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The DAX time series of returns does not appear to have statistically significant autocorrelations,
      \vskip1ex
      But the \texttt{acf} plot alone is not enough to test whether autocorrelations are statistically significant or not,
        <<dax_acf,echo=(-(1:2)),eval=FALSE,fig.width=4,fig.height=3.5,fig.show='hide'>>=
par(mar=c(5,0,1,2), oma=c(1,2,1,0), mgp=c(2,1,0), cex.lab=0.8, cex.axis=1.0, cex.main=0.8, cex.sub=0.5)
library(zoo)  # load package zoo
# improved autocorrelation function
acf_plus(coredata(dax_rets), lag=10, main="")
title(main="acf of DAX returns", line=-1)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/dax_acf-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Squared DAX Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Squared DAX returns do have statistically significant autocorrelations,
      \vskip1ex
      But squared random returns are not autocorrelated,
      <<dax_squared_acf,eval=FALSE,echo=(-(1:2)),fig.height=8,fig.show='hide'>>=
par(oma=c(15, 1, 1, 1), mgp=c(0, 0.5, 0), mar=c(1, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # set plot panels
# autocorrelation of squared DAX returns
acf_plus(coredata(dax_rets)^2,
         lag=10, main="")
title(main="acf of squared DAX returns",
      line=-1)
# autocorrelation of squared random returns
acf_plus(rnorm(length(dax_rets))^2,
         lag=10, main="")
title(main="acf of squared random returns",
      line=-1)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/dax_squared_acf-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{U.S. Macroeconomic Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \texttt{Ecdat} contains the \texttt{Macrodat} U.S. macroeconomic data,
      \vskip1ex
      \texttt{"lhur"} is the unemployment rate (average of months in quarter),
      \vskip1ex
      \texttt{"fygm3"} 3 month treasury bill interest rate (last month in quarter)
      <<macro_data,echo=(-(1:1)),eval=FALSE,fig.show='hide'>>=
library(zoo)  # load package zoo
library(Ecdat)  # load Ecdat
colnames(Macrodat)  # United States Macroeconomic Time Series
macro_zoo <- as.zoo(  # coerce to "zoo"
          Macrodat[, c("lhur", "fygm3")])
colnames(macro_zoo) <- c("unemprate", "3mTbill")
# ggplot2 in multiple panes
autoplot(  # generic ggplot2 for "zoo"
  object=macro_zoo, main="US Macro",
  facets=Series ~ .) + # end autoplot
  xlab("") +
theme(  # modify plot theme
  legend.position=c(0.1, 0.5),
  plot.title=element_text(vjust=-2.0),
  plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
  plot.background=element_blank(),
  axis.text.y=element_blank()
)  # end theme
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/macro_data-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Econometric Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Most econometric data displays a high degree of autocorrelation,
      \vskip1ex
      But time series of tradeable prices display very low autocorrelation,
      <<macro_corr,eval=FALSE,echo=(-(1:2)),fig.height=8,fig.show='hide'>>=
par(oma=c(15, 1, 1, 1), mgp=c(0, 0.5, 0), mar=c(1, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # set plot panels
macro_diff <- na.omit(diff(macro_zoo))

acf_plus(coredata(macro_diff[, "unemprate"]),
         lag=10)
title(main="quarterly unemployment rate",
      line=-1)

acf_plus(coredata(macro_diff[, "3mTbill"]),
         lag=10)
title(main="3 month T-bill EOQ", line=-1)
      @
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/macro_corr-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Ljung-Box Test of Autocorrelation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Ljung-Box} test \emph{null hypothesis} is that autocorrelations are equal to zero,
      \vskip1ex
      The test statistic is:
      \begin{displaymath}
        Q = n(n+2) \sum_{k=1}^{maxlag} \frac{{\hat\rho}_k^2}{n-k}
      \end{displaymath}
      Where $n$ is the sample size, and the ${\hat\rho}_k$ are sample autocorrelations,
      \vskip1ex
      The \emph{Ljung-Box} statistic follows the \emph{chi-squared} distribution with \emph{maxlag} degrees of freedom,
      \vskip1ex
      The \emph{Ljung-Box} statistic is small for time series that are \emph{not} autocorrelated,
      \vskip1ex
      The \emph{p}-value for DAX returns is large, and we conclude that the \emph{null hypothesis} is \texttt{TRUE}, and that DAX returns are \emph{not} autocorrelated,
      \vskip1ex
      The \emph{p}-value for changes in econometric data is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and that econometric data \emph{are} autocorrelated,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:4))>>=
library(Ecdat)  # load Ecdat
macro_zoo <- as.zoo(Macrodat[, c("lhur", "fygm3")])
colnames(macro_zoo) <- c("unemprate", "3mTbill")
macro_diff <- na.omit(diff(macro_zoo))
# Ljung-Box test for DAX data
# 'lag' is the number of autocorrelation coefficients
Box.test(dax_rets, lag=10, type="Ljung")

# changes in 3 month T-bill rate are autocorrelated
Box.test(macro_diff[, "3mTbill"],
         lag=10, type="Ljung")

# changes in unemployment rate are autocorrelated
Box.test(macro_diff[, "unemprate"],
         lag=10, type="Ljung")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Filtering Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<dax_filter,eval=FALSE,fig.width=6,fig.height=5,fig.show='hide'>>=
library(zoo)  # load zoo
library(ggplot2)  # load ggplot2
library(gridExtra)  # load gridExtra
# extract DAX time series
dax_ts <- EuStockMarkets[, 1]
# filter past values only (sides=1)
dax_filt <- filter(dax_ts,
                   filter=rep(1/5,5), sides=1)
# coerce to zoo and merge the time series
dax_filt <- merge(as.zoo(dax_ts),
                  as.zoo(dax_filt))
colnames(dax_filt) <- c("DAX", "DAX filtered")
dax_data <- window(dax_filt,
                   start=1997, end=1998)
autoplot(  # plot ggplot2
    dax_data, main="Filtered DAX",
    facets=NULL) +  # end autoplot
xlab("") + ylab("") +
theme(  # modify plot theme
    legend.position=c(0.1, 0.5),
    plot.title=element_text(vjust=-2.0),
    plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
    plot.background=element_blank(),
    axis.text.y=element_blank()
    )  # end theme
# end ggplot2
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/dax_filter-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation Function of Filtered Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Filtering a time series creates autocorrelations,
      <<dax_filter_acf,eval=FALSE,echo=(-(1:1)),fig.height=8,fig.show='hide'>>=
par(oma=c(15, 1, 1, 1), mgp=c(0, 0.5, 0), mar=c(1, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
dax_rets <- na.omit(diff(log(dax_filt)))
par(mfrow=c(2,1))  # set plot panels

acf_plus(coredata(dax_rets[, 1]), lag=10,
         xlab="")
title(main="DAX", line=-1)

acf_plus(coredata(dax_rets[, 2]), lag=10,
         xlab="")
title(main="DAX filtered", line=-1)
      @
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/dax_filter_acf-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Partial Autocorrelation Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An autocorrelation of lag \texttt{1} creates autocorrelations of lag \texttt{2, 3,...}, which may obscure higher order autocorrelations, 
      \vskip1ex
      A linear combination of a time series and its lag can be created, such that its lag \texttt{1} autocorrelation is zero,
      \vskip1ex
      The lag \texttt{2} autocorrelation of this new series is called the \emph{partial autocorrelation} of lag \texttt{2}, and represents the true second order autocorrelation, 
      \vskip1ex
      The \emph{partial autocorrelation} of lag \texttt{k} is the autocorrelation lag \texttt{k}, after all the autocorrelations of lag \texttt{1,..., k-1} have been removed,
        <<eustx_pacf,eval=FALSE,echo=(-(1:1)),fig.height=8,fig.show='hide'>>=
par(oma=c(15, 1, 1, 1), mgp=c(0, 0.5, 0), mar=c(1, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # set plot panels
# autocorrelation from "stats"
acf_plus(dax_rets[, 2], lag=10, xlab=NA, ylab=NA)
title(main="DAX filtered autocorrelations", line=-1)
# partial autocorrelation
pacf(dax_rets[, 2], lag=10, xlab=NA, ylab=NA)
title(main="DAX filtered partial autocorrelations",
      line=-1)
      @
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/eustx_pacf-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} time series process \emph{AR(p)} of order \emph{p} is defined as:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_{p} r_{i-p} + \varepsilon_i
      \end{displaymath}
      Where the $\varepsilon_i$ are independent random variables with zero mean and constant variance,
      \vskip1ex
      The \emph{AR(p)} process is a special case of an \emph{ARIMA} process,
      \vskip1ex
      The function \texttt{arima.sim()} simulates \emph{ARIMA} processes,
      \vspace{-1em}
    <<ar_process,eval=FALSE,echo=(-(1:3)),fig.height=5,fig.show='hide'>>=
# ARIMA processes
library(ggplot2)  # load ggplot2
library(gridExtra)  # load gridExtra
in_dex <- Sys.Date() + 0:728  # two year daily series
set.seed(1121)  # reset random numbers
zoo_arima <- zoo(  # AR time series of returns
  x=arima.sim(n=729, model=list(ar=0.2)),
  order.by=in_dex)  # zoo_arima
zoo_arima <- cbind(zoo_arima, cumsum(zoo_arima))
colnames(zoo_arima) <- c("AR returns", "AR prices")
autoplot(object=zoo_arima, # ggplot AR process
     facets="Series ~ .",
     main="Autoregressive process (phi=0.2)") +
  facet_grid("Series ~ .", scales="free_y") +
  xlab("") + ylab("") +
theme(
  legend.position=c(0.1, 0.5),
  plot.background=element_blank(),
  axis.text.y=element_blank())
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/ar_process-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Examples of Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{"model"} argument contains a \texttt{list} of \emph{ARIMA} coefficients $\{\varphi_i\}$,
      \vskip1ex
      Positive coefficient values cause positive \emph{autocorrelation}, and vice cersa,
      <<ar_param,eval=FALSE,fig.height=5,fig.show='hide'>>=
ar_coeff <- c(-0.8, 0.01, 0.8)  # AR coefficients
zoo_arima <- sapply(  # create three AR time series
  ar_coeff, function(phi) {
    set.seed(1121)  # reset random numbers
    arima.sim(n=729, model=list(ar=phi))
  } )
zoo_arima <- zoo(x=zoo_arima, order.by=in_dex)
# convert returns to prices
zoo_arima <- cumsum(zoo_arima)
colnames(zoo_arima) <-
  paste("autocorr", ar_coeff)
autoplot(zoo_arima, main="AR prices",
         facets=Series ~ .) +
    facet_grid(Series ~ ., scales="free_y") +
xlab("") +
theme(
  legend.position=c(0.1, 0.5),
  plot.title=element_text(vjust=-2.0),
  plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
  plot.background=element_blank(),
  axis.text.y=element_blank())
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/ar_param-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process of order \emph{one} \emph{AR(1)} is defined by the formula: $r_i = \varphi_1 r_{i-1} + \varepsilon_i$
      \vskip1ex
      An \emph{AR(1)} process can be simulated recursively as follows:\\
      \hskip1em$r_1 = \varepsilon_1$\\
      \hskip1em$r_2 = \varphi_1 r_1 + \varepsilon_2=\varepsilon_2 + \varphi_1 \varepsilon_1$\\
      \hskip1em$r_3 = \varepsilon_3 + \varphi_1 \varepsilon_2 + \varphi_1^2 \varepsilon_1$\\
      \hskip1em$r_4 = \varepsilon_4 + \varphi_1 \varepsilon_3 + \varphi_1^2 \varepsilon_2 + \varphi_1^3 \varepsilon_1$
      \vskip1ex
      If $\varphi_1 < 1.0$ then the influence of any single shock $\varepsilon_i$ decays exponentially,
      \vskip1ex
      If $\varphi_1 = 1.0$ then the influence of any single shock $\varepsilon_i$ persists forever, and the variance of $r_i$ increases linearly with time,
      \vskip1ex
      An \emph{AR(1)} process has an exponentially declining ACF and a non-zero PACF at lag one,
      \vspace{-1em}
      <<ar_acf,eval=FALSE,echo=(-(1:2)),fig.height=8,fig.show='hide'>>=
par(oma=c(15, 1, 1, 1), mgp=c(0, 0.5, 0), mar=c(1, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # set plot panels
# simulate AR(1) process
ari_ma <- arima.sim(n=729, model=list(ar=0.8))
# ACF of AR(1) process
acf_plus(ari_ma, lag=10, xlab="", ylab="",
         main="ACF of AR(1) process")
# PACF of AR(1) process
pacf(ari_ma, lag=10, xlab="", ylab="",
     main="PACF of AR(1) process")
      @
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/ar_acf-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Stationary Processes and Their Characteristic Equations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An autoregressive process is \emph{stationary} if its probability distribution does not change with time,
      \vskip1ex
      \emph{Stationary} processes have a constant mean and variance,
      \vskip1ex
      The \emph{autoregressive} process \emph{AR(p)}:
      $r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_{p} r_{i-p} + \varepsilon_i$
      \vskip1ex
      Has the following characteristic equation:
      $1 - \varphi_1 z - \varphi_2 z^2 - \ldots - \varphi_{p} z^p = 0$
      \vskip1ex
      An autoregressive process is stationary only if the absolute values of all the roots of its characteristic equation are greater than $1$,
      \vskip1ex
      An \emph{AR(1)} process:
      $r_i = \varphi_1 r_{i-1} + \varepsilon_i$
      has the following characteristic equation:
      $1 - \varphi_1 z = 0$,
      with a root equal to:
      $z = 1 / \varphi_1$,
      \vskip1ex
      If $\varphi_1 = 1$, then the characteristic equation has a \emph{unit root}:
      $z = 1 / \varphi_1$, and therefore isn't stationary,
      \vskip1ex
      The process follows:
      $r_i = r_{i-1} + \varepsilon_i$,
      \vskip1ex
      The above is called a \emph{Wiener} process, and it's a special case of a \emph{unit-root} process,
      \vskip1ex
      The variance of a \emph{Wiener} process is proportional to time,
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/stat_unit_root-1}
      \vspace{-4em}
      <<eval=FALSE,echo=(-(1:3)),fig.width=6,fig.height=5,fig.show='hide'>>=
library(zoo)  # load zoo
library(ggplot2)  # load ggplot2
set.seed(1121)  # initialize random number generator
rand_walk <- cumsum(zoo(matrix(rnorm(3*100), ncol=3),
                  order.by=(Sys.Date()+0:99)))
colnames(rand_walk) <-
  paste("rand_walk", 1:3, sep="_")
plot(rand_walk, main="Random walks",
     xlab="", ylab="", plot.type="single",
     col=c("black", "red", "blue"))
# add legend
legend(x="topleft",
       legend=colnames(rand_walk),
       col=c("black", "red", "blue"), lty=1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Integrated and Unit-root Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Asset prices are the sum of simple asset returns, hence they follow an \emph{integrated} process with respect to asset returns:
      \begin{displaymath}
        x_i={\sum_{i=1}^{n}r_i}
      \end{displaymath}
      If returns follow an \emph{AR(1)} process:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varepsilon_i
      \end{displaymath}
      Then asset prices follow the process:
      \begin{displaymath}
        x_i = (1+\varphi_1) x_{i-1} - \varphi_1 x_{i-2} + \varepsilon_i
      \end{displaymath}
      If $\varphi_1=0$ then asset prices follow a \emph{Wiener} process (random walk),
      \vskip1ex
      A \emph{Wiener} process is a special case of a \emph{unit-root} process,
      \vskip1ex
      \emph{Unit-root} processes are not stationary, since their \emph{variance} isn't constant,
      \vskip1ex
      If $\varphi_1=0$ (no autocorrelation of returns) then asset prices follow a \emph{Wiener} process (random walk),
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/stat_unit_root-1}
      \vspace{-4em}
      <<stat_unit_root,eval=FALSE,echo=(-(1:3)),fig.width=6,fig.height=5,fig.show='hide'>>=
library(zoo)  # load zoo
library(ggplot2)  # load ggplot2
set.seed(1121)  # initialize random number generator
rand_walk <- cumsum(zoo(matrix(rnorm(3*100), ncol=3),
                  order.by=(Sys.Date()+0:99)))
colnames(rand_walk) <-
  paste("rand_walk", 1:3, sep="_")
plot(rand_walk, main="Random walks",
     xlab="", ylab="", plot.type="single",
     col=c("black", "red", "blue"))
# add legend
legend(x="topleft",
       legend=colnames(rand_walk),
       col=c("black", "red", "blue"), lty=1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Dickey-Fuller Test for Unit-roots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The \emph{Dickey-Fuller} and \emph{Augmented Dickey-Fuller} tests are designed to test the \emph{null hypothesis} that a time series process has a \emph{unit root},
      \vskip1ex
      The \emph{Augmented Dickey-Fuller} test fits the following regression model:
      \begin{displaymath}
        r_i = {\gamma}_1 x_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_{p} r_{i-p} + \varepsilon_i
      \end{displaymath}
      where $r_i = x_i - x_{i-1}$,
      \vskip1ex
      The \emph{null hypothesis} is that the process has a unit root ($\gamma = 0$), while the alternative hypothesis is that the process is stationary ($\gamma < 0$),
      \vskip1ex
      The \emph{ADF} test is weak in the sense that it needs a lot of data to identify a \emph{unit root} process,
    \column{0.6\textwidth}
      \vspace{-1em}
      <<eval=FALSE,echo=(-(1:1))>>=
library(tseries)  # load tseries
# simulate AR(1) process
set.seed(1121)  # initialize random number generator
ari_ma <- arima.sim(n=729, model=list(ar=0.8))
adf.test(ari_ma)
set.seed(1121)  # initialize random number generator
ari_ma <- arima.sim(n=10000, model=list(ar=0.8))
adf.test(ari_ma)
set.seed(1121)  # initialize random number generator
rand_walk <- cumsum(rnorm(729))
adf.test(rand_walk)
set.seed(1121)  # initialize random number generator
rand_walk <- cumsum(rnorm(10000))
adf.test(rand_walk)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Identification of Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{AR}(3) process of order \emph{three} is defined by the formula:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \varphi_3 r_{i-3} + \varepsilon_i
      \end{displaymath}
      Autoregressive processes \emph{AR(p)} of order \emph{p} have an exponentially declining ACF and a non-zero PACF up to lag \emph{p},
      <<ar_pacf,eval=FALSE,echo=(-(1:2)),fig.height=8,fig.show='hide'>>=
par(oma=c(15, 1, 1, 1), mgp=c(0, 0.5, 0), mar=c(1, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # set plot panels
ar3_zoo <- zoo(  # AR(3) time series of returns
  x=arima.sim(n=365,
          model=list(ar=c(0.1, 0.5, 0.1))),
  order.by=in_dex)  # zoo_arima
# ACF of AR(3) process
acf_plus(ar3_zoo, lag=10,
       xlab="", ylab="", main="ACF of AR(3) process")

# PACF of AR(3) process
pacf(ar3_zoo, lag=10,
     xlab="", ylab="", main="PACF of AR(3) process")
      @
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/ar_pacf-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fitting Autoregressive Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.3\textwidth}
      The function \texttt{arima()} from the base package \texttt{stats} fits a specified ARIMA model to a univariate time series,
      \vskip1ex
      The function \texttt{auto.arima()} from the package \texttt{forecast} automatically fits an ARIMA model to a univariate time series,
    \column{0.7\textwidth}
      \vspace{-1em}
      <<echo=TRUE>>=
ar3_zoo <- arima.sim(n=1000,
            model=list(ar=c(0.1, 0.3, 0.1)))
arima(ar3_zoo, order = c(5,0,0))  # fit AR(5) model
library(forecast)  # load forecast
auto.arima(ar3_zoo)  # fit ARIMA model
      @
  \end{columns}
\end{block}

\end{frame}





%%%%%%%%%%%%%%%
\section{Homework Assignment}


%%%%%%%%%%%%%%%
\subsection{Homework Assignment}
\begin{frame}[t]{\secname}
\vspace{-1em}
\begin{block}{Required}
  Read all the lecture slides in \texttt{FRE7241\_Lecture\_3.pdf}, and run all the code in \texttt{FRE7241\_Lecture\_3.R}
\end{block}

\begin{block}{Recommended}
  Read the file \texttt{functions.pdf}, and run all the code in \texttt{functions.R},  
      \vskip1ex
  Read the following sections in the file \texttt{numerical\_analysis.pdf}: 
  \begin{itemize}[]
    \item \emph{Numerical Calculations}, 
    \item \emph{Optimizing \texttt{R} Code for Speed and Memory Usage}, 
    \item \emph{Writing Fast \texttt{R} Code Using Vectorized Operations}, 
    \item Run the code corresponding to the above sections from \texttt{numerical\_analysis.R}
  \end{itemize}
  Read the following sections in the file \texttt{R\_environment.pdf}: 
  \begin{itemize}[]
    \item \emph{Environments in \texttt{R}}, 
    \item \emph{Data Input and Output}, 
    \item Run the code corresponding to the above sections from \texttt{R\_environment.R}
  \end{itemize}
\end{block}

\end{frame}


\end{document}
