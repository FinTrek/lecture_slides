% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='scriptsize', fig.width=4, fig.height=4)
options(digits=3)
options(width=60, dev='pdf')
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
% \usepackage{mathtools}
\usepackage[latin1]{inputenc}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
\usepackage{listings}
\usepackage{xcolor}
\definecolor{anti_flashwhite}{rgb}{0.95, 0.95, 0.96}
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}



% Title page setup
\title[Numerical Analysis]{Numerical Analysis}
\subtitle{FRE6871 \& FRE7241, Spring 2018}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{\today}



%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Numerical Calculations}


%%%%%%%%%%%%%%%
\subsection{Floating Point Numbers}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{R} prints floating point numbers without showing their full internal representation, which can cause confusion about their true value,
      \vskip1ex
      \emph{Real} numbers which have an infinite number of significant digits can only be represented approximately inside a computer,
      \vskip1ex
      Floating point numbers are approximate representations of \emph{real} numbers inside a computer,
      \vskip1ex
      \emph{Machine precision} is a number that specifies the accuracy of floating point numbers in a computer,
      \vskip1ex
      The representation of floating point numbers in \texttt{R} depends on the \emph{machine precision} of the computer operating system,
      \vskip1ex
      The variable \texttt{.Machine} contains information about the numerical characteristics of the computer \texttt{R} is running on, such as the largest \texttt{double} and \texttt{integer} numbers, and the \emph{machine precision},
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
foo <- 0.3/3
foo  # printed as "0.1"
foo - 0.1  # foo is not equal to "0.1"
foo == 0.1  # foo is not equal to "0.1"
print(foo, digits=10)
print(foo, digits=16)
# foo is equal to "0.1" within machine precision
all.equal(foo, 0.1)
foo <- (3-2.9)
print(foo, digits=20)
# info machine precision of computer R is running on
# ?.Machine
# machine precision
.Machine$double.eps
      @
      The function \texttt{all.equal()} tests the equality of two objects to within the square root of the \emph{machine precision},
      \vskip1ex
      The generic function \texttt{format()} formats \texttt{R} objects for printing and display,
      \vskip1ex
      The generic function \texttt{print()} prints its argument and returns it \emph{invisibly},
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Floating Point Calculations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Calculations with floating point numbers are subject to \emph{numerical error} (they're not perfectly accurate),
      \vskip1ex
      Rounding a number means replacing it with the closest number of a given precision,
      \vskip1ex
      The \emph{IEC 60559} convention is to round to the nearest even number (\texttt{1.5} to \texttt{2}, and also \texttt{2.5} to \texttt{2}), which preserves the mean of a sequence,
      \vskip1ex
      The function \texttt{round()} rounds a number to the specified number of decimal places,
      \vskip1ex
      Truncating a number means replacing it with the largest integer which is less than the given number,
      \vskip1ex
      The function \texttt{trunc()} truncates a number,
      \vskip1ex
      The function \texttt{ceiling()} returns the smallest integer which is greater than the given number,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
foo <- sqrt(2)
foo^2  # printed as "2"
foo^2 == 2  # foo^2 is not equal to "2"
print(foo^2, digits=20)
# foo^2 is equal to "2" within machine precision
all.equal(foo^2, 2)
# numbers with precision 0.1
0.1*(1:10)
# round to precision 0.1
round(3.675, 1)
# round to precision 1.0
round(3.675)
# round to nearest even number
c(round(2.5), round(3.5), round(4.5))
round(4:20/2)  # round to nearest even number
trunc(3.675)  # truncate
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Comparing Objects With \texttt{identical()} and \texttt{all.equal()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{identical()} tests if two objects are exactly the same, and always returns a single logical \texttt{TRUE} or \texttt{FALSE} (never \texttt{NA} or logical \texttt{vectors}),
      \vskip1ex
      For atomic arguments \texttt{identical()} often gives the same result as the \texttt{"=="} operator, but it's not synonymous with it in general,
      \vskip1ex
      The \texttt{"=="} operator applies the \emph{recycling rule} to vector arguments and returns logical \texttt{vectors}, but \texttt{identical()} doesn't and returns a single logical value,
      \vskip1ex
      The function \texttt{all.equal()} tests the equality of two objects to within the square root of the \emph{machine precision},
      \vskip1ex
      The variable \texttt{.Machine} contains information about the numerical characteristics of the computer \texttt{R} is running on, such as the largest \texttt{double} and \texttt{integer} numbers, and the \emph{machine precision},
    \column{0.5\textwidth}
      \vspace{-1em}
        <<>>=
num_var <- 2
num_var==2
identical(num_var, 2)

identical(num_var, NULL)
# this doesn't work:
# num_var==NULL
is.null(num_var)

vec_tor <- c(2, 4, 6)
vec_tor==2
identical(vec_tor, 2)

# num_ber is equal to "1.0" within machine precision
num_ber <- 1.0 + 2*sqrt(.Machine$double.eps)
all.equal(num_ber, 1.0)

# info machine precision of computer R is running on
# ?.Machine
# machine precision
.Machine$double.eps
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Modular Arithmetic Operators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{R} has two modular arithmetic \emph{operators}:
      \begin{itemize}
        \item "\texttt{\%/\%}" performs \emph{modulo} division,
        \item "\texttt{\%\%}" calculates remainder of \emph{modulo} division,
      \end{itemize}
      \emph{Modulo} division of floating point (non-integer) numbers sometimes produces incorrect results because of limited \emph{machine precision} of floating point numbers,
      \vskip1ex
      For example, the number \texttt{0.2} is stored as a binary number slightly larger than \texttt{0.2}, so the result of calculating \texttt{0.6 \%/\% 0.2} is \texttt{2} instead of \texttt{3},
      \vskip1ex
      See discussion in:
      \url{http://stackoverflow.com/questions/13614749/modulus-bug-in-r}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
4.7 %/% 0.5  # modulo division
4.7 %% 0.5  # remainder of modulo division
# reversing modulo division usually
# returns the original number
(4.7 %% 0.5) + 0.5 * (4.7 %/% 0.5)
# modulo division of non-integer numbers can
# produce incorrect results
0.6 %/% 0.2  # produces 2 instead of 3
6 %/% 2  # use integers to get correct result
# 0.2 stored as binary number
# slightly larger than 0.2
print(0.2, digits=22)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Optimizing \texttt{R} Code for Speed and Memory Usage}


%%%%%%%%%%%%%%%
\subsection{Determining the Memory Usage of \texttt{R} Objects}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{object.size()} displays the amount of memory (in \emph{bytes}) allocated to \texttt{R} objects,
      \vskip1ex
      The generic function \texttt{format()} formats \texttt{R} objects for printing and display,
      \vskip1ex
      The method \texttt{format.object\_size()} defines a \emph{megabyte} as \texttt{1,048,576} \emph{bytes} ($2^{20}$), not \texttt{1,000,000} \emph{bytes},
      \vskip1ex
      The function \texttt{get()} accepts a character string and returns the value of the corresponding object in a specified \emph{environment},
      \vskip1ex
      \texttt{get()} retrieves objects that are referenced using character strings, instead of their names,
      \vskip1ex
      The function \texttt{mget()} accepts a vector of strings and returns a list of the corresponding objects,
      \vskip1ex
      The function \texttt{ll()} from package \texttt{gdata} displays the amount of memory (in \emph{bytes}) allocated to \texttt{R} objects,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# get size of an object
object.size(runif(1e6))
format(object.size(runif(1e6)), units="MB")
# get sizes of objects in workspace
sort(sapply(ls(),
  function(ob_ject) {
    format(object.size(get(ob_ject)), units="KB")}))
# get sizes of objects in workspace
sort(sapply(mget(ls()), object.size))
sort(sapply(mget(ls()),
      function(ob_ject) {
        format(object.size(ob_ject), units="KB")}
))
# get sizes of objects in etf_env environment
sort(sapply(ls(etf_env),
  function(ob_ject) {
    object.size(get(ob_ject, etf_env))}))
# get sizes of objects in etf_env environment
sort(sapply(mget(ls(etf_env), etf_env),
            object.size))
# get total size of all objects in workspace
print(object.size(x=mget(ls())), units="MB")
library(gdata)  # load package gdata
# get names, class, and size of objects in workspace
ob_jects <- ll(unit="bytes")
# sort by memory size (descending)
ob_jects[order(ob_jects[, 2], decreasing=TRUE), ]
ll()[order(ll()$KB, decreasing=TRUE), ]
# get sizes of objects in etf_env environment
ll(unit="bytes", etf_env)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Managing Very Large Datasets Using Package \protect\emph{SOAR}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vskip1ex
      The package \emph{SOAR} allows performing calculations with multiple, very large datasets, without loading them all at once into \texttt{R} memory,
      \vskip1ex
      Package \emph{SOAR} uses \emph{delayed assignment} of objects (\emph{lazy loading}), which means that they don't reside in \texttt{R} memory, but they're silently loaded from the hard drive when they're needed,
      \vskip1ex
      The function \texttt{Store()} removes objects from memory, stores them in an \emph{object cache}, and places the \emph{object cache} on the search path,
      \vskip1ex
      The \emph{object cache} is a sub-directory of the \emph{cwd} called \texttt{.R\_Cache}, and contains \texttt{.RData} files with the stored objects,
      \vskip1ex
      The stored objects aren't listed in the \texttt{R} workspace, but they are visible on the search path as \emph{promises},
      \vskip1ex
      The function \texttt{Ls()} lists the objects stored in the \emph{object cache}, and attaches the \emph{cache} to the search path,
      \vskip1ex
      The function \texttt{find()} finds where objects are located on the search path,
      \vskip1ex
      The function \texttt{data()} isn't required to load data sets that are set up for \emph{lazy loading},
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(SOAR)  # load package SOAR
# get sizes of objects in workspace
sort(sapply(mget(ls()), object.size))
Store(etf_list)  # store in object cache
# get sizes of objects in workspace
sort(sapply(mget(ls()), object.size))
search()  # get search path for R objects
Ls()  # list object cache
find("etf_list")  # find object on search path
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Memory Usage and Garbage Collection in \texttt{R}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vskip1ex
      \emph{Garbage collection} is the process of releasing memory occupied by objects no longer in use by a computer program,
      \vskip1ex
      The function \texttt{gc()} performs garbage collection and reports the memory used by \texttt{R} in units of \emph{Vcells} (vector cells, which are \texttt{8} \emph{bytes} each),
      \vskip1ex
      \texttt{R} performs garbage collection automatically, so calling \texttt{gc()} is designed mostly to report the memory used by \texttt{R},
      \vskip1ex
      The memory used by \texttt{R} is usually greater than the total size of all objects in the workspace, because \texttt{R} requires additional memory,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# get R memory
v_cells <- gc()["Vcells", "used"]
# create vector with 1,000,000 cells
foo_bar <- numeric(1000000)
# get extra R memory
gc()["Vcells", "used"] - v_cells
# get total size of all objects in workspace
print(object.size(x=mget(ls())), units="MB")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Benchmarking the Speed of \texttt{R} Code}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{system.time()} calculates the execution time (in seconds) used to evaluate a given expression,
      \vskip1ex
      \texttt{system.time()} returns the \emph{"user time"} (execution time of user instructions), the \emph{"system time"} (execution time of operating system calls), and \emph{"elapsed time"} (total execution time, including system latency waiting),
      \vskip1ex
      The function \texttt{microbenchmark()} from package \texttt{microbenchmark} calculates and compares the execution time of \texttt{R} expressions (in milliseconds), and is more accurate than \texttt{system.time()},
      \vskip1ex
      The time it takes to execute an expression is not always the same, since it depends on the state of the processor, caching, etc.
      \vskip1ex
      \texttt{microbenchmark()} executes the expression many times, and returns the distribution of total execution times,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(microbenchmark)
foo <- runif(1e6)
system.time(foo^0.5)
microbenchmark(sqrt(foo), foo^0.5, times=10)
      @
      The "\texttt{times}" parameter is the number of times the expression is evaluated.
      \vskip1ex
      The choice of the "\texttt{times}" parameter is a tradeoff between the time it takes to run \texttt{microbenchmark()}, and the desired accuracy,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{It's \protect\emph{Always} Important to Write Fast \texttt{R} Code}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      How to write fast \texttt{R} code:
      \begin{itemize}
        \item Avoid using \texttt{apply()} and \texttt{for()} loops for large datasets,
        \item Use \emph{compiled} \texttt{C++} \texttt{R} functions instead of using interpreted \texttt{R} code for the same task,
        \item Write functions in \texttt{C++} and compile them using \emph{Rcpp} and \emph{RcppArmadillo},
        \item \emph{Byte-compile} \texttt{R} functions using the \emph{byte compiler} in package \emph{compiler},
        \item Pre-allocate memory for new objects, instead of appending to them ("growing" them),
        \item Avoid using too many \texttt{R} function calls (every command in \texttt{R} is a function),
        \item Use \emph{function methods} directly instead of using \emph{generic functions},
        \item Create specialized functions by extracting only the essential \texttt{R} code from \emph{function methods}.
      \end{itemize}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{image/Jeremy_Clarkson_Linus_Torvalds.jpg}
      <<echo=TRUE,eval=FALSE>>=
# calculate the square root of a vector
vec_tor <- runif(1e6)
all.equal(sqrt(vec_tor), vec_tor^0.5)
# microbenchmark the two methods
library(microbenchmark)
summary(microbenchmark(
  sqrt(vec_tor), 
  vec_tor^0.5, 
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: How to Write Fast \texttt{R} Code}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{R} code can be very fast, provided that the user understands the best ways of writing fast \texttt{R} code:
      \begin{itemize}
        \item call \emph{compiled} functions instead of writing \texttt{R} code for the same task,
        \item call function methods directly instead of calling generic functions,
        \item create specialized functions by extracting only the essential \texttt{R} code from function methods,
        \item write your own \texttt{C++} functions, compile them using RcppArmadillo, and call them from \texttt{R},
        \item pre-allocate memory for new vectors,
        \item use \texttt{vapply()} and \texttt{lapply()} instead of \texttt{apply()} and \texttt{for()} loops,
        \item avoid writing too many \texttt{R} function calls (remember that every command in \texttt{R} is a function),
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(microbenchmark)
vec_tor <- runif(1e6)
system.time(vec_tor^0.5)
summary(
  microbenchmark(sqrt(vec_tor), vec_tor^0.5, times=10)
  )[, c(1, 4, 5)]
      @
      \hspace*{-1em}
      \scriptsize{
\begin{tabular}{ | c || p{2.1cm} | p{2.1cm} | }
 \hline
 \textbf{Task} & \textbf{Low-performance R} & \textbf{High-performance R} \\
 \hline
 \textbf{Loops} & for() or apply() loops & C-compiled and vectorized functions \\
 \hline
 \textbf{Memory} & Automatic R memory allocation & User memory allocation \\
 \hline
 \textbf{Dispatch} & Generic functions & Class methods \\
 \hline
 \textbf{Code} & Verbose R code & Rcpp code \\
 \hline
\end{tabular}
      }
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Writing Fast \texttt{R} Code Using \protect\emph{Compiled} \texttt{C++} Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Compiled} \texttt{C++} functions directly call compiled \texttt{C++} or \texttt{Fortran} code, which performs the calculations and returns the result back to \texttt{R},
      \vskip1ex
      This makes \emph{compiled} \texttt{C++} functions much faster than \emph{interpreted} functions, which have to be parsed by \texttt{R},
      \vskip1ex
      \texttt{sum()} is much faster than \texttt{mean()}, because \texttt{sum()} is a \emph{compiled} function, while \texttt{mean()} is an \emph{interpreted} function,
      \vskip1ex
      Given a single argument, \texttt{any()} is equivalent to \texttt{\%in\%}, but is much faster because it's a \emph{compiled} function,
      \vskip1ex
      \texttt{\%in\%} is a wrapper for \texttt{match()} defined as follows:\\
      \texttt{"\%in\%" <- function(x, table) match(x, table, nomatch=0) > 0},
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(microbenchmark)
# sum() is a compiled primitive function
sum
# mean() is a generic function
mean
foo <- runif(1e6)
# sum() is much faster than mean()
summary(
  microbenchmark(sum(foo), mean(foo), times=10)
  )[, c(1, 4, 5)]
# any() is a compiled primitive function
any
# any() is much faster than %in% wrapper for match()
summary(
  microbenchmark(any(foo == 1), {1 %in% foo}, times=10)
  )[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Writing Fast \texttt{R} Code Without Method Dispatch}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      As a general rule, calling generic functions is slower than directly calling individual methods, because generic functions must execute extra \texttt{R} code for method dispatch,
      \vskip1ex
      The generic function \texttt{as.data.frame()} coerces matrices and other objects into data frames,
      \vskip1ex
      The method \texttt{as.data.frame.matrix()} coerces only matrices into data frames,
      \vskip1ex
      \texttt{as.data.frame.matrix()} is about \texttt{50\%} faster than \texttt{as.data.frame()}, because it skips extra \texttt{R} code in \texttt{as.data.frame()} needed for argument validation, error checking, and method dispatch,
      \vskip1ex
      Users can create even faster functions of their own by extracting only the essential \texttt{R} code into their own specialized functions, ignoring \texttt{R} code needed to handle different types of data,
      \vskip1ex
      Such specialized functions are faster but less flexible, so they may fail with different types of data,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(microbenchmark)
mat_rix <- matrix(1:9, ncol=3, # create matrix
  dimnames=list(paste0("row", 1:3),
                paste0("col", 1:3)))
# create specialized function
matrix_to_dframe <- function(mat_rix) {
  n_col <- ncol(mat_rix)
  dframe <- vector("list", n_col)  # empty vector
  for (in_dex in 1:n_col)  # populate vector
    dframe <- mat_rix[, in_dex]
  attr(dframe, "row.names") <-  # add attributes
    .set_row_names(NROW(mat_rix))
  attr(dframe, "class") <- "data.frame"
  dframe  # return data frame
}  # end matrix_to_dframe
# compare speed of three methods
summary(microbenchmark(
  matrix_to_dframe(mat_rix),
  as.data.frame.matrix(mat_rix),
  as.data.frame(mat_rix),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Using \texttt{apply()} Instead of \texttt{for()} and \texttt{while()} Loops}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      All the different \texttt{R} loops have similar speed, with \texttt{apply()} the fastest, then \texttt{vapply()}, \texttt{lapply()} and \texttt{sapply()} slightly slower, and \texttt{for()} loops the slowest,
      \vskip1ex
      More importantly, the \texttt{apply()} syntax is more readable and concise, and fits the functional language paradigm of \texttt{R},  so is therefore preferred obver \texttt{for()} loops,
      \vskip1ex
      Both \texttt{vapply()} and \texttt{lapply()} are \emph{compiled} (\emph{primitive}) functions, and therefore can be faster than other \texttt{apply()} functions,
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# matrix with 5,000 rows
mat_rix <- matrix(rnorm(10000), ncol=2)
# allocate memory for row sums
row_sums <- numeric(NROW(mat_rix))
summary(microbenchmark(
  row_sums=rowSums(mat_rix),  # end row_sums
  ap_ply=apply(mat_rix, 1, sum),  # end apply
  l_apply=lapply(1:NROW(mat_rix), function(in_dex)
    sum(mat_rix[in_dex, ])),  # end lapply
  v_apply=vapply(1:NROW(mat_rix), function(in_dex)
    sum(mat_rix[in_dex, ]),
    FUN.VALUE=c(sum=0)),  # end vapply
  s_apply=sapply(1:NROW(mat_rix), function(in_dex)
    sum(mat_rix[in_dex, ])),  # end sapply
  for_loop=for (i in 1:NROW(mat_rix)) {
    row_sums[i] <- sum(mat_rix[i,])
  },  # end for
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Increasing Speed of Loops by Pre-allocating Memory}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{R} doesn't require allocating memory for new vectors or lists, allowing for them to "grow" each time a new element is added,
      \vskip1ex
      \texttt{R} allows assigning a value to a vector element that doesn't exist yet (hasn't been allocated),
      \vskip1ex
      But when \texttt{R} creates a bigger object from an existing one, it first allocates memory for the new object, and then copies the existing values to the new memory, which is very memory intensive and slow,
      \vskip1ex
      Using the functions \texttt{c()}, \texttt{append()}, \texttt{cbind()}, \texttt{rbind()}, and \texttt{paste()} to append data to objects is even slower than vector assignment,
      \vskip1ex
      Adding elements to a vector in a loop is very slow, and therefore not recommended,
      \vskip1ex
      Pre-allocating memory for large vectors before performing loops increases their speed,
      \vskip1ex
      The function \texttt{numeric(k)} returns a numeric vector of zeros of length \texttt{k},
      \vskip1ex
      \texttt{numeric(0)} returns an empty (zero length) numeric vector (not to be confused with a \texttt{NULL} object),
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
big_vector <- rnorm(5000)
summary(microbenchmark(
# allocate full memory for cumulative sum
  for_loop={cum_sum <- numeric(NROW(big_vector))
    cum_sum[1] <- big_vector[1]
    for (i in 2:NROW(big_vector)) {
      cum_sum[i] <- cum_sum[i-1] + big_vector[i]
    }},  # end for
# allocate zero memory for cumulative sum
  grow_vec={cum_sum <- numeric(0)
    cum_sum[1] <- big_vector[1]
    for (i in 2:NROW(big_vector)) {
# add new element to "cum_sum" ("grow" it)
      cum_sum[i] <- cum_sum[i-1] + big_vector[i]
    }},  # end for
# allocate zero memory for cumulative sum
  com_bine={cum_sum <- numeric(0)
    cum_sum[1] <- big_vector[1]
    for (i in 2:NROW(big_vector)) {
# add new element to "cum_sum" ("grow" it)
      cum_sum <- c(cum_sum, big_vector[i])
    }},  # end for
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Byte Compilation} of \texttt{R} Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{byte code compiler} translates \texttt{R} expressions into a simpler set of commands called \emph{bytecode}, which can be interpreted by a \emph{byte code interpreter} much faster. 
      \vskip1ex
      \emph{Byte-compilation} eliminates many routine interpreter operations, and typically speeds up processing by about \texttt{2} to \texttt{5} times.  
      \vskip1ex
      The package \texttt{compiler} (included in \texttt{R}) contains functions for \emph{byte-compilation}, 
      \vskip1ex
      The function \texttt{compiler::cmpfun()} performs \emph{byte-compilation} of a function,
      \vskip1ex
      When a function is passed into some functionals (like \texttt{microbenchmark()}) it is automatically \emph{byte-compiled} \emph{just-in-time} (JIT), so that when it's run the second time it runs faster,
      \vskip1ex
      The function \texttt{compiler::enableJIT()} enables or disables automatic \emph{JIT byte-compilation}, 
      \vskip1ex
      \emph{JIT} is disabled if the \texttt{level} argument is equal to \texttt{0}, with greater \texttt{level} values forcing more extensive compilation,
      \vskip1ex
      The default \emph{JIT} \texttt{level} is \texttt{3},
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# disable JIT
jit_level <- compiler::enableJIT(0)
# create inefficient function
my_mean <- function(x) {
  out_put <- 0; n_elem <- NROW(x)
  for(it in 1:n_elem)
    out_put <- out_put + x[it]/n_elem
  out_put
}  # end my_mean
# byte-compile function and inspect it
mymean_comp <- compiler::cmpfun(my_mean)
mymean_comp
# test function
vec_tor <- runif(1e3)
all.equal(mean(vec_tor), mymean_comp(vec_tor), my_mean(vec_tor))
# microbenchmark byte-compile function
summary(microbenchmark(
  mean(vec_tor), 
  mymean_comp(vec_tor), 
  my_mean(vec_tor),
  times=10))[, c(1, 4, 5)]
# create another inefficient function
sapply2 <- function(x, FUN, ...) {
  out_put <- vector(length=NROW(x))
  for (it in seq_along(x))
    out_put[it] <- FUN(x[it], ...)
  out_put
}  # end sapply2
sapply2_comp <- compiler::cmpfun(sapply2)
all.equal(sqrt(vec_tor), 
          sapply2(vec_tor, sqrt), 
          sapply2_comp(vec_tor, sqrt))
summary(microbenchmark(
  sqrt(vec_tor),
  sapply2_comp(vec_tor, sqrt),
  sapply2(vec_tor, sqrt),
  times=10))[, c(1, 4, 5)]
# enable JIT
compiler::enableJIT(jit_level)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Profiling} the Performance of \texttt{R} Expressions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Profiling} of a computer program means measuring the amount of memory and time used for the execution of its different components.
      \vskip1ex
      \emph{Profiling} can be implemented by polling a computer program in fixed time intervals, and writing the information (like the call stack) to a file.  
      \vskip1ex
      The command \texttt{Rprof(file\_name)} turns on the profiling of \texttt{R} expressions, and saves the profiling data into the file \texttt{file\_name}. 
      \vskip1ex
      If an \texttt{R} expression is executed after profiling is enabled, then its profiling data is written to the file \texttt{file\_name}. 
      \vskip1ex
      The command \texttt{Rprof(NULL)} turns off profiling.
      \vskip1ex
      The function \texttt{summaryRprof()} compiles a summary of the profiling data from a file.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# define functions for profiling
out_er <- function() {fa_st(); sl_ow()}
fa_st <- function() Sys.sleep(0.1)
sl_ow <- function() Sys.sleep(0.2)
# turn on profiling
Rprof(filename="C:/Develop/data_def/profile.out")
# run code for profiling
replicate(n=10, out_er())
# turn off profiling
Rprof(NULL)
# compile summary of profiling from file
summaryRprof("C:/Develop/data_def/profile.out")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{profvis} for Interactive Visualizations of \protect\emph{Profiling}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{profvis} creates interactive visualizations of \emph{profiling} data produced by function \texttt{Rprof()}: \\
      \hskip1em\url{https://rstudio.github.io/profvis/}
      \vskip1ex
      The function \texttt{profvis::profvis()} profiles an \texttt{R} expression and creates an interactive \emph{flame graph} visualization: \\
      \hskip1em\url{https://rstudio.github.io/profvis/examples.html}
      \vskip1ex
      \emph{Profiling} of different types of loops over the columns of \emph{matrices} and \emph{data frames} shows that \texttt{colMeans()} is the fastest for \emph{matrices}, while \texttt{lapply()} is the fastest for \emph{data frames}.
      \vskip1ex
      \texttt{profvis::profvis()} can also profile \emph{shiny apps}.
      \vskip1ex
      \emph{Profiling} can also be launched using the \emph{Profile} menu in \emph{RStudio}.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# profile plotting of regression
profvis::profvis({
  plot(price ~ carat, data=ggplot2::diamonds)
  mod_el <- lm(price ~ carat, data=ggplot2::diamonds)
  abline(mod_el, col="red")
})  # end profvis
# four methods of calculating matrix column means
mat_rix <- matrix(rnorm(1e5), ncol=5e4)
profvis::profvis({
  mean_s <- apply(mat_rix, 2, mean)
  mean_s <- colMeans(mat_rix)
  mean_s <- lapply(mat_rix, mean)
  mean_s <- vapply(mat_rix, mean, numeric(1))
})  # end profvis
# four methods of calculating data frame column means
data_frame <- as.data.frame(mat_rix)
profvis::profvis({
  mean_s <- apply(data_frame, 2, mean)
  mean_s <- colMeans(data_frame)
  mean_s <- lapply(data_frame, mean)
  mean_s <- vapply(data_frame, mean, numeric(1))
})  # end profvis
# profile a shiny app
profvis::profvis(
  shiny::runExample(example="06_tabsets", 
                    display.mode="normal")
)  # end profvis
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Writing Fast \texttt{R} Code Using Vectorized Operations}


%%%%%%%%%%%%%%%
\subsection{Vectorized Functions for Vector Computations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Vectorized} functions accept \texttt{vectors} as their arguments, and return a vector of the same length as their value,
      \vskip1ex
      Many \emph{vectorized} functions are also \emph{compiled} (they pass their data to compiled \texttt{C++} code), which makes them very fast,
      \vskip1ex
      The following \emph{vectorized compiled} functions calculate cumulative values over large vectors:
      \begin{itemize}
        \item \texttt{cummax()}
        \item \texttt{cummin()}
        \item \texttt{cumsum()}
        \item \texttt{cumprod()}
      \end{itemize}
      Standard arithmetic operations (\texttt{"+", "-"}, etc.) can be applied to \texttt{vectors}, and are implemented as \emph{vectorized compiled} functions,
      \vskip1ex
      \texttt{ifelse()} and \texttt{which()} are \emph{vectorized compiled} functions for logical operations,
      \vskip1ex
      But many \emph{vectorized} functions perform their calculations in \texttt{R} code, and are therefore slow, but convenient to use,
    \column{0.5\textwidth}
        <<eval=FALSE>>=
vec_tor1 <- rnorm(1000000)
vec_tor2 <- rnorm(1000000)
big_vector <- numeric(1000000)
# sum two vectors in two different ways
summary(microbenchmark(
  # sum vectors using "for" loop
  r_loop=(for (i in 1:NROW(vec_tor1)) {
    big_vector[i] <- vec_tor1[i] + vec_tor2[i]
  }),
  # sum vectors using vectorized "+"
  vec_torized=(vec_tor1 + vec_tor2),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
# allocate memory for cumulative sum
cum_sum <- numeric(NROW(big_vector))
cum_sum[1] <- big_vector[1]
# calculate cumulative sum in two different ways
summary(microbenchmark(
# cumulative sum using "for" loop
  r_loop=(for (i in 2:NROW(big_vector)) {
    cum_sum[i] <- cum_sum[i-1] + big_vector[i]
  }),
# cumulative sum using "cumsum"
  vec_torized=cumsum(big_vector),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vectorized Functions for Matrix Computations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{apply()} loops are very inefficient for calculating statistics over rows and columns of very large matrices,
      \vskip1ex
      \texttt{R} has very fast \emph{vectorized compiled} functions for calculating sums and means of rows and columns:
      \begin{itemize}
        \item \texttt{rowSums()}
        \item \texttt{colSums()}
        \item \texttt{rowMeans()}
        \item \texttt{colMeans()}
      \end{itemize}
      These \emph{vectorized} functions are also \emph{compiled} functions, so they're very fast because they pass their data to compiled \texttt{C++} code, which performs the loop calculations,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<eval=FALSE>>=
# matrix with 5,000 rows
mat_rix <- matrix(rnorm(10000), ncol=2)
# calculate row sums two different ways
all.equal(rowSums(mat_rix), 
  apply(mat_rix, 1, sum))
summary(microbenchmark(
  row_sums=rowSums(mat_rix),
  ap_ply=apply(mat_rix, 1, sum),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fast \texttt{R} Code for Matrix Computations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functions \texttt{pmax()} and \texttt{pmin()} calculate the "parallel" maxima (minima) of multiple vector arguments,
      \vskip1ex
      \texttt{pmax()} and \texttt{pmin()} return a vector, whose \emph{n}-th element is equal to the maximum (minimum) of the \emph{n}-th elements of the arguments, with shorter vectors recycled if necessary,
      \vskip1ex
      \texttt{pmax.int()} and \texttt{pmin.int()} are methods of generic functions \texttt{pmax()} and \texttt{pmin()}, designed for atomic vectors,
      \vskip1ex
      \texttt{pmax()} can be used to quickly calculate the maximum values of rows of a matrix, by first converting the matrix columns into a list, and then passing them to \texttt{pmax()},
      \vskip1ex
      \texttt{pmax.int()} and \texttt{pmin.int()} are very fast because they are \emph{compiled} functions (compiled from \texttt{C++} code),
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(microbenchmark)
str(pmax)
# calculate row maximums two different ways
summary(microbenchmark(
  p_max=
    do.call(pmax.int,
      lapply(seq_along(mat_rix[1, ]),
        function(in_dex) mat_rix[, in_dex])),
  l_apply=unlist(
    lapply(seq_along(mat_rix[, 1]),
        function(in_dex) max(mat_rix[in_dex, ]))),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package \texttt{matrixStats} for Fast Matrix Computations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \texttt{matrixStats} contains functions for calculating aggregations over matrix columns and rows, and other matrix computations, such as:
      \begin{itemize}
        \item estimating location and scale: \texttt{rowRanges()}, \texttt{colRanges()}, and \texttt{rowMaxs()}, \texttt{rowMins()}, etc.,
        \item testing and counting values: \texttt{colAnyMissings()}, \texttt{colAnys()}, etc.,
        \item cumulative functions: \texttt{colCumsums()}, \texttt{colCummins()}, etc.,
        \item binning and differencing: \texttt{binCounts()}, \texttt{colDiffs()}, etc.,
      \end{itemize}
      A summary of \texttt{matrixStats} functions can be found under:\\
      \url{https://cran.r-project.org/web/packages/matrixStats/vignettes/matrixStats-methods.html}
      \vskip1ex
      The \texttt{matrixStats} functions are very fast because they are \emph{compiled} functions (compiled from \texttt{C++} code),
    \column{0.5\textwidth}
      \vspace{-1em}
      <<eval=FALSE>>=
library(matrixStats)  # load package matrixStats
# calculate row min values three different ways
summary(microbenchmark(
  row_mins=rowMins(mat_rix),
  p_min=
    do.call(pmin.int,
            lapply(seq_along(mat_rix[1, ]),
                   function(in_dex)
                     mat_rix[, in_dex])),
  as_data_frame=
    do.call(pmin.int,
            as.data.frame.matrix(mat_rix)),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Writing Fast \texttt{R} Code Using Vectorized Operations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{R}-style code is code that relies on \emph{vectorized compiled} functions, instead of \texttt{for()} loops,
      \vskip1ex
      \texttt{for()} loops in \texttt{R} are slow because they call functions multiple times, and individual function calls are compute-intensive and slow,
      \vskip1ex
      The brackets \texttt{"[]"} operator is a \emph{vectorized compiled} function, and is therefore very fast,
      \vskip1ex
      Vectorized assignments using brackets \texttt{"[]"} and \texttt{Boolean} or \texttt{integer} vectors to subset vectors or matrices are therefore preferable to \texttt{for()} loops,
      \vskip1ex
      \texttt{R} code that uses \emph{vectorized compiled} functions can be as fast as \texttt{C++} code,
      \vskip1ex
      \texttt{R}-style code is also very \emph{expressive}, i.e. it allows performing complex operations with very few lines of code,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<eval=FALSE>>=
summary(microbenchmark(  # assign values to vector three different ways
# fast vectorized assignment loop performed in C using brackets "[]"
  brack_ets={vec_tor <- numeric(10)
    vec_tor[] <- 2},
# slow because loop is performed in R
  for_loop={vec_tor <- numeric(10)
    for (in_dex in seq_along(vec_tor))
      vec_tor[in_dex] <- 2},
# very slow because no memory is pre-allocated
# "vec_tor" is "grown" with each new element
  grow_vec={vec_tor <- numeric(0)
    for (in_dex in 1:10)
# add new element to "vec_tor" ("grow" it)
      vec_tor[in_dex] <- 2},
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
summary(microbenchmark(  # assign values to vector two different ways
# fast vectorized assignment loop performed in C using brackets "[]"
  brack_ets={vec_tor <- numeric(10)
    vec_tor[4:7] <- rnorm(4)},
# slow because loop is performed in R
  for_loop={vec_tor <- numeric(10)
    for (in_dex in 4:7)
      vec_tor[in_dex] <- rnorm(1)},
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vectorized Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Functions which use vectorized operations and functions are automatically \emph{vectorized} themselves,
      \vskip1ex
      Functions which only call other compiled \texttt{C++} vectorized functions, are also very fast,
      \vskip1ex
      But not all functions are vectorized, or they're not vectorized with respect to their \emph{parameters},
      \vskip1ex
      Some \emph{vectorized} functions perform their calculations in \texttt{R} code, and are therefore slow, but convenient to use,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# define function vectorized automatically
my_fun <- function(in_put, pa_ram) {
  pa_ram*in_put
}  # end my_fun
# "in_put" is vectorized
my_fun(in_put=1:3, pa_ram=2)
# "pa_ram" is vectorized
my_fun(in_put=10, pa_ram=2:4)
# define vectors of parameters of rnorm()
std_devs <-
  structure(1:3, names=paste0("sd=", 1:3))
me_ans <-
  structure(-1:1, names=paste0("mean=", -1:1))
# "sd" argument of rnorm() isn't vectorized
rnorm(1, sd=std_devs)
# "mean" argument of rnorm() isn't vectorized
rnorm(1, mean=me_ans)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing \texttt{sapply()} Loops Over Function Parameters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Many functions aren't vectorized with respect to their \emph{parameters},
      \vskip1ex
      Performing \texttt{sapply()} loops over a function's parameters produces vector output,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# sapply produces desired vector output
set.seed(1121)
sapply(std_devs, function(std_dev) rnorm(n=2, sd=std_dev))
set.seed(1121)
sapply(std_devs, rnorm, n=2, mean=0)
set.seed(1121)
sapply(me_ans,
       function(me_an) rnorm(n=2, mean=me_an))
set.seed(1121)
sapply(me_ans, rnorm, n=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Creating Vectorized Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In order to \emph{vectorize} a function with respect to one of its \emph{parameters}, it's necessary to perform a loop over it,
      \vskip1ex
      The function \texttt{Vectorize()} performs an \texttt{apply()} loop over the arguments of a function, and returns a vectorized version of the function,
      \vskip1ex
      \texttt{Vectorize()} vectorizes the arguments passed to \texttt{"vectorize.args"},
      \vskip1ex
      \texttt{Vectorize()} is an example of a \emph{higher-order} function: it accepts a function as its argument and returns a function as its value,
      \vskip1ex
      Functions that are vectorized using \texttt{Vectorize()} or \texttt{apply()} loops are just as slow as \texttt{apply()} loops, but convenient to use,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# rnorm() vectorized with respect to "std_dev"
vec_rnorm <- function(n, mean=0, sd=1) {
  if (NROW(sd)==1)
    rnorm(n=n, mean=mean, sd=sd)
  else
    sapply(sd, rnorm, n=n, mean=mean)
}  # end vec_rnorm
set.seed(1121)
vec_rnorm(n=2, sd=std_devs)
# rnorm() vectorized with respect to "mean" and "sd"
vec_rnorm <- Vectorize(FUN=rnorm,
              vectorize.args=c("mean", "sd")
)  # end Vectorize
set.seed(1121)
vec_rnorm(n=2, sd=std_devs)
set.seed(1121)
vec_rnorm(n=2, mean=me_ans)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \texttt{mapply()} Functional}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{mapply()} functional is a multivariate version of \texttt{sapply()}, that allows calling a non-vectorized function in a vectorized way,
      \vskip1ex
      \texttt{mapply()} accepts a multivariate function passed to the \texttt{"FUN"} argument and any number of vector arguments passed to the dots \texttt{"..."},
      \vskip1ex
      \texttt{mapply()} calls \texttt{"FUN"} on the vectors passed to the dots \texttt{"..."}, one element at a time:
      \begin{multline*}
        mapply(FUN=fun, vec_1, vec_2, \ldots) = \\
        [ fun(vec_{1,1}, vec_{2,1}, \ldots), \ldots, \\
        fun(vec_{1,i}, vec_{2,i}, \ldots), \ldots ]
      \end{multline*}
      \texttt{mapply()} passes the first vector to the first argument of \texttt{"FUN"}, the second vector to the second argument, etc.
      \vskip1ex
      The first element of the output vector is equal to \texttt{"FUN"} called on the first elements of the input vectors, the second element is \texttt{"FUN"} called on the second elements, etc.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
str(sum)
# na.rm is bound by name
mapply(sum, 6:9, c(5, NA, 3), 2:6, na.rm=TRUE)
str(rnorm)
# mapply vectorizes both arguments "mean" and "sd"
mapply(rnorm, n=5, mean=me_ans, sd=std_devs)
mapply(function(in_put, e_xp) in_put^e_xp,
       1:5, seq(from=1, by=0.2, length.out=5))
      @
      The output of \texttt{mapply()} is a vector of length equal to the longest vector passed to the dots \texttt{"..."} argument, with the elements of the other vectors recycled if necessary,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vectorizing Functions Using \texttt{mapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{mapply()} functional is a multivariate version of \texttt{sapply()}, that allows calling a non-vectorized function in a vectorized way,
      \vskip1ex
      \texttt{mapply()} can be used to vectorize several function arguments simultaneously,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# rnorm() vectorized with respect to "mean" and "sd"
vec_rnorm <- function(n, mean=0, sd=1) {
  if (NROW(mean)==1 && NROW(sd)==1)
    rnorm(n=n, mean=mean, sd=sd)
  else
    mapply(rnorm, n=n, mean=mean, sd=sd)
}  # end vec_rnorm
# call vec_rnorm() on vector of "sd"
vec_rnorm(n=2, sd=std_devs)
# call vec_rnorm() on vector of "mean"
vec_rnorm(n=2, mean=me_ans)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vectorized \texttt{if-else} Statements Using Function \texttt{ifelse()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{ifelse()} performs \emph{vectorized} \texttt{if-else} statements on vectors,
      \vskip1ex
      \texttt{ifelse()} is much faster than performing an element-wise loop in \texttt{R},
        <<func_ifelse,eval=FALSE,echo=TRUE,fig.show='hide'>>=
# create two numeric vectors
vec_tor1 <- sin(0.25*pi*1:10)
vec_tor2 <- cos(0.25*pi*1:10)
# create third vector using 'ifelse'
vec_tor3 <- ifelse(vec_tor1 > vec_tor2,
                  vec_tor1, vec_tor2)
# cbind all three together
vec_tor4 <- cbind(vec_tor1, vec_tor2, vec_tor3)

# set plotting parameters
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0),
    cex.lab=0.8, cex.axis=0.8, cex.main=0.8,
    cex.sub=0.5)
# plot matrix
matplot(vec_tor4, type="l", lty="solid",
        col=c("green", "blue", "red"),
        lwd=2, xlab="", ylab="")
# add legend
legend(x="bottomright", legend=colnames(vec_tor4),
       title="", inset=0.05, cex=0.8, lwd=2,
       lty=1, col=c("green", "blue", "red"))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/func_ifelse-1}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Simulation}


%%%%%%%%%%%%%%%
\subsection{Monte Carlo Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Monte Carlo} simulation consists of generating random samples from a given probability distribution,
      \vskip1ex
      The \emph{Monte Carlo} data samples can then used to calculate different parameters of the probability distribution (moments, quantiles, etc.), and its functionals,
      \vskip1ex
      The \emph{quantile} of a probability distribution is the value of the \emph{random variable} \texttt{x}, such that the probability of values less than \texttt{x} is equal to the given \emph{probability} \texttt{p},
      \vskip1ex
      The \emph{quantile} of a data sample can be calculated by first sorting the sample, and then finding the value corresponding closest to the given \emph{probability} \texttt{p},
      \vskip1ex
      The function \texttt{quantile()} calculates the sample quantiles, but it's quite slow,
      \vskip1ex
      The function \texttt{sort()} returns a vector sorted into ascending order,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # reset random number generator
# sample from Standard Normal Distribution
len_gth <- 1000
sam_ple <- rnorm(len_gth)
# sample mean - MC estimate
mean(sam_ple)
# sample standard deviation - MC estimate
sd(sam_ple)
# Monte Carlo estimate of cumulative probability
sam_ple <- sort(sam_ple)
pnorm(1)
sum(sam_ple<1)/len_gth
# Monte Carlo estimate of quantile
conf_level <- 0.99
qnorm(conf_level)
sam_ple[conf_level*len_gth]
quantile(sam_ple, probs=conf_level)
# analyze the source code of quantile()
stats:::quantile.default
# microbenchmark quantile
library(microbenchmark)
summary(microbenchmark(
  sam_ple=sam_ple[cut_off],
  quan_tile=quantile(sam_ple, probs=conf_level),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Brownian Motion Using \texttt{while()} Loops}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{while()} loops are often used in simulations, when the number of required loops is unknown in advance,
      \vskip1ex
      Below is an example of a simulation of the path of \emph{Brownian Motion} crossing a barrier level,
      \vspace{-1em}
        <<simu_while,eval=FALSE,echo=(-(1:3)),fig.show='hide'>>=
x11(width=6, height=5)
par(oma=c(1, 1, 1, 1), mar=c(2, 2, 2, 1), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
set.seed(1121)  # reset random number generator
bar_rier <- 20  # barrier level
len_gth <- 1000  # number of simulation steps
pa_th <- numeric(len_gth)  # allocate path vector
pa_th[1] <- 0  # initialize path
in_dex <- 2  # initialize simulation index
while ((in_dex <= len_gth) &&
         (pa_th[in_dex - 1] < bar_rier)) {
# simulate next step
  pa_th[in_dex] <-
    pa_th[in_dex - 1] + rnorm(1)
  in_dex <- in_dex + 1  # advance in_dex
}  # end while
# fill remaining pa_th after it crosses bar_rier
if (in_dex <= len_gth)
  pa_th[in_dex:len_gth] <- pa_th[in_dex - 1]
# create daily time series starting 2011
ts_path <- ts(data=pa_th, frequency=365, start=c(2011, 1))
plot(ts_path, type="l", col="black",
     lty="solid", lwd=2, xlab="", ylab="")
abline(h=bar_rier, lwd=2, col="red")
title(main="Brownian motion crossing a barrier level", 
      line=0.5)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/simu_brown_barrier.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Brownian Motion Using Vectorized Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Simulations in \texttt{R} can be accelerated by pre-computing a vector of random numbers, instead of generatng them one at a time in a loop,
      \vskip1ex
      Vectors of random numbers allow using \emph{vectorized} functions, instead of inefficient (slow) \texttt{while()} loops,
      \vspace{-1em}
        <<simu_vector,eval=FALSE,echo=(-(1:3)),fig.show='hide'>>=
x11(width=6, height=5)
par(oma=c(1, 1, 1, 1), mar=c(2, 2, 2, 1), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
set.seed(1121)  # reset random number generator
bar_rier <- 20  # barrier level
len_gth <- 1000  # number of simulation steps
# simulate path of Brownian motion
pa_th <- cumsum(rnorm(len_gth))
# find index when pa_th crosses bar_rier
cro_ss <- which(pa_th > bar_rier)
# fill remaining pa_th after it crosses bar_rier
if (NROW(cro_ss)>0) {
  pa_th[(cro_ss[1]+1):len_gth] <-
    pa_th[cro_ss[1]]
}  # end if
# create daily time series starting 2011
ts_path <- ts(data=pa_th, frequency=365,
             start=c(2011, 1))
# create plot with horizontal line
plot(ts_path, type="l", col="black",
     lty="solid", lwd=2, xlab="", ylab="")
abline(h=bar_rier, lwd=2, col="red")
title(main="Brownian motion crossing a barrier level", 
      line=0.5)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/simu_brown_barrier.png}
      The trade-off between speed and memory usage: more memory may be used than necessary, since the simulation may stop before all the pre-computed random numbers are used up,
      \vskip1ex
      But the simulation is much faster because the path is simulated using \emph{vectorized} functions,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Estimators Using Bootstrap Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard errors of estimators can be calculated using a \emph{bootstrap} simulation,
      \vskip1ex
      The \emph{bootstrap} procedure generates new data by randomly sampling with replacement from the observed (empirical) data set,
      \vskip1ex
      The \emph{bootstrapped} dataset is used to re-calculate the estimator many times, producing a vector of values,
      \vskip1ex
      The \emph{bootstrapped} estimator values are then used to calculate the probability distribution of the estimator and its standard error,
      \vskip1ex
      Bootstrapping doesn't provide accurate estimates for estimators which are sensitive to the ordering and correlations in the data,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # reset random number generator
# sample from Standard Normal Distribution
len_gth <- 1000
sam_ple <- rnorm(len_gth)
# sample mean
mean(sam_ple)
# sample standard deviation
sd(sam_ple)
# bootstrap of sample mean and median
boot_strap <- sapply(1:10000, function(x) {
  boot_sample <- sam_ple[sample.int(len_gth,
                                    replace=TRUE)]
  c(mean=mean(boot_sample),
    median=median(boot_sample))
})  # end sapply
boot_strap[, 1:3]
# standard error from formula
sd(sam_ple)/sqrt(len_gth)
# standard error of mean from bootstrap
sd(boot_strap["mean", ])
# standard error of median from bootstrap
sd(boot_strap["median", ])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Bootstrapping Standard Errors Using Parallel Computing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bootstrap} procedure performs a loop, which naturally lends itself to parallel computing,
      \vskip1ex
      Different functions from package \emph{parallel} need to be called depending on the operating system (\emph{Windows}, \emph{Mac-OSX}, or \emph{Linux}),
      \vskip1ex
      The function \texttt{makeCluster()} starts running \texttt{R} processes on several CPU cores under \emph{Windows},
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs apply loops under \emph{Windows}, using parallel computing on several CPU cores,
      \vskip1ex
      The \texttt{R} processes started by \texttt{makeCluster()} don't inherit any data from the parent \texttt{R} process,
      \vskip1ex
      Therefore the required data must be passed into \texttt{parLapply()} via the dots \texttt{"..."} argument,
      \vskip1ex
      The function \texttt{mclapply()} performs apply loops using parallel computing on several CPU cores under \emph{Mac-OSX} or \emph{Linux},
      \vskip1ex
      The function \texttt{stopCluster()} stops the \texttt{R} processes running on several CPU cores,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # load package parallel
num_cores <- detectCores() - 1  # number of cores
clus_ter <- makeCluster(num_cores)  # initialize compute cluster under Windows
set.seed(1121)  # reset random number generator
# sample from Standard Normal Distribution
len_gth <- 1000
sam_ple <- rnorm(len_gth)
# bootstrap mean and median under Windows
boot_strap <- parLapply(clus_ter, 1:10000,
  function(x, sam_ple, len_gth) {
  boot_sample <- sam_ple[sample.int(len_gth, replace=TRUE)]
  c(mean=mean(boot_sample), median=median(boot_sample))
  }, sam_ple=sam_ple, len_gth=len_gth)  # end parLapply
# bootstrap mean and median under Mac-OSX or Linux
boot_strap <- mclapply(1:10000,
  function(x) {
  boot_sample <- sam_ple[sample.int(len_gth, replace=TRUE)]
  c(mean=mean(boot_sample), median=median(boot_sample))
  }, mc.cores=num_cores)  # end mclapply
boot_strap <- rutils::do_call(rbind, boot_strap)
# means and standard errors from bootstrap
apply(boot_strap, MARGIN=2,
      function(x) c(mean=mean(x), sd=sd(x)))
# standard error from formula
sd(sam_ple)/sqrt(len_gth)
stopCluster(clus_ter)  # stop R processes over cluster under Windows
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Bootstrapping From Empirical Datasets}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Bootstrapping is usually performed by resampling from an observed (empirical) dataset,
      \vskip1ex
      Resampling consists of randomly selecting data from an existing dataset, with replacement,
      \vskip1ex
      Resampling produces a new \emph{bootstrapped} dataset with similar properties to the existing dataset,
      \vskip1ex
      The \emph{bootstrapped} dataset is used to re-calculate the estimator many times, producing a vector of values,
      \vskip1ex
      The \emph{bootstrapped} estimator values are then used to calculate the probability distribution of the estimator and its standard error,
      \vskip1ex
      Bootstrapping doesn't provide accurate estimates for estimators which are sensitive to the ordering and correlations in the data,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# sample from time series of ETF returns
sam_ple <- rutils::etf_env$re_turns[, "VTI"]
sam_ple <- na.omit(sam_ple)
len_gth <- NROW(sam_ple)
# bootstrap mean and median under Windows
library(parallel)  # load package parallel
num_cores <- detectCores() - 1  # number of cores
clus_ter <- makeCluster(num_cores)  # initialize compute cluster under Windows
clusterSetRNGStream(clus_ter, 1121)  # reset random number generator in all cores
num_boot <- 1e4
boot_strap <- parLapply(clus_ter, 1:num_boot,
  function(x, sam_ple, len_gth) {
    boot_sample <- sam_ple[sample.int(len_gth, replace=TRUE)]
    c(sd=sd(boot_sample), mad=mad(boot_sample))
  }, sam_ple=sam_ple, len_gth=len_gth)  # end parLapply
# bootstrap mean and median under Mac-OSX or Linux
boot_strap <- mclapply(1:num_boot,
  function(x) {
    boot_sample <- sam_ple[sample.int(len_gth, replace=TRUE)]
    c(mean=mean(boot_sample), median=median(boot_sample))
  }, mc.cores=num_cores)  # end mclapply
boot_strap <- rutils::do_call(rbind, boot_strap)
# means and standard errors from bootstrap
apply(boot_strap, MARGIN=2,
      function(x) c(mean=mean(x), sd=sd(x)))
# standard error assuming normal distribution of returns
sd(sam_ple)/sqrt(num_boot)
stopCluster(clus_ter)  # stop R processes over cluster under Windows
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Bootstrapping From Time Series of Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Bootstrapping from a time series of prices requires first converting the prices to percentage returns, then bootstrapping the returns, and finally converting them back to prices,
      <<echo=TRUE,eval=FALSE>>=
# load time series of ETF prices
sam_ple <- rutils::etf_env$VTI[, 4]
len_gth <- NROW(sam_ple)
# calculate percentage returns
sam_ple <- rutils::diff_it(log(sam_ple))
# define barrier level with respect to sam_ple
bar_rier <- 0.8*max(exp(cumsum(sam_ple)))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # load package parallel
num_cores <- detectCores() - 1  # number of cores
clus_ter <- makeCluster(num_cores)  # initialize compute cluster under Windows
num_boot <- 1e4
# perform parallel bootstrap under Windows
set.seed(1121)  # reset random number generator
boot_strap <- parLapply(clus_ter, 1:num_boot,
  function(x, sam_ple, len_gth, bar_rier) {
    boot_sample <- sam_ple[sample.int(len_gth, replace=TRUE)]
    # calculate prices from percentage returns
    boot_sample <- exp(cumsum(boot_sample))
    # calculate payout
    sum(boot_sample > bar_rier) > 0
  }, sam_ple=sam_ple, len_gth=len_gth, bar_rier=bar_rier)  # end parLapply
stopCluster(clus_ter)  # stop R processes over cluster under Windows
# perform parallel bootstrap under Mac-OSX or Linux
boot_strap <- mclapply(1:num_boot,
  function(x) {
    boot_sample <- sam_ple[sample.int(len_gth, replace=TRUE)]
    # calculate prices from percentage returns
    boot_sample <- exp(cumsum(boot_sample))
    # calculate payout
    sum(boot_sample > bar_rier) > 0
  }, mc.cores=num_cores)  # end mclapply
boot_strap <- rutils::do_call(rbind, boot_strap)
# calculate probability of crossing barrier
sum(boot_strap)/num_boot
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Variance Reduction Using Antithetic Sampling}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Antithetic Sampling} is a \emph{Variance Reduction} technique in which a new random sample is computed from an existing sample, without generating new random numbers,
      \vskip1ex
      In the case of a \emph{Normal} random sample $\phi$, the new \emph{antithetic} sample is equal to minus the existing sample: $\phi_{new} = -\phi$, 
      \vskip1ex
      In the case of a \emph{Uniform} random sample $\phi$, the new \emph{antithetic} sample is equal to \texttt{1} minus the existing sample: $\phi_{new} = 1-\phi$, 
      \vskip1ex
      \emph{Antithetic Sampling} doubles the number of independent samples, so it reduces the standard error by $\sqrt{2}$,
      \vskip1ex
      \emph{Antithetic Sampling} doesn't change any other parameters of the simulation,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # reset random number generator
# sample from Standard Normal Distribution
len_gth <- 1000
sam_ple <- rnorm(len_gth)
# estimate the 95% quantile
boot_strap <- sapply(1:10000, function(x) {
  boot_sample <- sam_ple[sample.int(len_gth,
                                    replace=TRUE)]
  quantile(boot_sample, 0.95)
})  # end sapply
sd(boot_strap)
# estimate the 95% quantile using antithetic sampling
boot_strap <- sapply(1:10000, function(x) {
  boot_sample <- sam_ple[sample.int(len_gth,
                                    replace=TRUE)]
  quantile(c(boot_sample, -boot_sample), 0.95)
})  # end sapply
# standard error of quantile from bootstrap
sd(boot_strap)
sqrt(2)*sd(boot_strap)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Regression Coefficients Using Bootstrap}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard errors of the regression coefficients can be calculated using a \emph{bootstrap} simulation,
      \vskip1ex
      The \emph{bootstrap} procedure creates new design matrices by randomly sampling with replacement from the design matrix,
      \vskip1ex
      Regressions are performed on the \emph{bootstrapped} design matrices, and the regression coefficients are saved into a matrix of \emph{bootstrapped} coefficients,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # initialize random number generator
# define explanatory and response variables
de_sign <- rnorm(100, mean=2)
noise <- rnorm(100)
res_ponse <- -3 + de_sign + noise
# define design matrix and regression formula
de_sign <- data.frame(res_ponse, de_sign)
for_mula <- paste(colnames(de_sign)[1],
  paste(colnames(de_sign)[-1], collapse="+"),
  sep=" ~ ")
# bootstrap the regression
boot_strap <- sapply(1:100, function(x) {
  boot_sample <- sample.int(dim(de_sign)[1],
                            replace=TRUE)
  mod_el <- lm(for_mula,
                data=de_sign[boot_sample, ])
  mod_el$coefficients
})  # end sapply
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Bootstrapped Regression Coefficients}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bootstrapped} coefficient values can be used to calculate the probability distribution of the coefficients and their standard errors,
        <<boot_distribution,eval=FALSE,echo=(-(1:2)),fig.show='hide'>>=
par(oma=c(1, 2, 1, 0), mgp=c(2, 1, 0), mar=c(5, 1, 1, 1), cex.lab=0.8, cex.axis=1.0, cex.main=0.8, cex.sub=0.5)
x11(width=6, height=6)
# means and standard errors from bootstrap
apply(boot_strap, MARGIN=1,
      function(x) c(mean=mean(x), sd=sd(x)))
# means and standard errors from regression summary
mod_el <- lm(for_mula, data=de_sign)
mod_el$coefficients
summary(mod_el)$coefficients[, "Std. Error"]
plot(density(boot_strap["de_sign", ]),
     lwd=2, xlab="regression slopes",
     main="Bootstrapped regression slopes")
abline(v=mean(boot_strap["de_sign", ]),
       lwd=2, col="red")
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/boot_reg.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Bootstrapping Regressions Using Parallel Computing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bootstrap} procedure performs a loop, which naturally lends itself to parallel computing,
      \vskip1ex
      Different functions from package \emph{parallel} need to be called depending on the operating system (\emph{Windows}, \emph{Mac-OSX}, or \emph{Linux}),
      \vskip1ex
      The function \texttt{makeCluster()} starts running \texttt{R} processes on several CPU cores under \emph{Windows},
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs apply loops under \emph{Windows}, using parallel computing on several CPU cores,
      \vskip1ex
      The \texttt{R} processes started by \texttt{makeCluster()} don't inherit any data from the parent \texttt{R} process,
      \vskip1ex
      Therefore the required data must be passed into \texttt{parLapply()} via the dots \texttt{"..."} argument,
      \vskip1ex
      The function \texttt{mclapply()} performs apply loops using parallel computing on several CPU cores under \emph{Mac-OSX} or \emph{Linux},
      \vskip1ex
      The function \texttt{stopCluster()} stops the \texttt{R} processes running on several CPU cores,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # load package parallel
num_cores <- detectCores() - 1  # number of cores
clus_ter <- makeCluster(num_cores)  # initialize compute cluster under Windows
# bootstrap the regression under Windows
boot_strap <- parLapply(clus_ter, 1:1000,
  function(x, for_mula, de_sign) {
    boot_sample <-
      sample.int(dim(de_sign)[1], replace=TRUE)
    mod_el <- lm(for_mula,
      data=de_sign[boot_sample, ])
    mod_el$coefficients
  },
  for_mula=for_mula,
  de_sign=de_sign)  # end parLapply
# bootstrap the regression under Mac-OSX or Linux
boot_strap <- mclapply(1:1000,
  function(x) {
    boot_sample <-
      sample.int(dim(de_sign)[1], replace=TRUE)
    lm(for_mula,
      data=de_sign[boot_sample, ])$coefficients
  }, mc.cores=num_cores)  # end mclapply
stopCluster(clus_ter)  # stop R processes over cluster under Windows
boot_strap <- rutils::do_call(rbind, boot_strap)
# means and standard errors from bootstrap
apply(boot_strap, MARGIN=2,
      function(x) c(mean=mean(x), sd=sd(x)))
x11(width=6, height=6)
plot(density(boot_strap[, "de_sign"]),
     lwd=2, xlab="regression slopes",
     main="Bootstrapped regression slopes")
abline(v=mean(boot_strap[, "de_sign"]),
       lwd=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Parallel Computing}


%%%%%%%%%%%%%%%
\subsection{Parallel Computing in \texttt{R}}
\begin{frame}[t]{\subsecname}
\vspace{-1em}

\begin{block}{Parallel Computing in \texttt{R}}
      Parallel computing means splitting a computing task into separate sub-tasks, and then simultaneously computing the sub-tasks on several computers or CPU cores,
      \vskip1ex
      There are many different packages that allow parallel computing in \texttt{R}, most importantly package \emph{parallel}, and packages \texttt{foreach}, \texttt{doParallel}, and related packages:\\
      \hskip1em\url{http://cran.r-project.org/web/views/HighPerformanceComputing.html}\\
      \hskip1em\url{http://blog.revolutionanalytics.com/high-performance-computing/}\\
      \hskip1em\url{http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/}\\
\end{block}

\begin{block}{\texttt{R} Base Package \emph{parallel}}
  The package \emph{parallel} provides functions for parallel computing using multiple cores of CPUs,
  \vskip1ex
  The package \emph{parallel} is part of the standard \texttt{R} distribution, so it doesn't need to be installed.\\
  \hskip1em\url{http://adv-r.had.co.nz/Profiling.html\#parallelise}\\
  \hskip1em\url{https://github.com/tobigithub/R-parallel/wiki/R-parallel-package-overview}\\
\end{block}

\begin{block}{Packages \texttt{foreach}, \texttt{doParallel}, and Related Packages}
      \hskip1em\url{http://blog.revolutionanalytics.com/2015/10/updates-to-the-foreach-package-and-its-friends.html}\\
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Parallel Computing Using Package \protect\emph{parallel}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{parallel} provides functions for parallel computing using multiple cores of CPUs,
      \vskip1ex
      The package \emph{parallel} is part of the standard \texttt{R} distribution, so it doesn't need to be installed,
      \vskip1ex
      Different functions from package \emph{parallel} need to be called depending on the operating system (\emph{Windows}, \emph{Mac-OSX}, or \emph{Linux}),
    \column{0.5\textwidth}
      \vspace{-1em}
      <<eval=FALSE>>=
library(parallel)  # load package parallel
# get short description
packageDescription("parallel")
# load help page
help(package="parallel")
# list all objects in "parallel"
ls("package:parallel")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Parallel Loops Using Package \protect\emph{parallel}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Some computing tasks naturally lend themselves to parallel computing, like for example performing loops,
      \vskip1ex
      Different functions from package \emph{parallel} need to be called depending on the operating system (\emph{Windows}, \emph{Mac-OSX}, or \emph{Linux}),
      \vskip1ex
      The function \texttt{mclapply()} performs apply loops (similar to \texttt{lapply()}) using parallel computing on several CPU cores under \emph{Mac-OSX} or \emph{Linux},
      \vskip1ex
      Under \emph{Windows}, a cluster of \texttt{R} processes (one per each CPU core) need to be started first, by calling the function \texttt{makeCluster()},
      \vskip1ex
      \emph{Mac-OSX} and \emph{Linux} don't require calling the function \texttt{makeCluster()},
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs apply loops under \emph{Windows}, using parallel computing on several CPU cores,
      \vskip1ex
      The function \texttt{stopCluster()} stops the \texttt{R} processes running on several CPU cores,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # load package parallel
# calculate number of available cores
num_cores <- detectCores() - 1
# define function that pauses execution
paws <- function(x, sleep_time) {
  Sys.sleep(sleep_time)
  x
}  # end paws
# perform parallel loop under Mac-OSX or Linux
paw_s <- mclapply(1:10, paws, mc.cores=num_cores,
                sleep_time=0.01)
# initialize compute cluster under Windows
clus_ter <- makeCluster(num_cores)
# perform parallel loop under Windows
paw_s <- parLapply(clus_ter, 1:10, paws,
                 sleep_time=0.01)
library(microbenchmark)  # load package microbenchmark
# compare speed of lapply versus parallel computing
summary(microbenchmark(
  l_apply=lapply(1:10, paws, sleep_time=0.01),
  parl_apply=
    parLapply(clus_ter, 1:10, paws, sleep_time=0.01),
  times=10)
)[, c(1, 4, 5)]
# stop R processes over cluster under Windows
stopCluster(clus_ter)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Computing Overhead of Parallel Computing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Parallel computing requires additional resources and time for distributing the computing tasks and collecting the output, which produces a computing overhead,
      \vskip1ex
      Therefore parallel computing can actually be slower for small computations, or for computations that can't be naturally separated into sub-tasks,
      <<echo=(-(1:10)),eval=FALSE>>=
library(parallel)  # load package parallel
# calculate number of available cores
num_cores <- detectCores() - 1
# initialize compute cluster under Windows
clus_ter <- makeCluster(num_cores)
# define function that pauses execution
paws <- function(x, sleep_time) {
  Sys.sleep(sleep_time)
  x
}  # end paws
# compare speed of lapply with parallel computing
iter_ations <- 3:10
compute_times <- sapply(iter_ations,
  function(max_iterations, sleep_time) {
    out_put <- summary(microbenchmark(
      lapply=lapply(1:max_iterations, paws,
                    sleep_time=sleep_time),
      parallel=parLapply(clus_ter, 1:max_iterations,
              paws, sleep_time=sleep_time),
      times=10))[, c(1, 4)]
    structure(out_put[, 2],
              names=as.vector(out_put[, 1]))
    }, sleep_time=0.01)
compute_times <- t(compute_times)
rownames(compute_times) <- iter_ations
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/parallel_plot.png}\\
      \vspace{-1em}
      <<parallel_plot,echo=(-(1:1)),eval=FALSE>>=
library(parallel)  # load package parallel
plot(x=rownames(compute_times),
     y=compute_times[, "lapply"],
     type="l", lwd=2, col="blue",
     main="Compute times",
     xlab="number of iterations in loop", ylab="",
     ylim=c(0, max(compute_times[, "lapply"])))
lines(x=rownames(compute_times),
      y=compute_times[, "parallel"], lwd=2, col="green")
legend(x="topleft", legend=colnames(compute_times),
       inset=0.1, cex=1.0, bg="white",
       lwd=2, lty=c(1, 1), col=c("blue", "green"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Parallel Computing Over Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Very often we need to perform time consuming calculations over columns of matrices,
      \vskip1ex
      The function \texttt{parCapply()} performs an apply loop over columns of matrices using parallel computing on several CPU cores,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:5)),eval=FALSE>>=
library(parallel)  # load package parallel
# calculate number of available cores
num_cores <- detectCores() - 1
# initialize compute cluster under Windows
clus_ter <- makeCluster(num_cores)
# define large matrix
mat_rix <- matrix(rnorm(7*10^5), ncol=7)
# define aggregation function over column of matrix
agg_regate <- function(col_umn) {
  out_put <- 0
  for (in_dex in 1:NROW(col_umn))
    out_put <- out_put + col_umn[in_dex]
  out_put
}  # end agg_regate
# perform parallel aggregations over columns of matrix
agg_regations <-
  parCapply(clus_ter, mat_rix, agg_regate)
# compare speed of apply with parallel computing
summary(microbenchmark(
  ap_ply=apply(mat_rix, MARGIN=2, agg_regate),
  parl_apply=
    parCapply(clus_ter, mat_rix, agg_regate),
  times=10)
)[, c(1, 4, 5)]
# stop R processes over cluster under Windows
stopCluster(clus_ter)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Initializing Parallel Clusters Under \protect\emph{Windows}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Under \emph{Windows} the child processes in the parallel compute cluster don't inherit data and objects from their parent process,
      \vskip1ex
      Therefore the required data must be either passed into \texttt{parLapply()} via the dots \texttt{"..."} argument, or by calling the function \texttt{clusterExport()},
      \vskip1ex
      Objects from packages must be either referenced using the double-colon operator \texttt{"::"}, or the packages must be loaded in the child processes,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:5)),eval=FALSE>>=
library(parallel)  # load package parallel
# calculate number of available cores
num_cores <- detectCores() - 1
# initialize compute cluster under Windows
clus_ter <- makeCluster(num_cores)
ba_se <- 2
# fails because child processes don't know ba_se:
parLapply(clus_ter, 2:4,
          function(exponent) ba_se^exponent)
# ba_se passed to child via dots ... argument:
parLapply(clus_ter, 2:4,
          function(exponent, ba_se) ba_se^exponent,
          ba_se=ba_se)
# ba_se passed to child via clusterExport:
clusterExport(clus_ter, "ba_se")
parLapply(clus_ter, 2:4,
          function(exponent) ba_se^exponent)
# fails because child processes don't know zoo::index():
parSapply(clus_ter, c("VTI", "IEF", "DBC"),
          function(sym_bol)
            NROW(index(get(sym_bol, envir=rutils::etf_env))))
# zoo function referenced using "::" in child process:
parSapply(clus_ter, c("VTI", "IEF", "DBC"),
          function(sym_bol)
            NROW(zoo::index(get(sym_bol, envir=rutils::etf_env))))
# package zoo loaded in child process:
parSapply(clus_ter, c("VTI", "IEF", "DBC"),
          function(sym_bol) {
            stopifnot("package:zoo" %in% search() || require("zoo", quietly=TRUE))
            NROW(index(get(sym_bol, envir=rutils::etf_env)))
          })  # end parSapply
# stop R processes over cluster under Windows
stopCluster(clus_ter)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reproducible Parallel Simulations Under \protect\emph{Windows}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Simulations use pseudo-random number generators, and in order to perform reproducible results, they must set the \emph{seed} value, so that the number generators produce the same sequence of pseudo-random numbers,
      \vskip1ex
      The function \texttt{set.seed()} initializes the random number generator by specifying the \emph{seed} value, so that the number generator produces the same sequence of numbers for a given \emph{seed} value,
      \vskip1ex
      But under \emph{Windows} \texttt{set.seed()} doesn't initialize the random number generators of child processes, and they don't produce the same sequence of numbers,
      \vskip1ex
      The function \texttt{clusterSetRNGStream()} initializes the random number generators of child processes under \emph{Windows},
      \vskip1ex
      The function \texttt{set.seed()} does initialize the random number generators of child processes under \emph{Mac-OSX} and \emph{Linux},
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # load package parallel
# calculate number of available cores
num_cores <- detectCores() - 1
# initialize compute cluster under Windows
clus_ter <- makeCluster(num_cores)
# set seed for cluster under Windows
# doesn't work: set.seed(1121)
clusterSetRNGStream(clus_ter, 1121)
# perform parallel loop under Windows
out_put <- parLapply(clus_ter, 1:70, rnorm, n=100)
sum(unlist(out_put))
# stop R processes over cluster under Windows
stopCluster(clus_ter)
# perform parallel loop under Mac-OSX or Linux
out_put <- mclapply(1:10, rnorm, mc.cores=num_cores, n=100)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Optimization}


%%%%%%%%%%%%%%%
\subsection{One-dimensional Optimization Using The Functional \texttt{optimize()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{optimize()} performs \emph{one-dimensional} optimization over a single independent variable,
      \vskip1ex
      \texttt{optimize()} searches for the minimum of the objective function with respect to its first argument, in the specified interval,
      \vskip1ex
      \texttt{optimize()} returns a list containing the location of the minimum and the objective function value,
        <<eval=FALSE,echo=(-(1:1))>>=
options(width=50, dev='pdf')
str(optimize)
# objective function with multiple minima
object_ive <- function(in_put, param1=0.01) {
  sin(0.25*pi*in_put) + param1*(in_put-1)^2
}  # end object_ive
unlist(optimize(f=object_ive, interval=c(-4, 2)))
unlist(optimize(f=object_ive, interval=c(0, 8)))
options(width=60, dev='pdf')
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/optim_one_dim-1}
      \vspace{-4em}
        <<optim_one_dim,eval=FALSE,echo=(-(1:1)),fig.show='hide'>>=
par(oma=c(1, 1, 1, 1), mgp=c(2, 1, 0), mar=c(5, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# plot the objective function
curve(expr=object_ive, type="l", xlim=c(-8, 9),
xlab="", ylab="", lwd=2)
# add title
title(main="Objective Function", line=-1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{rgl} for Interactive 3d Surface Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{persp3d()} plots an \emph{interactive} 3d surface plot of a function or a matrix,
      \vskip1ex
      \emph{rgl} is an \texttt{R} package for 3d and perspective plotting, based on the \emph{OpenGL} framework,
      <<rgl_surf3d,eval=FALSE,echo=TRUE,rgl=TRUE,fig.width=6,fig.height=6,fig.show="hide">>=
library(rgl)  # load rgl
# define function of two variables
sur_face <- function(x, y) y*sin(x)
# draw 3d surface plot of function
persp3d(x=sur_face, xlim=c(-5, 5), ylim=c(-5, 5),
        col="green", axes=FALSE)
# draw 3d surface plot of matrix
x_lim <- seq(from=-5, to=5, by=0.1)
y_lim <- seq(from=-5, to=5, by=0.1)
persp3d(z=outer(x_lim, y_lim, FUN=sur_face),
        xlab="x", ylab="y", zlab="sur_face",
        col="green")
# save current view to png file
rgl.snapshot("surface_plot.png")
# define function of two variables and two parameters
sur_face <- function(x, y, par_1=1, par_2=1)
  sin(par_1*x)*sin(par_2*y)
# draw 3d surface plot of function
persp3d(x=sur_face, xlim=c(-5, 5), ylim=c(-5, 5),
        col="green", axes=FALSE,
        par_1=1, par_2=2)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/rgl_surf3d.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Multi-dimensional Optimization Using \texttt{optim()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{optim()} performs \emph{multi-dimensional} optimization,
      \vskip1ex
      The argument \texttt{fn} is the objective function to be minimized,
      \vskip1ex
      The argument of \texttt{fn} that is to be optimized, must be a vector argument,
      \vskip1ex
      The argument \texttt{par} is the initial vector argument value,
      \vskip1ex
      \texttt{optim()} accepts additional parameters bound to the dots \texttt{"..."} argument, and passes them to the \texttt{fn} objective function,
      \vskip1ex
      The arguments \texttt{lower} and \texttt{upper} specify the search range for the variables of the objective function \texttt{fn},
      \vskip1ex
      \texttt{method="L-BFGS-B"} specifies the quasi-Newton \emph{gradient} optimization method,
      \vskip1ex
      \texttt{optim()} returns a list containing the location of the minimum and the objective function value,
      \vskip1ex
      The \emph{gradient} methods used by \texttt{optim()} can only find the local minimum, not the global minimum, 
    \column{0.5\textwidth}
        <<eval=FALSE,echo=TRUE>>=
# Rastrigin function with vector argument for optimization
rastri_gin <- function(vec_tor, pa_ram=25){
  sum(vec_tor^2 - pa_ram*cos(vec_tor))
}  # end rastri_gin
vec_tor <- c(pi/6, pi/6)
rastri_gin(vec_tor=vec_tor)
# draw 3d surface plot of Rastrigin function
rgl::persp3d(
  x=Vectorize(function(x, y) rastri_gin(vec_tor=c(x, y))), 
  xlim=c(-10, 10), ylim=c(-10, 10),
  col="green", axes=FALSE, zlab="", main="rastri_gin")
# optimize with respect to vector argument
op_tim <- optim(par=vec_tor, fn=rastri_gin,
                method="L-BFGS-B",
                upper=c(4*pi, 4*pi),
                lower=c(pi/2, pi/2),
                pa_ram=1)
# optimal parameters and value
op_tim$par
op_tim$value
rastri_gin(op_tim$par, pa_ram=1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Log-likelihood Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{likelihood} function $\mathcal{L}(\theta|\bar{x})$ is a function of the parameters of a statistical model $(\theta)$, given a sample of observed values $(\bar{x})$, taken under the model's probability distribution $P(x|\theta)$:
      \begin{displaymath}
        \mathcal{L}(\theta|x) = \prod_{i=1}^n P(x_i|\theta)
      \end{displaymath}
      The \emph{likelihood} function measures how \emph{likely} are the parameters of a statistical model, given a sample of observed values $(\bar{x})$,
      \vskip1ex
      The \emph{maximum-likelihood} estimate (\emph{MLE}) of the model's parameters are those that maximize the \emph{likelihood} function:
      \begin{displaymath}
        \theta_{MLE} = \operatorname*{arg\,max}_{\theta} {\mathcal{L}(\theta|x)}
      \end{displaymath}
      In practice the logarithm of the \emph{likelihood} $\log(\mathcal{L})$ is maximized, instead of the \emph{likelihood} itself,
      \vskip1ex
      The function \texttt{outer()} calculates the \emph{outer} product of two matrices, and by default multiplies the elements of its arguments,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# sample of normal variables
sam_ple <- rnorm(1000, mean=4, sd=2)
# objective function is log-likelihood
object_ive <- function(pa_r, sam_ple) {
  sum(2*log(pa_r[2]) +
    ((sam_ple - pa_r[1])/pa_r[2])^2)
}  # end object_ive
# vectorize objective function
vec_objective <- Vectorize(
  FUN=function(mean, sd, sam_ple)
    object_ive(c(mean, sd), sam_ple),
  vectorize.args=c("mean", "sd")
)  # end Vectorize
# objective function on parameter grid
par_mean <- seq(1, 6, length=50)
par_sd <- seq(0.5, 3.0, length=50)
objective_grid <- outer(par_mean, par_sd,
        vec_objective, sam_ple=sam_ple)
objective_min <- which(  # grid search
  objective_grid==min(objective_grid),
  arr.ind=TRUE)
objective_min
par_mean[objective_min[1]]  # mean
par_sd[objective_min[2]]  # sd
objective_grid[objective_min]
objective_grid[(objective_min[, 1] + -1:1),
               (objective_min[, 2] + -1:1)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Perspective Plot of Likelihood Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{persp()} plots a 3d perspective surface plot of a function specified over a grid of argument values,
      \vskip1ex
      The argument \texttt{"z"} accepts a matrix containing the function values,
      \vskip1ex
      \texttt{persp()} belongs to the base \texttt{graphics} package, and doesn't create interactive plots,
      \vskip1ex
      The function \texttt{persp3d()} plots an \emph{interactive} 3d surface plot of a function or a matrix,
      \vskip1ex
      \texttt{rgl} is an \texttt{R} package for 3d and perspective plotting, based on the \emph{OpenGL} framework,
      <<optim_objective,echo=(-(1:1)),eval=FALSE,fig.width=10,fig.height=10,fig.show='hide'>>=
par(cex.lab=2.0, cex.axis=2.0, cex.main=2.0, cex.sub=2.0)
# perspective plot of log-likelihood function
persp(z=-objective_grid,
      theta=45, phi=30, shade=0.5,
      border="green", zlab="objective",
      main="objective function")
# interactive perspective plot of log-likelihood function
library(rgl)  # load package rgl
par3d(cex=2.0)  # scale text by factor of 2
persp3d(z=-objective_grid, zlab="objective",
        col="green", main="objective function")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/optim_objective-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimization of Objective Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{optim()} performs optimization of an objective function,
      \vskip1ex
      The function \texttt{fitdistr()} from package \emph{MASS} fits a univariate distribution to a sample of data, by performing \emph{maximum likelihood} optimization,
\vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# initial parameters
par_init <- c(mean=0, sd=1)
# perform optimization using optim()
optim_fit <- optim(par=par_init,
  fn=object_ive, # log-likelihood function
  sam_ple=sam_ple,
  method="L-BFGS-B", # quasi-Newton method
  upper=c(10, 10), # upper constraint
  lower=c(-10, 0.1)) # lower constraint
# optimal parameters
optim_fit$par
# perform optimization using MASS::fitdistr()
optim_fit <- MASS::fitdistr(sam_ple, densfun="normal")
optim_fit$estimate
optim_fit$sd
      @
\vspace{-2em}
      <<optim_basic,echo=TRUE,eval=FALSE,fig.width=5,fig.height=5,fig.show='hide'>>=
# plot histogram
histo_gram <- hist(sam_ple, plot=FALSE)
plot(histo_gram, freq=FALSE,
     main="histogram of sample")
curve(expr=dnorm(x, mean=optim_fit$par["mean"],
                 sd=optim_fit$par["sd"]),
      add=TRUE, type="l", lwd=2, col="red")
legend("topright", inset=0.0, cex=0.8, title=NULL,
       leg="optimal parameters",
       lwd=2, bg="white", col="red")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/optim_basic-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Mixture Model Likelihood Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
\vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# sample from mixture of normal distributions
sam_ple <- c(rnorm(100, sd=1.0),
                   rnorm(100, mean=4, sd=1.0))
# objective function is log-likelihood
object_ive <- function(pa_r, sam_ple) {
  likelihood <- pa_r[1]/pa_r[3] *
  dnorm((sam_ple-pa_r[2])/pa_r[3]) +
  (1-pa_r[1])/pa_r[5]*dnorm((sam_ple-pa_r[4])/pa_r[5])
  if (any(likelihood <= 0)) Inf else
    -sum(log(likelihood))
}  # end object_ive
# vectorize objective function
vec_objective <- Vectorize(
  FUN=function(mean, sd, w, m1, s1, sam_ple)
    object_ive(c(w, m1, s1, mean, sd), sam_ple),
  vectorize.args=c("mean", "sd")
)  # end Vectorize
# objective function on parameter grid
par_mean <- seq(3, 5, length=50)
par_sd <- seq(0.5, 1.5, length=50)
objective_grid <- outer(par_mean, par_sd,
          vec_objective, sam_ple=sam_ple,
          w=0.5, m1=2.0, s1=2.0)
rownames(objective_grid) <- round(par_mean, 2)
colnames(objective_grid) <- round(par_sd, 2)
objective_min <- which(objective_grid==
  min(objective_grid), arr.ind=TRUE)
objective_min
objective_grid[objective_min]
objective_grid[(objective_min[, 1] + -1:1),
               (objective_min[, 2] + -1:1)]
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      <<optim_mix_like,echo=TRUE,eval=FALSE,fig.width=10,fig.height=10,fig.show='hide'>>=
# perspective plot of objective function
persp(par_mean, par_sd, -objective_grid,
      theta=45, phi=30,
      shade=0.5,
      col=rainbow(50),
      border="green",
      main="objective function")
      @
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/optim_mix_like-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimization of Mixture Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
\vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# initial parameters
par_init <- c(weight=0.5, m1=0, s1=1, m2=2, s2=1)
# perform optimization
optim_fit <- optim(par=par_init,
            fn=object_ive,
            sam_ple=sam_ple,
            method="L-BFGS-B",
            upper=c(1,10,10,10,10),
            lower=c(0,-10,0.2,-10,0.2))
optim_fit$par
      @
\vspace{-2em}
      <<optim_mixture,echo=TRUE,eval=FALSE,fig.width=5,fig.height=5,fig.show='hide'>>=
# plot histogram
histo_gram <- hist(sam_ple, plot=FALSE)
plot(histo_gram, freq=FALSE,
     main="histogram of sample")
fit_func <- function(x, pa_r) {
  pa_r["weight"] *
    dnorm(x, mean=pa_r["m1"], sd=pa_r["s1"]) +
  (1-pa_r["weight"]) *
    dnorm(x, mean=pa_r["m2"], sd=pa_r["s2"])
}  # end fit_func
curve(expr=fit_func(x, pa_r=optim_fit$par), add=TRUE,
      type="l", lwd=2, col="red")
legend("topright", inset=0.0, cex=0.8, title=NULL,
       leg="optimal parameters",
       lwd=2, bg="white", col="red")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/optim_mixture-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Package \protect\emph{ROI} Optimization Framework}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{ROI} provides a framework for defining optimization problems and their associated constraints, and an interface to fast optimization functions,
      \vskip1ex
      The function \texttt{DEoptim()} from package \emph{DEoptim} performs \emph{global} optimization using the \emph{Differential Evolution} algorithm,
      \vskip1ex
      \emph{Differential Evolution} is a genetic algorithm which evolves a population of solutions over several generations,\\
      \hskip1em\url{http://www1.icsi.berkeley.edu/~storn/code.html}
      \vskip1ex
      The first generation of solutions is selected randomly,
      \vskip1ex
      Each new generation is obtained by combining solutions from the previous generation,       \vskip1ex
      The best solutions are selected for creating the next generation, 
      \vskip1ex
      The \emph{Differential Evolution} algorithm is well suited for very large multi-dimensional optimization problems, such as portfolio optimization, 
      \vskip1ex
      \emph{Gradient} optimization methods are more efficient than \emph{Differential Evolution} for smooth objective functions with no local minima, 
    \column{0.5\textwidth}
        <<eval=FALSE,echo=TRUE>>=
# Rastrigin function with vector argument for optimization
rastri_gin <- function(vec_tor, pa_ram=25){
  sum(vec_tor^2 - pa_ram*cos(vec_tor))
}  # end rastri_gin
vec_tor <- c(pi/6, pi/6)
rastri_gin(vec_tor=vec_tor)
library(DEoptim)
## optimize rastri_gin using DEoptim
op_tim <-  DEoptim(rastri_gin, 
  upper=c(6, 6), lower=c(-6, -6), 
  DEoptim.control(trace=FALSE, itermax=50))
# optimal parameters and value
op_tim$optim$bestmem
rastri_gin(op_tim$optim$bestmem)
summary(op_tim)
plot(op_tim)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{DEoptim} for Global Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{DEoptim()} from package \emph{DEoptim} performs \emph{global} optimization using the \emph{Differential Evolution} algorithm,
      \vskip1ex
      \emph{Differential Evolution} is a genetic algorithm which evolves a population of solutions over several generations,\\
      \hskip1em\url{http://www1.icsi.berkeley.edu/~storn/code.html}
      \vskip1ex
      The first generation of solutions is selected randomly,
      \vskip1ex
      Each new generation is obtained by combining solutions from the previous generation,       \vskip1ex
      The best solutions are selected for creating the next generation, 
      \vskip1ex
      The \emph{Differential Evolution} algorithm is well suited for very large multi-dimensional optimization problems, such as portfolio optimization, 
      \vskip1ex
      \emph{Gradient} optimization methods are more efficient than \emph{Differential Evolution} for smooth objective functions with no local minima, 
    \column{0.5\textwidth}
        <<eval=FALSE,echo=TRUE>>=
# Rastrigin function with vector argument for optimization
rastri_gin <- function(vec_tor, pa_ram=25){
  sum(vec_tor^2 - pa_ram*cos(vec_tor))
}  # end rastri_gin
vec_tor <- c(pi/6, pi/6)
rastri_gin(vec_tor=vec_tor)
library(DEoptim)
## optimize rastri_gin using DEoptim
op_tim <-  DEoptim(rastri_gin, 
  upper=c(6, 6), lower=c(-6, -6), 
  DEoptim.control(trace=FALSE, itermax=50))
# optimal parameters and value
op_tim$optim$bestmem
rastri_gin(op_tim$optim$bestmem)
summary(op_tim)
plot(op_tim)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Package \protect\emph{Rcpp} for Running \texttt{C++} Programs}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{Rcpp} for Calling \texttt{C++} Programs from \texttt{R}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{Rcpp} allows calling \texttt{C++} programs from \texttt{R}, by compiling the \texttt{C++} code and creating \texttt{R} functions,
      \vskip1ex
      \emph{Rcpp} functions are \texttt{R} functions that were compiled from \texttt{C++} code using package \emph{Rcpp},
      \vskip1ex
      \emph{Rcpp} functions are much faster than code written in \texttt{R}, so they're suitable for large numerical calculations,
      \vskip1ex
      The package \emph{Rcpp} relies on \emph{Rtools} for compiling the \texttt{C++} code: \\
      \hskip1em\url{https://cran.r-project.org/bin/windows/Rtools/}
      \vskip1ex
      You can learn more about the package \emph{Rcpp} here: \\
      \hskip1em\url{http://adv-r.had.co.nz/Rcpp.html}\\
      \hskip1em\url{http://www.rcpp.org/}\\
      \hskip1em\url{http://gallery.rcpp.org/}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<eval=FALSE>>=
# verify that rtools are working properly:
devtools::find_rtools()
devtools::has_devel()

# load package Rcpp
library(Rcpp)
# get documentation for package Rcpp
# get short description
packageDescription("Rcpp")
# load help page
help(package="Rcpp")
# list all datasets in "Rcpp"
data(package="Rcpp")
# list all objects in "Rcpp"
ls("package:Rcpp")
# remove Rcpp from search path
detach("package:Rcpp")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Function \texttt{cppFunction()} for Compiling \texttt{C++} code}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{cppFunction()} compiles \texttt{C++} code into an \texttt{R} function.
      \vskip1ex
      The function \texttt{cppFunction()} creates an \texttt{R} function only for the current \texttt{R} session, and it must be recompiled for every new \texttt{R} session.
      \vskip1ex
      The function \texttt{sourceCpp()} compiles \texttt{C++} code contained in a file into \texttt{R} functions.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# define Rcpp function
Rcpp::cppFunction("
  int times_two(int x)
    { return 2 * x;}
  ")  # end cppFunction
# run Rcpp function
times_two(3)
# source Rcpp functions from file
Rcpp::sourceCpp(file="C:/Develop/R/lecture_slides/scripts/rcpp_mult.cpp")
# multiply two numbers
rcpp_mult(2, 3)
rcpp_mult(1:3, 6:4)
# multiply two vectors
rcpp_mult_vec(2, 3)
rcpp_mult_vec(1:3, 6:4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Loops in \protect\emph{Rcpp Sugar}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Loops written in \emph{Rcpp} can be two orders of magnitude faster than loops in \texttt{R}!
      \vskip1ex
      \emph{Rcpp Sugar} allows using \texttt{R}-style vectorized syntax in \emph{Rcpp} code.
      <<echo=TRUE,eval=FALSE>>=
# define Rcpp function with loop
Rcpp::cppFunction("
double inner_mult(NumericVector x, NumericVector y) {
int x_size = x.size();
int y_size = y.size();
if (x_size != y_size) {
    return 0;
  } else {
    double total = 0;
    for(int i = 0; i < x_size; ++i) {
      total += x[i] * y[i];
  }
  return total;
  }
}")  # end cppFunction
# run Rcpp function
inner_mult(1:3, 6:4)
inner_mult(1:3, 6:3)
# define Rcpp Sugar function with loop
Rcpp::cppFunction("
double inner_mult_sugar(NumericVector x, NumericVector y) {
  return sum(x * y);
}")  # end cppFunction
# run Rcpp Sugar function
inner_mult_sugar(1:3, 6:4)
inner_mult_sugar(1:3, 6:3)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# define R function with loop
inner_mult_r <- function(x, y) {
    to_tal <- 0
    for(i in 1:NROW(x)) {
      to_tal <- to_tal + x[i] * y[i]
    }
    to_tal
}  # end inner_mult_r
# run R function
inner_mult_r(1:3, 6:4)
inner_mult_r(1:3, 6:3)
# compare speed of Rcpp and R
library(microbenchmark)
summary(microbenchmark(
  pure_r=inner_mult_r(1:10000, 1:10000),
  inner_r=1:10000 %*% 1:10000,
  r_cpp=inner_mult(1:10000, 1:10000),
  r_cpp_sugar=inner_mult_sugar(1:10000, 1:10000),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Ornstein-Uhlenbeck Process Using \protect\emph{Rcpp}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Simulating the Ornstein-Uhlenbeck Process in \emph{Rcpp} is about 30 times faster than in \texttt{R}!
      <<echo=TRUE,eval=FALSE>>=
# define Ornstein-Uhlenbeck function in R
sim_ou <- function(len_gth=1000, eq_price=5.0,
                    vol_at=0.01, the_ta=0.01) {
  re_turns <- numeric(len_gth)
  price_s <- numeric(len_gth)
  price_s[1] <- eq_price
  for (i in 2:len_gth) {
    re_turns[i] <- the_ta*(eq_price - price_s[i-1]) + vol_at*rnorm(1)
    price_s[i] <- price_s[i-1] * exp(re_turns[i])
  }  # end for
  price_s
}  # end sim_ou
# simulate Ornstein-Uhlenbeck process in R
eq_price <- 5.0; vol_at <- 0.01
the_ta <- 0.01; len_gth <- 1000
set.seed(1121)  # reset random numbers
ou_sim <- sim_ou(len_gth=len_gth, eq_price=eq_price, vol_at=vol_at, the_ta=the_ta)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# define Ornstein-Uhlenbeck function in Rcpp
Rcpp::cppFunction("
NumericVector sim_ou_rcpp(double eq_price, 
                      double vol_at, 
                      double the_ta, 
                      NumericVector in_nov) {
  int len_gth = in_nov.size();
  NumericVector price_s(len_gth);
  NumericVector re_turns(len_gth);
  price_s[0] = eq_price;
  for (int it = 1; it < len_gth; it++) {
    re_turns[it] = the_ta*(eq_price - price_s[it-1]) + vol_at*in_nov[it-1];
    price_s[it] = price_s[it-1] * exp(re_turns[it]);
  }  // end for
  return price_s;
}")  # end cppFunction
# simulate Ornstein-Uhlenbeck process in Rcpp
set.seed(1121)  # reset random numbers
ou_sim_rcpp <- sim_ou_rcpp(eq_price=eq_price, 
  vol_at=vol_at, 
  the_ta=the_ta, 
  in_nov=rnorm(len_gth))
all.equal(ou_sim, ou_sim_rcpp)
# compare speed of Rcpp and R
library(microbenchmark)
summary(microbenchmark(
  pure_r=sim_ou(len_gth=len_gth, eq_price=eq_price, vol_at=vol_at, the_ta=the_ta),
  r_cpp=sim_ou_rcpp(eq_price=eq_price, vol_at=vol_at, the_ta=the_ta, in_nov=rnorm(len_gth)),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Compiling \texttt{C++} Code Using \protect\emph{Rcpp Attributes}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Rcpp attributes} are instructions for the \texttt{C++} compiler, embedded in the \emph{Rcpp} code as \texttt{C++} comments, and preceded by the \texttt{"//"} symbol. 
      \vskip1ex
      The \texttt{Rcpp::depends} attribute specifies additional \texttt{C++} library dependencies.
      \vskip1ex
      The \texttt{Rcpp::export} attribute specifies that a function should be exported to \texttt{R}, where it can be called as an \texttt{R} function.
      \vskip1ex
      Only functions which are preceded by the \texttt{Rcpp::export} attribute are exported to \texttt{R}.
      \vskip1ex
      The function \texttt{sourceCpp()} compiles \texttt{C++} code contained in a file into \texttt{R} functions.
      <<echo=TRUE,eval=FALSE>>=
# source Rcpp function for Ornstein-Uhlenbeck process from file
Rcpp::sourceCpp(file="C:/Develop/R/lecture_slides/scripts/sim_ou.cpp")
# simulate Ornstein-Uhlenbeck process in Rcpp
set.seed(1121)  # reset random numbers
ou_sim_rcpp <- sim_ou_rcpp(eq_price=eq_price, 
  vol_at=vol_at, 
  the_ta=the_ta, 
  in_nov=rnorm(len_gth))
all.equal(ou_sim, ou_sim_rcpp)
# compare speed of Rcpp and R
library(microbenchmark)
summary(microbenchmark(
  pure_r=sim_ou(len_gth=len_gth, eq_price=eq_price, vol_at=vol_at, the_ta=the_ta),
  r_cpp=sim_ou_rcpp(eq_price=eq_price, vol_at=vol_at, the_ta=the_ta, in_nov=rnorm(len_gth)),
  times=10))[, c(1, 4, 5)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
// Rcpp header with information for C++ compiler
#include <Rcpp.h> // include Rcpp C++ header files
using namespace Rcpp; // use Rcpp C++ namespace

// The function sim_ou_rcpp() simulates an Ornstein-Uhlenbeck process
// export the function roll_maxmin() to R
// [[Rcpp::export]]
NumericVector sim_ou_rcpp(double eq_price, 
                          double vol_at, 
                          double the_ta, 
                          NumericVector in_nov) {
  int len_gth = in_nov.size();
  NumericVector price_s(len_gth);
  NumericVector re_turns(len_gth);
  price_s[0] = eq_price;
  for (int it = 1; it < len_gth; it++) {
    re_turns[it] = the_ta*(eq_price - price_s[it-1]) + vol_at*in_nov[it-1];
    price_s[it] = price_s[it-1] * exp(re_turns[it]);
  }  // end for
  return price_s;
}  // end sim_ou_rcpp
    \end{lstlisting}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generating Random Numbers Using Logistic Map in \protect\emph{Rcpp}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{logistic map} in \emph{Rcpp} is about seven times faster than the loop in \texttt{R}, and even slightly faster than the standard \texttt{runif()} function in \texttt{R}!
      <<echo=TRUE,eval=FALSE>>=
# calculate uniformly distributed pseudo-random sequence
uni_form <- function(see_d, len_gth=10) {
  out_put <- numeric(len_gth)
  out_put[1] <- see_d
  for (i in 2:len_gth) {
    out_put[i] <- 4*out_put[i-1]*(1-out_put[i-1])
  }  # end for
  acos(1-2*out_put)/pi
}  # end uni_form

# source Rcpp functions from file
Rcpp::sourceCpp(file="C:/Develop/R/lecture_slides/scripts/uni_form.cpp")
# microbenchmark Rcpp code
library(microbenchmark)
summary(microbenchmark(
  pure_r=runif(1e5),
  r_loop=uni_form(0.3, 1e5),
  r_cpp=uniform_rcpp(0.3, 1e5),
  times=10))[, c(1, 4, 5)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
// Rcpp header with information for C++ compiler
#include <Rcpp.h> // include Rcpp C++ header files
using namespace Rcpp; // use Rcpp C++ namespace

// This is a simple example of exporting a C++ function to R. 
// You can source this function into an R session using the 
// function Rcpp::sourceCpp() 
// (or via the Source button on the editor toolbar).
// Learn more about Rcpp at:
//
//   http://www.rcpp.org/
//   http://adv-r.had.co.nz/Rcpp.html
//   http://gallery.rcpp.org/

// function uni_form() produces a vector of 
// uniformly distributed pseudo-random numbers
// [[Rcpp::export]]
NumericVector uniform_rcpp(double see_d, int len_gth) {
// define pi
static const double pi = 3.14159265;
// allocate output vector
  NumericVector out_put(len_gth);
// initialize output vector
  out_put[0] = see_d; 
// perform loop
  for (int i=1; i < len_gth; ++i) {
    out_put[i] = 4*out_put[i-1]*(1-out_put[i-1]);
  }  // end for
// rescale output vector and return it
  return acos(1-2*out_put)/pi;
}
    \end{lstlisting}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{RcppArmadillo} for Fast Linear Algebra}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{RcppArmadillo} allows calling the high-level \emph{Armadillo} \texttt{C++} linear algebra library,
      \vskip1ex
      \emph{Armadillo} provides ease of use and speed, with syntax similar to \emph{Matlab},       \vskip1ex
      \emph{RcppArmadillo} functions are often faster than even compiled \texttt{R} functions, because they use better optimized \texttt{C++} code:\\
      \url{http://arma.sourceforge.net/speed.html}\\
      \vskip1ex
      You can learn more about \emph{RcppArmadillo}: \\
      \tiny \url{http://arma.sourceforge.net/}\\
      \tiny \url{http://dirk.eddelbuettel.com/code/rcpp.armadillo.html}\\
      \tiny \url{https://cran.r-project.org/web/packages/RcppArmadillo/index.html}\\
      \tiny \url{https://github.com/RcppCore/RcppArmadillo}
      <<echo=TRUE,eval=FALSE>>=
library(RcppArmadillo)
# source Rcpp functions from file
Rcpp::sourceCpp(file="C:/Develop/R/lecture_slides/scripts/armadillo_functions.cpp")
vec1 <- runif(1e5)
vec2 <- runif(1e5)
vec_in(vec1, vec2)
vec1 %*% vec2
      @
    \column{0.5\textwidth}
      \vspace{-2em}
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
// Rcpp header with information for C++ compiler
#include <RcppArmadillo.h> // include C++ header file from Armadillo library
using namespace arma; // use C++ namespace from Armadillo library
// declare dependency on RcppArmadillo
// [[Rcpp::depends(RcppArmadillo)]]

// Examples of RcppArmadillo functions below

// vec_in() calculates the inner (dot) product of two vectors.
// It accepts pointers to the two vectors and returns a double.
//' @export
// [[Rcpp::export]]
double vec_in(const arma::vec& vec1, const arma::vec& vec2){
  return arma::dot(vec1, vec2);
}  // end vec_in

// mat_2vec_in() calculates the inner (dot) product of a matrix
// with two vectors.
// It accepts pointers to the matrix and vectors, and returns a double.
//' @export
// [[Rcpp::export]]
double mat_2vec_in(const arma::vec& vec_tor2, const arma::mat& mat_rix, const arma::vec& vec_tor1){
  return arma::as_scalar(trans(vec_tor2) * (mat_rix * vec_tor1));
}  // end mat_2vec_in
    \end{lstlisting}
      \vspace{-1.5em}
      <<echo=TRUE,eval=FALSE>>=
# microbenchmark RcppArmadillo code
summary(microbenchmark(
  vec_in=vec_in(vec1, vec2),
  r_code=(vec1 %*% vec2),
  times=100))[, c(1, 4, 5)]  # end microbenchmark summary
# microbenchmark shows: 
# vec_in() is several times faster than %*%, especially for longer vectors.
#     expr     mean   median
# 1 vec_in 110.7067 110.4530
# 2 r_code 585.5127 591.3575
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fast Matrix Algebra Using \protect\emph{RcppArmadillo}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{RcppArmadillo} functions can be made even faster by operating on pointers to matrices and performing calculations in place, without copying large matrices,
      \vskip1ex
      \emph{RcppArmadillo} functions can be compiled using the same \emph{Rtools} as those for \emph{Rcpp} functions:\\
      \hskip1em\url{https://cran.r-project.org/bin/windows/Rtools/}
      <<echo=TRUE,eval=FALSE>>=
library(RcppArmadillo)
# source Rcpp functions from file
Rcpp::sourceCpp(file="C:/Develop/R/lecture_slides/scripts/armadillo_functions.cpp")
mat_rix <- matrix(runif(1e5), nc=1e3)
# de-mean using apply()
new_mat <- apply(mat_rix, 2, 
  function(x) (x-mean(x)))
# de-mean using demean_mat()
demean_mat(mat_rix)
all.equal(new_mat, mat_rix)
# microbenchmark RcppArmadillo code
summary(microbenchmark(
  demean_mat=demean_mat(mat_rix),
  apply=(apply(mat_rix, 2, mean)),
  times=100))[, c(1, 4, 5)]  # end microbenchmark summary
# microbenchmark shows: 
# demean_mat() is over 70 times faster than apply()
#         expr       mean   median
# 1 demean_mat   127.7539  125.604
# 2      apply 10781.7534 9291.674

# perform matrix inversion
# create random positive semi-definite matrix
mat_rix <- matrix(runif(25), nc=5)
mat_rix <- t(mat_rix) %*% mat_rix
# invert the matrix
matrix_inv <- solve(mat_rix)
inv_mat(mat_rix)
all.equal(inv_mat, mat_rix)
# microbenchmark RcppArmadillo code
library(microbenchmark)
summary(microbenchmark(
  inv_mat=inv_mat(mat_rix),
  solve=solve(mat_rix),
  times=100))[, c(1, 4, 5)]  # end microbenchmark summary
# microbenchmark shows: 
# inv_mat() is over 10 times faster than solve()
#      expr     mean median
# 1 inv_mat  3.42669  2.933
# 2 solve   32.00254 31.280
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
// Rcpp header with information for C++ compiler
#include <RcppArmadillo.h> // include C++ header file from Armadillo library
using namespace arma; // use C++ namespace from Armadillo library
// declare dependency on RcppArmadillo
// [[Rcpp::depends(RcppArmadillo)]]

// Examples of RcppArmadillo functions below

// demean_mat() calculates a matrix with de-meaned columns.
// It accepts a pointer to a matrix and operates on the matrix in place.
// It returns the number of columns of the input matrix.
//' @export
// [[Rcpp::export]]
double demean_mat(arma::mat& mat_rix){
  for (unsigned int i = 0; i < mat_rix.n_cols; i++) {
    mat_rix.col(i) -= arma::mean(mat_rix.col(i));
  }  // end for
  return mat_rix.n_cols;
}  // end demean_mat

// inv_mat() calculates the inverse of symmetric positive
// definite matrix.
// It accepts a pointer to a matrix and operates on the matrix in place.
// It returns the number of columns of the input matrix.
// It uses RcppArmadillo.
//' @export
// [[Rcpp::export]]
double inv_mat(arma::mat& mat_rix){
  mat_rix = arma::inv_sympd(mat_rix);
  return mat_rix.n_cols;
}  // end inv_mat
    \end{lstlisting}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Linear Algebra}


%%%%%%%%%%%%%%%
\subsection{Vector and Matrix Calculus}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
    \begin{columns}[T]
    \column{0.5\textwidth}
      Let $\boldsymbol{v}$ and $\boldsymbol{w}$ be vectors, with $\boldsymbol{v} = \left\{ v_i \right\}_{i=1}^{i=n}$, and let $\mathbbm{1}$ be the unit vector, with $\mathbbm{1} = \left\{ 1 \right\}_{i=1}^{i=n}$,
      \vskip1ex
      Then the inner product of $\boldsymbol{v}$ and $\boldsymbol{w}$ can be written as $\boldsymbol{v}^T \boldsymbol{w} = \boldsymbol{w}^T \boldsymbol{v} = {\sum_{i=1}^n {v_i w_i}}$,
      \vskip1ex
      We can then express the sum of the elements of $\boldsymbol{v}$ as the inner product: $\boldsymbol{v}^T \mathbbm{1} = \mathbbm{1}^T \boldsymbol{v} = {\sum_{i=1}^n v_i}$,
      \vskip1ex
      And the sum of squares of $\boldsymbol{v}$ as the inner product: $\boldsymbol{v}^T \boldsymbol{v} = {\sum_{i=1}^n v_i^2}$,
      \vskip1ex
      Let $\mathbb{A}$ be a matrix, with $\mathbb{A} = \left\{ A_{ij} \right\}_{{i,j}=1}^{{i,j}=n}$,
      \vskip1ex
      Then the inner product of matrix $\mathbb{A}$ with vectors $\boldsymbol{v}$ and $\boldsymbol{w}$ can be written as: 
      \begin{displaymath}
        \boldsymbol{v}^T \mathbb{A} \, \boldsymbol{w} = \boldsymbol{w}^T \mathbb{A}^T \boldsymbol{v} = {\sum_{{i,j}=1}^n {A_{ij} v_i w_j}}
      \end{displaymath}
    \column{0.5\textwidth}
      The derivative of a scalar variable with respect to a vector variable is a vector, for example:
      \begin{align*}
        \frac{d (\boldsymbol{v}^T \mathbbm{1})}{d \boldsymbol{v}} = d_v[\boldsymbol{v}^T \mathbbm{1}] = d_v[\mathbbm{1}^T \boldsymbol{v}] = \mathbbm{1}^T\\
        d_v[\boldsymbol{v}^T \boldsymbol{w}] = d_v[\boldsymbol{w}^T \boldsymbol{v}] = \boldsymbol{w}^T\\
        d_v[\boldsymbol{v}^T \mathbb{A} \, \boldsymbol{w}] = \boldsymbol{w}^T \mathbb{A}^T\\
        d_v[\boldsymbol{v}^T \mathbb{A} \, \boldsymbol{v}] = \boldsymbol{v}^T \mathbb{A} + \boldsymbol{v}^T \mathbb{A}^T
      \end{align*}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Eigenvectors and Eigenvalues of Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The vector $w$ is an \emph{eigenvector} of the matrix $\mathbb{A}$, if it satisfies the \emph{eigenvalue} equation:
      \begin{displaymath}
        \mathbb{A} \, w = \lambda \, w
      \end{displaymath}
      Where $\lambda$ is the \emph{eigenvalue} corresponding to the \emph{eigenvector} $w$,
      \vskip1ex
      The number of \emph{eigenvalues} of a matrix is equal to its dimension,
      \vskip1ex
      Real symmetric matrices have real \emph{eigenvalues}, and their \emph{eigenvectors} are orthogonal to each other,
      \vskip1ex
      The \emph{eigenvectors} can be normalized to \texttt{1},
      \vskip1ex
      The \emph{eigenvectors} form an \emph{orthonormal basis} in which the matrix $\mathbb{A}$ is diagonal,
      \vskip1ex
      The function \texttt{eigen()} calculates the \emph{eigenvectors} and \emph{eigenvalues} of numeric matrices,
      \vskip1ex
      An excellent interactive visualization of \emph{eigenvectors} and \emph{eigenvalues} is available here:\\
      \hskip1em\url{http://setosa.io/ev/eigenvectors-and-eigenvalues/}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eigen_values.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# create random real symmetric matrix
mat_rix <- matrix(runif(25), nc=5)
mat_rix <- mat_rix + t(mat_rix)
# calculate eigenvectors and eigenvalues
ei_gen <- eigen(mat_rix)
eigen_vec <- ei_gen$vectors
dim(eigen_vec)
# plot eigenvalues
barplot(ei_gen$values, 
  xlab="", ylab="", las=3, 
  names.arg=paste0("ev", 1:NROW(ei_gen$values)), 
  main="Eigenvalues of a real symmetric matrix")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Eigen Decomposition of Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Real symmetric matrices have real \emph{eigenvalues}, and their \emph{eigenvectors} are orthogonal to each other,
      \vskip1ex
      The \emph{eigenvectors} form an \emph{orthonormal basis} in which the matrix $\mathbb{A}$ is diagonal:
      \begin{displaymath}
        \mathbb{D} = \mathbb{O}^T \mathbb{A} \, \mathbb{O}
      \end{displaymath}
      Where $\mathbb{D}$ is a \emph{diagonal} matrix containing the \emph{eigenvalues} of matrix $\mathbb{A}$, and $\mathbb{O}$ is an \emph{orthogonal} matrix of its \emph{eigenvectors},
      \vskip1ex
      Any real symmetric matrix $\mathbb{A}$ can be decomposed into a product of its \emph{eigenvalues} and its \emph{eigenvectors} (the \emph{eigen decomposition}):
      \begin{displaymath}
        \mathbb{A} = \mathbb{O} \, \mathbb{D} \, \mathbb{O}^T
      \end{displaymath}
      The \emph{eigen decomposition} expresses a matrix as the product of a rotation, followed by a scaling, followed by the inverse rotation,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# eigenvectors form an orthonormal basis
round(t(eigen_vec) %*% eigen_vec, 
  digits=4)
# diagonalize matrix using eigenvector matrix
round(t(eigen_vec) %*% (mat_rix %*% eigen_vec), 
  digits=4)
ei_gen$values
# eigen decomposition of matrix by rotating the diagonal matrix
eigen_decomp <- eigen_vec %*% (ei_gen$values * t(eigen_vec))
# create diagonal matrix of eigenvalues
# diago_nal <- diag(ei_gen$values)
# eigen_decomp <- eigen_vec %*% (diago_nal %*% t(eigen_vec))
all.equal(mat_rix, eigen_decomp)
      @
      \emph{Orthogonal} matrices represent rotations in \emph{hyperspace}, and their inverse is equal to their transpose: $\mathbb{O}^{-1} = \mathbb{O}^T$, 
      \vskip1ex
      The \emph{diagonal} matrix $\mathbb{D}$ represents a scaling (stretching) transformation proportional to the \emph{eigenvalues},
      \vskip1ex
      The \texttt{\%*\%} operator performs \emph{inner} (\emph{scalar}) multiplication of vectors and matrices,
      \vskip1ex
      \emph{Inner} multiplication multiplies the rows of one matrix with the columns of another matrix, so that each pair produces a single number,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Positive Definite} Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Matrices with positive \emph{eigenvalues} are called \emph{positive definite} matrices,
      \vskip1ex
      Matrices with non-negative \emph{eigenvalues} are called \emph{positive semi-definite} matrices (some of their \emph{eigenvalues} may be zero),
      \vskip1ex
      An example of \emph{positive definite} matrices are the covariance matrices of linearly independent variables, 
      \vskip1ex
      But the covariance matrices of linearly dependent variables have some \emph{eigenvalues} equal to zero, in which case they are \emph{singular}, and only \emph{positive semi-definite},
      \vskip1ex
      All covariance matrices are \emph{positive semi-definite} and all \emph{positive semi-definite} matrices are the covariance matrix of some multivariate distribution,
      \vskip1ex
      Matrices which have some \emph{eigenvalues} equal to zero are called \emph{singular} (degenerate) matrices, 
      \vskip1ex
      For any real matrix $\mathbb{A}$, the matrix $\mathbb{A}^T \mathbb{A}$ is \emph{positive semi-definite},
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eigen_posdef.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# create random positive semi-definite matrix
mat_rix <- matrix(runif(25), nc=5)
mat_rix <- t(mat_rix) %*% mat_rix
# calculate eigenvectors and eigenvalues
ei_gen <- eigen(mat_rix)
ei_gen$values
# plot eigenvalues
barplot(ei_gen$values, las=3, 
  xlab="", ylab="", 
  names.arg=paste0("ev", 1:NROW(ei_gen$values)), 
  main="Eigenvalues of positive semi-definite matrix")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Singular Value Decomposition (\protect\emph{SVD}) of Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Singular Value Decomposition} (\emph{SVD}) is a generalization of the \emph{eigen decomposition} of \emph{positive semi-definite} matrices, 
      \vskip1ex
      The \emph{SVD} of a rectangular matrix $\mathbb{A}$ with dimensions \texttt{m} rows and \texttt{n} columns is defined as the factorization:
      \begin{displaymath}
        \mathbb{A} = \mathbb{U} \Sigma \mathbb{V}^T
      \end{displaymath}
      If (\texttt{m > n}), then $\mathbb{U}$ is an (\texttt{m x n}) rectangular matrix, $\Sigma$ is an (\texttt{n x n}) diagonal matrix containing the \emph{singular values}, and $\mathbb{V}$ is an (\texttt{n x n}) \emph{orthogonal} matrix, and vice versa if (\texttt{m < n}),
      \vskip1ex
      The (\texttt{m x n}) rectangular matrix $\mathbb{U}$ consists of \texttt{n} columns of \emph{orthonormal} left-\emph{singular} vectors, with $\mathbb{U}^T \mathbb{U} = \mathbbm{1}$,
      \vskip1ex
      The columns of the \emph{orthogonal} matrix $\mathbb{V}$ consist of \texttt{n} \emph{orthonormal} right-\emph{singular} vectors, with $\mathbb{V}^T \mathbb{V} = \mathbbm{1}$,
      \vskip1ex
      The \emph{singular} vectors are only defined up to a reflection (sign), i.e. if \texttt{vec} is a \emph{singular} vector, then so is \texttt{-vec},
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# dimensions of left and right matrices
n_left <- 6 ; n_right <- 4
# create random positive semi-definite matrix
left_mat <- matrix(runif(n_left^2), nc=n_left)
left_mat <- crossprod(left_mat)
# or
left_mat <- left_mat %*% t(left_mat)
# calculate left eigenvectors
ei_gen <- eigen(left_mat)
left_mat <- ei_gen$vectors[, 1:n_right]
# create random positive semi-definite matrix
right_mat <- matrix(runif(n_right^2), nc=n_right)
right_mat <- crossprod(right_mat)
# or
right_mat <- right_mat %*% t(right_mat)
# calculate right eigenvectors and singular values
ei_gen <- eigen(right_mat)
right_mat <- ei_gen$vectors
sing_values <- ei_gen$values
# compose rectangular matrix
mat_rix <- 
  left_mat %*% (sing_values * t(right_mat))
# mat_rix <- left_mat %*% diag(sing_values) %*% t(right_mat)
      @
      \vspace{-1em}
      In the special case when $\mathbb{A}$ is a \emph{positive semi-definite} matrix, the \emph{SVD} reduces to the \emph{eigen decomposition}, with $\mathbb{U} = \mathbb{V}$,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Singular Value Decomposition Using Function \texttt{svd()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{SVD} of a rectangular matrix $\mathbb{A}$ with dimensions \texttt{m} rows and \texttt{n} columns is defined as the factorization:
      \begin{displaymath}
        \mathbb{A} = \mathbb{U} \Sigma \mathbb{V}^T
      \end{displaymath}
      The (\texttt{m x n}) rectangular matrix $\mathbb{U}$ consists of \texttt{n} columns of \emph{orthonormal} left-\emph{singular} vectors, with $\mathbb{U}^T \mathbb{U} = \mathbbm{1}$,
      \vskip1ex
      The left-\emph{singular} vectors are the \emph{eigenvectors} of the matrix $\mathbb{A} \mathbb{A}^T$, 
      \vskip1ex
      The columns of the \emph{orthogonal} matrix $\mathbb{V}$ consist of \texttt{n} \emph{orthonormal} right-\emph{singular} vectors, with $\mathbb{V}^T \mathbb{V} = \mathbbm{1}$,
      \vskip1ex
      The right-\emph{singular} vectors are the \emph{eigenvectors} of the matrix $\mathbb{A}^T \mathbb{A}$, 
      \vskip1ex
      The left-\emph{singular} matrix $\mathbb{U}$ combined with the right-\emph{singular} matrix $\mathbb{V}$ define a rotation transformation into a coordinate system where the matrix $\mathbb{A}$ becomes diagonal:
      \begin{displaymath}
        \Sigma = \mathbb{U}^T \mathbb{A} \mathbb{V}
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# perform singular value decomposition
s_vd <- svd(mat_rix)
# compare SVD with inputs
all.equal(abs(s_vd$u), abs(left_mat))
all.equal(abs(s_vd$v), abs(right_mat))
all.equal(s_vd$d, ei_gen$values)
      @
      The function \texttt{svd()} performs \emph{Singular Value Decomposition} (\emph{SVD}) of a rectangular matrix, and returns a list of three elements: the \emph{singular values}, and the matrices of left-\emph{singular} vectors and the right-\emph{singular} vectors,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Inverse of Square Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The inverse of a square matrix $\mathbb{A}$ is defined as a square matrix $\mathbb{A}^{-1}$ that satisfies the equation:
      \begin{displaymath}
        \mathbb{A}^{-1} \mathbb{A} = \mathbb{A} \mathbb{A}^{-1} = \mathbbm{1}
      \end{displaymath}
      Where $\mathbbm{1}$ is the identity matrix, 
      \vskip1ex
      The inverse $\mathbb{A}^{-1}$ matrix can also be expressed as a product of the inverse of its \emph{eigenvalues} ($\mathbb{D}$) and its \emph{eigenvectors} ($\mathbb{O}$):
      \begin{displaymath}
        \mathbb{A}^{-1} = \mathbb{O} \, \mathbb{D}^{-1} \, \mathbb{O}^T
      \end{displaymath}
      But \emph{singular} (degenerate) matrices (which have some \emph{eigenvalues} equal to zero) don't have an inverse,
      \vskip1ex
      The function \texttt{solve()} solves systems of linear equations, and also inverts square matrices, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# create random positive semi-definite matrix
mat_rix <- matrix(runif(25), nc=5)
mat_rix <- t(mat_rix) %*% mat_rix
# calculate the inverse of mat_rix
in_verse <- solve(a=mat_rix)
# multiply inverse with matrix
round(in_verse %*% mat_rix, 4)
round(mat_rix %*% in_verse, 4)

# calculate eigenvectors and eigenvalues
ei_gen <- eigen(mat_rix)
eigen_vec <- ei_gen$vectors

# perform eigen decomposition of inverse
eigen_inverse <- 
  eigen_vec %*% (t(eigen_vec) / ei_gen$values)
all.equal(in_verse, eigen_inverse)
# decompose diagonal matrix with inverse of eigenvalues
# diago_nal <- diag(1/ei_gen$values)
# eigen_inverse <-
#   eigen_vec %*% (diago_nal %*% t(eigen_vec))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generalized Inverse of Rectangular Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The generalized inverse of an (\texttt{m x n}) rectangular matrix $\mathbb{A}$ is defined as an (\texttt{n x m}) matrix $\mathbb{A}^{-1}$ that satisfies the equation:
      \begin{displaymath}
        \mathbb{A} \mathbb{A}^{-1} \mathbb{A} = \mathbbm{A}
      \end{displaymath}
      The generalized inverse matrix $\mathbb{A}^{-1}$ can be expressed as a product of the inverse of its \emph{singular values} ($\Sigma$) and its left and right \emph{singular} matrices ($\mathbb{U}$ and $\mathbb{V}$):
      \begin{displaymath}
        \mathbb{A}^{-1} = \mathbb{V} \, \Sigma^{-1} \, \mathbb{U}^T
      \end{displaymath}
      The generalized inverse $\mathbb{A}^{-1}$ can also be expressed as the \emph{Moore-Penrose pseudo-inverse}:
      \begin{displaymath}
        \mathbb{A}^{-1} = (\mathbb{A}^T \mathbb{A})^{-1} \mathbb{A}^T
      \end{displaymath}
      In the case when the inverse matrix $\mathbb{A}^{-1}$ exists, then the \emph{pseudo-inverse} matrix simplifies to the inverse: $(\mathbb{A}^T \mathbb{A})^{-1} \mathbb{A}^T = \mathbb{A}^{-1} (\mathbb{A}^T)^{-1} \mathbb{A}^T = \mathbb{A}^{-1}$
      \vskip1ex
      The function \texttt{MASS::ginv()} calculates the generalized inverse of a matrix, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# create random rectangular matrix
# case when: n_left > n_right
n_left <- 6 ; n_right <- 4
mat_rix <- matrix(runif(n_left*n_right), 
  nc=n_right)
# calculate generalized inverse of mat_rix
in_verse <- MASS::ginv(mat_rix)
round(in_verse %*% mat_rix, 4)
all.equal(mat_rix, 
          mat_rix %*% in_verse %*% mat_rix)
# create random rectangular matrix
# case when: n_left < n_right
n_left <- 4 ; n_right <- 6
mat_rix <- matrix(runif(n_left*n_right), 
  nc=n_right)
# calculate generalized inverse of mat_rix
in_verse <- MASS::ginv(mat_rix)
round(mat_rix %*% in_verse, 4)
# perform singular value decomposition
s_vd <- svd(mat_rix)
# calculate generalized inverse from SVD
svd_inverse <- s_vd$v %*% (t(s_vd$u) / s_vd$d)
all.equal(svd_inverse, in_verse)
# calculate Moore-Penrose pseudo-inverse
mp_inverse <- 
  MASS::ginv(t(mat_rix) %*% mat_rix) %*% t(mat_rix)
all.equal(mp_inverse, in_verse)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generalized Inverse of Singular Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Singular} matrices have some \emph{singular values} equal to zero, so they don't have an inverse matrix which satisfies the equation: $\mathbb{A}^{-1} \mathbb{A} = \mathbb{A} \mathbb{A}^{-1} = \mathbbm{1}$,
      \vskip1ex
      But if the \emph{singular values} that are equal to zero are removed, then a generalized inverse for \emph{singular} matrices can be specified by:
      \begin{displaymath}
        \mathbb{A}^{-1} = \mathbb{V}_n \, \Sigma_n^{-1} \, \mathbb{U}_n^T
      \end{displaymath}
      Where $\mathbb{U}_n$, $\mathbb{V}_n$ and $\Sigma_n$ are the \emph{SVD} matrices with rows and columns corresponding to zero \emph{singular values} removed, 
      <<echo=TRUE,eval=FALSE>>=
# create random singular matrix
n_left <- 4 ; n_right <- 6
mat_rix <- matrix(runif(n_left*n_right), nc=n_right)
mat_rix <- t(mat_rix) %*% mat_rix
# calculate generalized inverse of mat_rix
in_verse <- MASS::ginv(mat_rix)
# verify inverse of mat_rix
all.equal(mat_rix, 
  mat_rix %*% in_verse %*% mat_rix)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# perform singular value decomposition
s_vd <- svd(mat_rix)
# set tolerance for determining zero singular values
to_l <- sqrt(.Machine$double.eps)
# check for zero singular values
s_vd$d
not_zero <- (s_vd$d > (to_l * s_vd$d[1]))
# calculate generalized inverse from SVD
svd_inverse <- 
  s_vd$v[, not_zero] %*% 
  (t(s_vd$u[, not_zero]) / s_vd$d[not_zero])
all.equal(svd_inverse, in_verse)
# calculate Moore-Penrose pseudo-inverse
mp_inverse <- 
  MASS::ginv(t(mat_rix) %*% mat_rix) %*% t(mat_rix)
all.equal(mp_inverse, in_verse)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Diagonalizing Generalized Inverse of Singular Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The left-\emph{singular} matrix $\mathbb{U}$ combined with the right-\emph{singular} matrix $\mathbb{V}$ define a rotation transformation into a coordinate system where the matrix $\mathbb{A}$ becomes diagonal:
      \begin{displaymath}
        \Sigma = \mathbb{U}^T \mathbb{A} \mathbb{V}
      \end{displaymath}
      The generalized inverse of \emph{singular} matrices doesn't satisfy the equation: $\mathbb{A}^{-1} \mathbb{A} = \mathbb{A} \mathbb{A}^{-1} = \mathbbm{1}$, but if it's rotated into the same coordinate system where $\mathbb{A}$ is diagonal, then we have:
      \begin{displaymath}
        \mathbb{U}^T (\mathbb{A}^{-1} \mathbb{A}) \, \mathbb{V} = \mathbbm{1}_n
      \end{displaymath}
      So that $\mathbb{A}^{-1} \mathbb{A}$ is diagonal in the same coordinate system where $\mathbb{A}$ is diagonal, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# diagonalize the "unit" matrix
uni_t <- mat_rix %*% in_verse
round(uni_t, 4)
round(mat_rix %*% in_verse, 4)
round(t(s_vd$u) %*% uni_t %*% s_vd$v, 4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Solving Linear Equations Using \texttt{solve()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A system of linear equations can be defined as: 
      \begin{displaymath}
        \mathbb{A} \, x = b
      \end{displaymath}
      Where $\mathbb{A}$ is a matrix, $b$ is a vector, and \texttt{x} is the unknown vector, 
      \vskip1ex
      The solution of the system of linear equations is equal to: 
      \begin{displaymath}
        x = \mathbb{A}^{-1} b
      \end{displaymath}
      Where $\mathbb{A}^{-1}$ is the \emph{inverse} of the matrix $\mathbb{A}$,
      \vskip1ex
      The function \texttt{solve()} solves systems of linear equations, and also inverts square matrices, 
      \vskip1ex
      The \texttt{\%*\%} operator performs \emph{inner} (\emph{scalar}) multiplication of vectors and matrices,
      \vskip1ex
      \emph{Inner} multiplication multiplies the rows of one matrix with the columns of another matrix, so that each pair produces a single number:
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# define a square matrix
mat_rix <- matrix(c(1, 2, -1, 2), nc=2)
vec_tor <- c(2, 1)
# calculate the inverse of mat_rix
in_verse <- solve(a=mat_rix)
in_verse %*% mat_rix
# calculate solution using inverse of mat_rix
solu_tion <- in_verse %*% vec_tor
mat_rix %*% solu_tion
# calculate solution of linear system
solu_tion <- solve(a=mat_rix, b=vec_tor)
mat_rix %*% solu_tion
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Cholesky Decomposition}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Cholesky} decomposition of a \emph{positive definite} matrix $\mathbb{A}$ is defined as:
      \begin{displaymath}
        \mathbb{A} = \mathbb{L}^T \mathbb{L}
      \end{displaymath}
      Where $\mathbb{L}$ is an upper triangular matrix with positive diagonal elements,
      \vskip1ex
      The matrix $\mathbb{L}$ can be considered the square root of $\mathbb{A}$,
      \vskip1ex
      The vast majority of random \emph{positive semi-definite} matrices are also \emph{positive definite},
      \vskip1ex
      The function \texttt{chol()} calculates the \emph{Cholesky} decomposition of a \emph{positive definite} matrix, 
      \vskip1ex
      The functions \texttt{chol2inv()} and \texttt{chol()} calculate the inverse of a \emph{positive definite} matrix two times faster than \texttt{solve()}, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# create large random positive semi-definite matrix
mat_rix <- matrix(runif(1e4), nc=100)
mat_rix <- t(mat_rix) %*% mat_rix
# calculate eigen decomposition
ei_gen <- eigen(mat_rix)
eigen_values <- ei_gen$values
eigen_vec <- ei_gen$vectors
# set tolerance for determining zero singular values
to_l <- sqrt(.Machine$double.eps)
# if needed convert to positive definite matrix
not_zero <- (eigen_values > (to_l * eigen_values[1]))
if (sum(!not_zero) > 0) {
  eigen_values[!not_zero] <- 2*to_l
  mat_rix <- eigen_vec %*% 
    (eigen_values * t(eigen_vec))
}  # end if
# calculate the Cholesky mat_rix
choles_ky <- chol(mat_rix)
choles_ky[1:5, 1:5]
all.equal(mat_rix, t(choles_ky) %*% choles_ky)
# calculate inverse from Cholesky
chol_inverse <- chol2inv(choles_ky)
all.equal(solve(mat_rix), chol_inverse)
# compare speed of Cholesky inversion
library(microbenchmark)
summary(microbenchmark(
  sol_ve=solve(mat_rix),
  choles_ky=chol2inv(chol(mat_rix)),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Correlated Returns Using Cholesky Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Cholesky} decomposition of a covariance matrix can be used to simulate correlated \emph{Normal} returns following the given covariance matrix: $\mathbb{C} = \mathbb{L}^T \mathbb{L}$
      \vskip1ex
      Let $\mathbb{R}$ be a matrix with columns of \emph{uncorrelated} returns following the \emph{Standard Normal} distribution,
      \vskip1ex
      The \emph{correlated} returns $\mathbb{R}_c$ can be calculated from the \emph{uncorrelated} returns $\mathbb{R}$ by multilying them by the \emph{Cholesky} matrix $\mathbb{L}$:
      \begin{displaymath}
        \mathbb{R}_c = \mathbb{L}^T \mathbb{R}
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# calculate random covariance matrix
cov_mat <- matrix(runif(25), nc=5)
cov_mat <- t(cov_mat) %*% cov_mat
# calculate the Cholesky mat_rix
choles_ky <- chol(cov_mat)
choles_ky
# simulate random uncorrelated returns
n_assets <- 5
n_rows <- 10000
re_turns <- matrix(rnorm(n_assets*n_rows), nc=n_assets)
# calculate correlated returns by applying Cholesky
corr_returns <- re_turns %*% choles_ky
# calculate covariance matrix
cov_returns <- crossprod(corr_returns) / (n_rows-1)
all.equal(cov_mat, cov_returns)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Eigenvalues of Singular Covariance Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If $\mathbb{R}$ is a matrix of returns (with zero mean) for a portfolio of \texttt{k} assets (columns), over \texttt{n} time periods (rows), then the sample covariance matrix is equal to:
      \begin{displaymath}
        \mathbb{C} = \mathbb{R}^T \mathbb{R} / (n-1)
      \end{displaymath}
      If the number of time periods of returns is less than the number of portfolio assets, then the returns are collinear, and the sample covariance matrix is \emph{singular} (some \emph{eigenvalues} are zero), 
      \vskip1ex
      The function \texttt{crossprod()} performs \emph{inner} (\emph{scalar}) multiplication, exactly the same as the \texttt{\%*\%} operator, but it is slightly faster,
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# simulate random portfolio returns
n_assets <- 10
n_rows <- 100
re_turns <- matrix(rnorm(n_assets*n_rows), nc=n_assets)
# de-mean the returns
re_turns <- apply(re_turns, MARGIN=2, function(x) (x-mean(x)))
# calculate covariance matrix
cov_mat <- crossprod(re_turns) / (n_rows-1)
# calculate eigenvectors and eigenvalues
ei_gen <- eigen(cov_mat)
ei_gen$values
barplot(ei_gen$values, # plot eigenvalues
  xlab="", ylab="", las=3, 
  names.arg=paste0("ev", 1:NROW(ei_gen$values)), 
  main="Eigenvalues of covariance matrix")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eigen_covmat.png}\\
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# calculate eigenvectors and eigenvalues
# as function of number of returns
n_data <- ((n_assets/2):(2*n_assets))
e_values <- sapply(n_data, function(x) {
  re_turns <- re_turns[1:x, ]
  re_turns <- apply(re_turns, MARGIN=2, 
    function(y) (y-mean(y)))
  cov_mat <- crossprod(re_turns) / (x-1)
  min(eigen(cov_mat)$values)
})  # end sapply
plot(y=e_values, x=n_data, t="l", 
  xlab="", ylab="", lwd=3, col="blue",
  main="Smallest eigenvalue of covariance matrix\nas function of number of returns")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Covariance Matrix Shrinkage Estimator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimates of the covariance matrix suffer from statistical errors, and those errors are magnified when the covariance matrix is inverted,
      \vskip1ex
      In the \emph{shrinkage} technique the covariance matrix $\mathbb{C}_s$ is estimated as a weighted sum of the sample covariance estimator $\mathbb{C}$ plus a target matrix $\mathbb{T}$:
      \begin{displaymath}
        \mathbb{C}_s = (1-\alpha) \, \mathbb{C} + \alpha \, \mathbb{T}
      \end{displaymath}
      The target matrix $\mathbb{T}$ represents an estimate of the covariance matrix subject to some constraint, such as that all the correlations are equal to each other,
      \vskip1ex
      The shrinkage intensity $\alpha$ determines the amount of shrinkage that is applied, with $\alpha = 1$ representing a complete shrinkage towards the target matrix,
      \vskip1ex
      The \emph{shrinkage} estimator reduces the estimate variance at the expense of increasing its bias (known as the bias-variance tradeoff),
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# create random covariance matrix
set.seed(1121)
mat_rix <- matrix(rnorm(5e2), nc=5)
cov_mat <- cov(mat_rix)
cor_mat <- cor(mat_rix)
std_dev <- sqrt(diag(cov_mat))
# calculate target matrix
cor_mean <- mean(cor_mat[upper.tri(cor_mat)])
tar_get <- matrix(cor_mean, nr=NROW(cov_mat), nc=NCOL(cov_mat))
diag(tar_get) <- 1
tar_get <- t(t(tar_get * std_dev) * std_dev)
# calculate shrinkage covariance matrix
al_pha <- 0.5
cov_shrink <- (1-al_pha)*cov_mat + al_pha*tar_get
# calculate inverse matrix
in_verse <- solve(cov_shrink)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Regularized Inverse of Covariance Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The statistical errors in the covariance matrix are most pronounced in the higher order eigenvalues and eigenvectors,
      \vskip1ex
      The \emph{regularization} technique calculates the inverse of the covariance matrix while reducing the effects of statistical errors,
      \vskip1ex
      The \emph{regularization} technique involves calculating the inverse of the covariance matrix $\mathbb{C}$ from a limited number of eigenvectors, ignoring the higher order eigenvectors:
      \begin{displaymath}
        \mathbb{C}^{-1} = \mathbb{O}_n \, \mathbb{D}_n^{-1} \, \mathbb{O}_n^T
      \end{displaymath}
      Where $\mathbb{D}_n$ and $\mathbb{O}_n$ are matrices with the higher order eigenvalues and eigenvectors removed, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# create random covariance matrix
set.seed(1121)
mat_rix <- matrix(rnorm(5e2), nc=5)
cov_mat <- cov(mat_rix)
# perform eigen decomposition
ei_gen <- eigen(cov_mat)
eigen_vec <- ei_gen$vectors
# calculate regularized inverse matrix
max_eigen <- 2
in_verse <- eigen_vec[, 1:max_eigen] %*% 
  (t(eigen_vec[, 1:max_eigen]) / ei_gen$values[1:max_eigen])
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Machine Learning}


%%%%%%%%%%%%%%%
\subsection{draft: Data Science}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Data Science is very important to quantitative finance,
      \vskip1ex
      Below is an example of a simulation of the path of \emph{Brownian Motion} crossing a barrier level,
        <<echo=(-(1:2)),eval=FALSE>>=
par(oma=c(1, 1, 1, 1), mgp=c(2, 1, 0), mar=c(5, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
set.seed(1121)  # reset random number generator
bar_rier <- 20  # barrier level
len_gth <- 1000  # number of simulation steps
pa_th <- numeric(len_gth)  # allocate path vector
pa_th[1] <- 0  # initialize path
in_dex <- 2  # initialize simulation index
while ((in_dex <= len_gth) &&
         (pa_th[in_dex - 1] < bar_rier)) {
# simulate next step
  pa_th[in_dex] <-
    pa_th[in_dex - 1] + rnorm(1)
  in_dex <- in_dex + 1  # advance in_dex
}  # end while
# fill remaining pa_th after it crosses bar_rier
if (in_dex <= len_gth)
  pa_th[in_dex:len_gth] <- pa_th[in_dex - 1]
# create daily time series starting 2011
ts_path <- ts(data=pa_th, frequency=365, start=c(2011, 1))
plot(ts_path, type="l", col="black",  # create plot
     lty="solid", xlab="", ylab="")
abline(h=bar_rier, lwd=2, col="red")  # add horizontal line
title(main="Brownian motion crossing a barrier level",
      line=0.5)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/data_science.png}
  \end{columns}
\end{block}

\end{frame}


\end{document}
