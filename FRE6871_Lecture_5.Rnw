% FRE6871_Lecture_5

% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='scriptsize', fig.width=4, fig.height=4)
options(width=60, dev='pdf')
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{array}
\usepackage{multirow}
\usepackage{mathtools}
% bbold package for unitary vector or matrix symbol
\usepackage{bbold}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[FRE6871 Lecture\#5]{FRE6871 \texttt{R} in Finance}
\subtitle{Lecture\#5, Fall 2018}

\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@poly.edu}
\date{October 8, 2018}
% \date{\today}
% \pgfdeclareimage[height=0.5cm]{university-logo}{engineering_long_white}
% \logo{\pgfuseimage{engineering_long_white}}


%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Classification}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Logistic} Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{logistic} function expresses the probability of a numerical variable ranging over the whole interval of real numbers:
      \begin{displaymath}
        p(x) = \frac{1}{1 + \exp(-\lambda x)}
      \end{displaymath}
      Where $\lambda$ is the scale (dispersion) parameter,
      \vskip1ex
      The \emph{logistic} function can be inverted to obtain the \emph{Odds Ratio} (the ratio of probabilities for favorable to unfavorable outcomes):
      \begin{displaymath}
        \frac{p(x)}{1 - p(x)} = \exp(\lambda x)
      \end{displaymath}
      The function \texttt{plogis()} gives the cumulative probability of the \emph{Logistic} distribution,
        <<echo=(-(1:1)),eval=FALSE>>=
par(oma=c(1, 1, 1, 1), mar=c(2, 1, 1, 1), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
lamb_da <- c(0.5, 1, 1.5)
col_ors <- c("red", "black", "blue")
# plot three curves in loop
for (in_dex in 1:3) {
  curve(expr=plogis(x, scale=lamb_da[in_dex]),
        xlim=c(-4, 4), type="l",
        xlab="", ylab="", lwd=2,
        col=col_ors[in_dex], add=(in_dex>1))
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/logistic_func.png}
      \vspace{-2em}
        <<echo=TRUE,eval=FALSE>>=
# add title
title(main="Logistic function", line=0.5)
# add legend
legend("topleft", title="Scale parameters",
       paste("lambda", lamb_da, sep="="),
       inset=0.05, cex=0.8, lwd=2,
       lty=c(1, 1, 1), col=col_ors)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing \protect\emph{Logistic} Regression Using the Function \texttt{glm()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Linear} regression isn't suitable when the response variable is categorical data (\texttt{factor}),
      \vskip1ex
      But \emph{logistic} regression (\emph{logit}) can be used to model data with a categorical response variable,
      \vskip1ex
      The function \texttt{glm()} fits generalized linear models, including \emph{logistic} regressions,
      \vskip1ex
      \texttt{glm()} can fit two different types of response variables: categorical data (\texttt{factors}) from individual observations, or counts of categorical data (\texttt{integers}) from groups of observations,
      \vskip1ex
      The family object \texttt{binomial(link="logit")} specifies a binomial distribution of residuals,
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
set.seed(1121)
# simulate overlapping scores data
scores_1 <- runif(100, max=0.6)
scores_2 <- runif(100, min=0.4)
# perform Wilcoxon test for mean
wilcox.test(scores_1, scores_2)
# combine scores and add categorical variable
score_s <- c(scores_1, scores_2)
res_ponse <- c(logical(100), !logical(100))
# perform logit regression
g_lm <- glm(res_ponse ~ score_s, family=binomial(logit))
class(g_lm)
summary(g_lm)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/logistic_density.png}
      \vspace{-2em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=7, height=5)
par(mar=c(3, 3, 2, 2), mgp=c(2, 1, 0), oma=c(0, 0, 0, 0))
or_der <- order(score_s)
plot(x=score_s[or_der], y=g_lm$fitted.values[or_der], 
     type="l", lwd=4, col="orange",
     main="Category Densities and Logistic Function",
     xlab="score", ylab="density")
den_sity <- density(score_s[res_ponse])
den_sity$y <- den_sity$y/max(den_sity$y)
lines(den_sity, col="red")
polygon(c(min(den_sity$x), den_sity$x, max(den_sity$x)), c(min(den_sity$y), den_sity$y, min(den_sity$y)), col=rgb(1, 0, 0, 0.2), border=NA)
den_sity <- density(score_s[!res_ponse])
den_sity$y <- den_sity$y/max(den_sity$y)
lines(den_sity, col="blue")
polygon(c(min(den_sity$x), den_sity$x, max(den_sity$x)), c(min(den_sity$y), den_sity$y, min(den_sity$y)), col=rgb(0, 0, 1, 0.2), border=NA)
# add legend
legend(x="top", cex=1.0, bty="n", lty=c(1, NA, NA), 
       lwd=c(6, NA, NA), pch=c(NA, 15, 15),
       legend=c("logistic fit", "TRUE", "FALSE"),
       col=c("orange", "red", "blue"), 
       text.col=c("black", "red", "blue"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{ISLR} With Datasets for Machine Learning}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{ISLR} contains datasets used in the book \emph{"Introduction to Statistical Learning"}:\
      \fullcite{islbook}
    \column{0.5\textwidth}
      <<echo=TRUE,eval=FALSE>>=
library(ISLR)  # load package ISLR
# get documentation for package tseries
packageDescription("ISLR")  # get short description

help(package="ISLR")  # load help page

library(ISLR)  # load package ISLR

data(package="ISLR")  # list all datasets in ISLR

ls("package:ISLR")  # list all objects in ISLR

detach("package:ISLR")  # remove ISLR from search path
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \texttt{Default} Dataset}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{Default} dataset is a data frame in package \emph{ISLR}, with credit default data,
      \vskip1ex
      The \texttt{Default} data frame contains two columns of binary categorical data (\texttt{factors}): \texttt{default} and \texttt{student}, and two columns of numerical data: \texttt{balance} and \texttt{income},
      \vskip1ex
      The columns \texttt{student}, \texttt{balance}, and \texttt{income} can be used as \emph{predictors} to predict the \texttt{default} column,
      <<echo=TRUE,eval=FALSE>>=
library(ISLR)  # load package ISLR
# load credit default data
attach(Default)
summary(Default)
sapply(Default, class)
dim(Default); head(Default)
x_lim <- range(balance)
y_lim <- range(income)
# plot data points for non-defaulters
default_ed <- (default=="Yes")
plot(income ~ balance,
     main="Default Dataset from Package ISLR",
     xlim=x_lim, ylim=y_lim,
     data=Default[!default_ed, ],
     pch=4, col="blue")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/islr_default_data.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot data points for defaulters
points(income ~ balance,
       data=Default[default_ed, ],
       pch=4, lwd=2, col="red")
# add legend
legend(x="topright", bty="n",
       legend=c("non-defaulters", "defaulters"),
       col=c("blue", "red"), lty=1, lwd=6, pch=4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Boxplots of the \texttt{Default} Dataset}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{Box Plot} (box-and-whisker plot) is a graphical display of a distribution of values,
      \vskip1ex
      The \emph{box} represents the upper and lower quartiles, \\
      the vertical lines (whiskers) represent values beyond the quartiles, \\
      and open circles represent values beyond the nominal range (outliers),
      \vskip1ex
      The function \texttt{boxplot()} plots a box-and-whisker plot for a distribution of values,
      \vskip1ex
      \texttt{boxplot()} has two \texttt{methods}: one for \texttt{formula} objects (involving categorical variables), and another for \texttt{data frames},
      \vskip1ex
      The \emph{Wilcoxon} test shows that the \texttt{balance} column provides a strong separation between defaulters and non-defaulters, but the \texttt{income} column doesn't,
      <<echo=TRUE,eval=FALSE>>=
default_ed <- (default=="Yes")
# Wilcoxon test for balance predictor
wilcox.test(balance[default_ed], balance[!default_ed])
# Wilcoxon test for income predictor
wilcox.test(income[default_ed], income[!default_ed])
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/islr_default_boxplot.png}
      \vspace{-2em}
      <<echo=(-(1:2)),eval=FALSE>>=
library(ISLR)  # load package ISLR
attach(Default)  # load credit default data
par(mfrow=c(1,2))  # set plot panels
# balance boxplot
boxplot(formula=balance ~ default,
        col="lightgrey",
        main="balance", xlab="default")
# income boxplot
boxplot(formula=income ~ default,
        col="lightgrey",
        main="income", xlab="default")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Modeling Credit Defaults Using \protect\emph{Logistic} Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{balance} column can be used to calculate the probability of default using \emph{logistic} regression,
      \vskip1ex
      The residuals in \emph{logistic} regression are the differences betweeen the actual response values (\texttt{0} and \texttt{1}), and the calculated probabilities of default,
      \vskip1ex
      The \emph{logit} residuals are not normally distributed, so the data is fitted using the \emph{maximum-likelihood} method, instead of least squares,
      \vskip1ex
      The family object \texttt{binomial(link="logit")} specifies a binomial distribution of residuals in the \emph{logistic} regression model,
      <<echo=TRUE,eval=FALSE>>=
# fit logistic regression model
g_lm <- glm(default ~ balance,
              family=binomial(logit))
class(g_lm)
summary(g_lm)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/islr_logistic_reg.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
plot(x=balance, y=default_ed,
     main="Logistic regression of credit defaults", col="orange",
     xlab="credit balance", ylab="defaults")
or_der <- order(balance)
lines(x=balance[or_der], y=g_lm$fitted.values[or_der],
      col="blue", lwd=2)
legend(x="topleft", inset=0.1,
       legend=c("defaults", "logit fitted values"),
       col=c("orange", "blue"), lty=c(NA, 1), pch=c(1, NA), lwd=c(3, 3))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Modeling Cumulative Defaults Using \protect\emph{Logistic} Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{glm()} can model a \emph{logistic} regression using either a \texttt{Boolean} response variable, or using a response variable specified as a frequency, 
      \vskip1ex
      In the second case, the response variable should be defined as a two-column matrix, with the cumulative frequency of success (\texttt{TRUE}) and a cumulative frequency of failure (\texttt{FALSE}),
      \vskip1ex
      These two different ways of specifying the \emph{logistic} regression are related, but they are not equivalent, because they have different error terms,
      <<echo=(-(1:2)),eval=FALSE>>=
library(ISLR)  # load package ISLR
attach(Default)  # load credit default data
# calculate cumulative defaults
default_ed <- (default=="Yes")
to_tal <- sum(default_ed)
default_s <- sapply(balance, function(ba_lance) {
    sum(default_ed[balance <= ba_lance])
})  # end sapply
# perform logit regression
g_lm <- glm(
  cbind(default_s, to_tal-default_s) ~
    balance,
  family=binomial(logit))
summary(g_lm)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/islr_logistic_count.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
plot(x=balance, y=default_s/to_tal, col="orange", lwd=1,
     main="Cumulative defaults versus balance",
     xlab="credit balance", ylab="cumulative defaults")
or_der <- order(balance)
lines(x=balance[or_der], y=g_lm$fitted.values[or_der],
      col="blue", lwd=2)
legend(x="topleft", inset=0.1,
       legend=c("cumulative defaults", "fitted values"),
       col=c("orange", "blue"), lty=c(NA, 1), pch=c(1, NA), lwd=c(3, 3))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Multifactor \protect\emph{Logistic} Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Logistic} regression calculates the probability of categorical variables, from the \emph{Odds Ratio} of continuous \emph{predictors}:
      \begin{displaymath}
        p = \frac{1}{1 + \exp(- \lambda_0 - \sum_{i=1}^n \lambda_i x_i)}
      \end{displaymath}
      The \emph{generic} function \texttt{summary()} produces a list of regression model summary and diagnostic statistics:
      \begin{itemize}
        \item coefficients: matrix with estimated coefficients, their \emph{z}-values, and \emph{p}-values,
        \item \emph{Null} deviance: measures the differences betweeen the response values and the probabilities calculated using only the intercept,
        \item \emph{Residual} deviance: measures the differences betweeen the response values and the model probabilities,
      \end{itemize}
      The \texttt{balance} and \texttt{student} columns are statistically significant, but the \texttt{income} column is not,
    \column{0.5\textwidth}
      \vspace{-2em}
      <<echo=(-(1:3)),eval=TRUE>>=
library(ISLR)  # load package ISLR
attach(Default)  # load credit default data
# fit multifactor logistic regression model
col_names <- colnames(Default)
for_mula <- as.formula(paste(col_names[1],
  paste(col_names[-1], collapse="+"), sep=" ~ "))
g_lm <- glm(for_mula, data=Default,
              family=binomial(logit))
summary(g_lm)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Confounding Variables in Multifactor \protect\emph{Logistic} Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{student} column is a confounding variable since it's correlated with the \texttt{balance} column,
      \vskip1ex
      Students are less likely to default than non-students with the same \texttt{balance},
      \vskip1ex
      But on average students have higher \texttt{balances} than non-students, which makes them more likely to default,
      \vskip1ex
      That's why the multifactor regression coefficient for \texttt{student} is negative, while the single factor coefficient for \texttt{student} is positive,
      <<echo=(-(1:2)),eval=FALSE>>=
library(ISLR)  # load package ISLR
attach(Default)  # load credit default data
default_ed <- (default=="Yes")
stu_dent <- (student=="Yes")
# calculate cumulative defaults
default_s <- sapply(balance,
  function(ba_lance) {
    c(stu_dent=sum(default_ed[stu_dent & (balance <= ba_lance)]),
      non_student=sum(default_ed[(!stu_dent) & (balance <= ba_lance)]))
})  # end sapply
to_tal <- c(sum(default_ed[stu_dent]), sum(default_ed[!stu_dent]))
default_s <- t(default_s / to_tal)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/islr_student_boxplot.png}
      \vspace{-2em}
      <<echo=(-(1:2)),eval=FALSE>>=
library(ISLR)  # load package ISLR
attach(Default)  # load credit default data
# plot cumulative defaults
par(mfrow=c(1,2))  # set plot panels
or_der <- order(balance)
plot(x=balance[or_der], y=default_s[or_der, 1],
     col="red", t="l", lwd=2,
     main="Cumulative defaults of\n students and non-students",
     xlab="credit balance", ylab="")
lines(x=balance[or_der], y=default_s[or_der, 2],
      col="blue", lwd=2)
legend(x="topleft", bty="n",
       legend=c("students", "non-students"),
       col=c("red", "blue"), text.col=c("red", "blue"),
       lwd=3)
# balance boxplot for student factor
boxplot(formula=balance ~ student,
        col="lightgrey",
        main="balance", xlab="student")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Forecasting Credit Defaults using Logistic Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{predict()} is a generic function for forecasting based on a given model.
      \vskip1ex
      The method \texttt{predict.glm()} produces forecasts for a generalized linear model, in the form of probabilities for the \texttt{Boolean} response variable.
      \vskip1ex
      The \texttt{Boolean} forecasts are obtained by comparing the forecast probabilities with a discrimination threshold.
      \vskip1ex
      The null hypothesis is that \texttt{default="No"}.
      \vskip1ex
      A positive result corresponds to rejecting the null hypothesis, while a negative result corresponds to accepting the null hypothesis.
      \vskip1ex
      The forecasts are subject to two different types of errors: \emph{type I} and \emph{type II} errors.
      \vskip1ex
      A \emph{type I} error is the incorrect rejection of a \texttt{TRUE} null hypothesis (i.e. a "false positive"), for example, when there is no default, but it's forecast to be a default.
      \vskip1ex
      A \emph{type II} error is the incorrect acceptance of a \texttt{FALSE} null hypothesis (i.e. a "false negative"), for example, when there is a default, but it's forecast to be no default.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# fit full logistic regression model
for_mula <- as.formula(paste(col_names[1],
  paste(col_names[-1], collapse="+"), sep=" ~ "))
g_lm <- glm(for_mula, data=Default, family=binomial(logit))
fore_casts <- predict(g_lm, type="response")
fore_casts[1:6]
all.equal(g_lm$fitted.values, fore_casts)
# discrimination threshold
thresh_old <- 0.05
# calculate confusion matrix
table(default_ed, (fore_casts>thresh_old))
sum(default_ed)
sum(Default$default=="Yes")
# fit logistic regression over training data
set.seed(1121)  # reset random number generator
sam_ple <- sample(x=1:NROW(Default), size=NROW(Default)/2)
train_data <- Default[sam_ple, ]
g_lm <- glm(for_mula, data=train_data, family=binomial(link="logit"))
# forecast over test data
test_data <- Default[-sam_ple, ]
fore_casts <- predict(g_lm, newdata=test_data, type="response")
# calculate confusion matrix
table(test_data$default=="No",
      (fore_casts<thresh_old))
# FALSE positive (type I error)
sum(test_data$default=="No" & (fore_casts>thresh_old))
# FALSE negative (type II error)
sum(test_data$default=="Yes" & (fore_casts<thresh_old))
detach(Default)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Confusion Matrix of a Classification Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The confusion matrix summarizes the performance of a classification model on a set of test data for which the true values are known.
      \vskip1ex
      The \emph{true positive} rate (known as the \emph{sensitivity}) is the fraction of \texttt{FALSE} null hypothesis cases that are correctly classified as \texttt{FALSE}.
      \vskip1ex
      The \emph{false negative} rate is the fraction of \texttt{FALSE} null hypothesis cases that are incorrectly classified as \texttt{TRUE} (\emph{type II} error).
      \vskip1ex
      The sum of the \emph{true positive} plus the \emph{false negative} rate is equal to \texttt{1}.
      \vskip1ex
      The \emph{true negative} rate (known as the \emph{specificity}) is the fraction of \texttt{TRUE} null hypothesis cases that are correctly classified as \texttt{TRUE}.
      \vskip1ex
      The \emph{false positive} rate is the fraction of \texttt{TRUE} null hypothesis cases that are incorrectly classified as \texttt{FALSE} (\emph{type I} error).
      \vskip1ex
      The sum of the \emph{true negative} plus the \emph{false positive} rate is equal to \texttt{1}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:11)),eval=TRUE>>=
library(ISLR)  # load package ISLR
attach(Default)  # load credit default data
col_names <- colnames(Default)
for_mula <- as.formula(paste(col_names[1], paste(col_names[-1], collapse="+"), sep=" ~ "))
set.seed(1121)  # reset random number generator
sam_ple <- sample(x=1:NROW(Default), size=NROW(Default)/2)
train_data <- Default[sam_ple, ]
g_lm <- glm(for_mula, data=train_data, family=binomial(link="logit"))
test_data <- Default[-sam_ple, ]
fore_casts <- predict(g_lm, newdata=test_data, type="response")
thresh_old <- 0.05
# calculate confusion matrix
confu_sion <- table(test_data$default=="No",
                    (fore_casts<thresh_old))
dimnames(confu_sion) <- list(hypothesis=rownames(confu_sion),
  forecast=colnames(confu_sion))
confu_sion
confu_sion <- confu_sion / rowSums(confu_sion)
c(typeI=confu_sion[2, 1], typeII=confu_sion[1, 2])
      @
      <<eval=FALSE,results='asis',echo=FALSE>>=
# below is an unsuccessful attempt to draw confusion matrix using xtable
confusion_matrix <- matrix(c("| true positive \\\\ (sensitivity)", "| false negative \\\\ (type II error)", "| false positive \\\\ (type I error)", "| true negative \\\\ (specificity)"), nc=2)
dimnames(confusion_matrix) <- list(forecast=c("FALSE", "TRUE"),
                                   hypothesis=c("FALSE", "TRUE"))
print(xtable::xtable(confusion_matrix,
      caption="Confusion Matrix"),
      caption.placement="top",
      comment=FALSE, size="scriptsize",
      include.rownames=TRUE,
      include.colnames=TRUE)
# end unsuccessful attempt to draw confusion table using xtable
      @
      \newcommand\MyBox[2]{
        \fbox{\lower0.75cm
          \vbox to 1.2cm{\vfil
            \hbox to 1.7cm{\parbox{\textwidth}{#1\\#2}}
            \vfil}
        }
      }
      \renewcommand\arraystretch{0.3}
      \setlength\tabcolsep{0pt}
      \begin{tabular}{c >{\bfseries}r @{\hspace{0.5em}}c @{\hspace{0.4em}}c @{\hspace{0.5em}}l}
      \multirow{10}{*}{\parbox{0.5cm}{\bfseries Hypothesis}} &
      & \multicolumn{2}{c}{\bfseries Forecast} & \\
      & & \bfseries FALSE & \bfseries TRUE \\
      & FALSE & \MyBox{True Positive}{(sensitivity)} & \MyBox{False Negative}{(type II error)} \\[2.4em]
      & TRUE & \MyBox{False Positive}{(type I error)} & \MyBox{True Negative}{(specificity)}
      \end{tabular}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Receiver Operating Characteristic (ROC) Curve}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The ROC curve is the plot of the \emph{true positive} rate, as a function of the \emph{false positive} rate, and illustrates the performance of a binary classifier,
      \vskip1ex
      The area under the ROC curve (AUC) is a measure of the performance of a binary classification model,
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# confusion matrix as function of thresh_old
con_fuse <- function(res_ponse, fore_casts, thresh_old) {
    confu_sion <- table(res_ponse, (fore_casts<thresh_old))
    confu_sion <- confu_sion / rowSums(confu_sion)
    c(typeI=confu_sion[2, 1], typeII=confu_sion[1, 2])
  }  # end con_fuse
con_fuse(test_data$default=="No", fore_casts, thresh_old=thresh_old)
# define vector of discrimination thresholds
threshold_s <- seq(0.01, 0.95, by=0.01)^2
# calculate error rates
error_rates <- sapply(threshold_s, con_fuse,
  res_ponse=(test_data$default=="No"),
  fore_casts=fore_casts)  # end sapply
error_rates <- t(error_rates)
# calculate area under ROC curve (AUC)
(error_rates[, "typeII"] - 1) %*%
  rutils::diff_it(error_rates[, "typeI"])
# or
type_two <- (error_rates[, "typeII"] +
  rutils::lag_it(error_rates[, "typeII"]))/2
(type_two - 1) %*% rutils::diff_it(error_rates[, "typeI"])
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/islr_defaults_roc.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot ROC Curve for Defaults
plot(x=error_rates[, "typeI"],
     y=1-error_rates[, "typeII"],
     xlab="FALSE positive rate",
     ylab="TRUE positive rate",
     main="ROC Curve for Defaults",
     type="l", lwd=3, col="blue")
abline(a=0.0, b=1.0, lwd=3, col="orange")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Bonds and Interest Rates}


%%%%%%%%%%%%%%%
\subsection{Downloading Treasury Bond Rates from \protect\emph{FRED}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The constant maturity Treasury rates are yields of hypothetical fixed-maturity bonds, interpolated from the market yields of actual Treasury bonds, 
      \vskip1ex
      The \emph{FRED} database contains current and historical constant maturity Treasury rates, \\
      \hskip1em\url{https://fred.stlouisfed.org/series/DGS5}
      \vskip1ex
      \texttt{getSymbols()} creates objects in the specified \emph{environment} from the input strings (names),
      \vskip1ex
      It then assigns the data to those objects, without returning them as a function value, as a \emph{side effect},
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# symbols for constant maturity Treasury rates
sym_bols <- c("DGS1", "DGS2", "DGS5", "DGS10", "DGS20", "DGS30")
library(quantmod)  # load package quantmod
env_rates <- new.env()  # new environment for data
# download data for sym_bols into env_rates
getSymbols(sym_bols, env=env_rates, src="FRED")
ls(env_rates)  # list files in env_rates
# get class of object in env_rates
class(get(x=sym_bols[1], envir=env_rates))
# another way
class(env_rates$DGS10)
colnames(env_rates$DGS10)
save(env_rates, file="C:/Develop/R/lecture_slides/data/rates_data.RData")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/treas_10y_rate.png}
    \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=4)
par(mar=c(2, 2, 0, 0), oma=c(0, 0, 0, 0))
head(env_rates$DGS10, 3)
# get class of all objects in env_rates
eapply(env_rates, class)
# get class of all objects in R workspace
lapply(ls(), function(ob_ject) class(get(ob_ject)))
# plot 10-year constant maturity Treasury rate
chart_Series(env_rates$DGS10["1990/"], 
  name="10-year constant maturity Treasury rate")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Treasury Yield Curve}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{yield curve} is a vector of interest rates at different maturities, on a given date,
      \vskip1ex
      The \emph{yield curve} shape changes depending on the economic conditions: in recessions rates drop and the curve flattens, while in expansions rates rise and the curve steepens, 
    \vspace{-1em}
      <<echo=(-(1:3)),eval=FALSE>>=
par(mar=c(3, 3, 2, 0), oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
# load constant maturity Treasury rates
load(file="C:/Develop/R/lecture_slides/data/rates_data.RData")
# get end-of-year dates since 2006
date_s <- xts::endpoints(env_rates$DGS1["2006/"], on="years")
date_s <- zoo::index(env_rates$DGS1["2006/"])[date_s]
# create time series of end-of-year rates
rate_s <- eapply(env_rates, function(ra_te) ra_te[date_s])
rate_s <- rutils::do_call(cbind, rate_s)
# rename columns and rows, sort columns, and transpose into matrix
colnames(rate_s) <- substr(colnames(rate_s), start=4, stop=11)
rate_s <- rate_s[, order(as.numeric(colnames(rate_s)))]
colnames(rate_s) <- paste0(colnames(rate_s), "yr")
rate_s <- t(rate_s)
colnames(rate_s) <- substr(colnames(rate_s), start=1, stop=4)
# plot matrix using plot.zoo()
col_ors <- colorRampPalette(c("red", "blue"))(NCOL(rate_s))
plot.zoo(rate_s, main="Yield curve since 2006", lwd=3, xaxt="n", 
         plot.type="single", xlab="maturity", ylab="yield", col=col_ors)
# add x-axis
axis(1, seq_along(rownames(rate_s)), rownames(rate_s))
# add legend
legend("bottomright", legend=colnames(rate_s),
       col=col_ors, lty=1, lwd=4, inset=0.05, cex=0.8)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/yield_curve.png}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# alternative plot using matplot()
matplot(rate_s, main="Yield curve since 2006", xaxt="n", lwd=3, lty=1, 
        type="l", xlab="maturity", ylab="yield", col=col_ors)
# add x-axis
axis(1, seq_along(rownames(rate_s)), rownames(rate_s))
# add legend
legend("bottomright", legend=colnames(rate_s),
       col=col_ors, lty=1, lwd=4, inset=0.05, cex=0.8)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Correlation Matrix of the Yield Curve}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The covariance matrix $\mathbf{V}$, of the data matrix $\mathbf{r}$, is given by:
      \begin{displaymath}
        \mathbf{V} = \frac{\mathbf{r}^T \, \mathbf{r}} {n-1}
      \end{displaymath}
      \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=4)
par(mar=c(0, 0, 0, 0), oma=c(0, 0, 0, 0), mgp=c(0, 0, 0))
# load constant maturity Treasury rates
load(file="C:/Develop/R/lecture_slides/data/rates_data.RData")
# symbols for constant maturity Treasury rates
sym_bols <- c("DGS1", "DGS2", "DGS5", "DGS10", "DGS20")
# calculate daily rates changes
rate_s <- xts:::na.locf.xts(rutils::do_call(cbind, 
    as.list(env_rates)[sym_bols]))
rate_s <- xts:::na.locf.xts(rate_s)
rate_s <- xts:::na.locf.xts(rate_s, fromLast=TRUE)
re_turns <- rutils::diff_it(rate_s)
date_s <- index(re_turns)
# de-mean (center) and scale the returns
re_turns <- t(t(re_turns) - colMeans(re_turns))
re_turns <- t(t(re_turns) / sqrt(colSums(re_turns^2)/(NROW(re_turns)-1)))
re_turns <- xts(re_turns, date_s)
# correlation matrix of Treasury rates
cor_mat <- cor(re_turns)
# reorder correlation matrix based on clusters
library(corrplot)
or_der <- corrMatOrder(cor_mat, order="hclust", 
  hclust.method="complete")
cor_mat <- cor_mat[or_der, or_der]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/cor_yield.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot the correlation matrix
col_ors <- colorRampPalette(c("red", "white", "blue"))
corrplot(cor_mat, title=NA, tl.col="black", 
  tl.cex=0.8, mar=c(0,0,0,0), method="square", 
  col=col_ors(8), cl.offset=0.75, cl.cex=0.7, 
  cl.align.text="l", cl.ratio=0.25)
title("Correlation of Treasury rates", line=-1)
# draw rectangles on the correlation matrix plot
corrRect.hclust(cor_mat, k=NROW(cor_mat) %/% 2, 
  method="complete", col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Principal Component Vectors}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Principal components} are linear combinations of the \texttt{k} return vectors $\mathbf{r}_i$:
      \begin{displaymath}
        \mathbf{pc}_j = \sum_{i=1}^k {w_{ij} \, \mathbf{r}_i}
      \end{displaymath}
      Where $\mathbf{w}_j$ is a vector of weights (loadings) of the \emph{principal component} \texttt{j}, with $\mathbf{w}_j^T \mathbf{w}_j = 1$,
      \vskip1ex
      The weights $\mathbf{w}_j$ are chosen to maximize the variance of the \emph{principal components}, under the condition that they are orthogonal:
      \begin{align*}
        \mathbf{w}_j = {\operatorname{\arg \, \max}} \, \left\{ \mathbf{pc}_j^T \, \mathbf{pc}_j \right\} \\
        \mathbf{pc}_i^T \, \mathbf{pc}_j = 0 \> (i \neq j)
      \end{align*}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# create initial vector of portfolio weights
n_weights <- NROW(sym_bols)
weight_s <- rep(1/sqrt(n_weights), n_weights)
names(weight_s) <- sym_bols
# objective function equal to minus portfolio variance
object_ive <- function(weight_s, re_turns) {
  portf_rets <- re_turns %*% weight_s
  -sum(portf_rets*portf_rets) + 
    1e7*(1 - sum(weight_s*weight_s))^2
}  # end object_ive
# objective for equal weight portfolio
object_ive(weight_s, re_turns)
# compare speed of vector multiplication methods
summary(microbenchmark(
  trans_pose=t(re_turns) %*% re_turns,
  s_um=sum(re_turns*re_turns),
  times=10))[, c(1, 4, 5)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/pca_rates_load.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# find weights with maximum variance
optim_run <- optim(par=weight_s,
  fn=object_ive,
  re_turns=re_turns,
  method="L-BFGS-B",
  upper=rep(1.0, n_weights),
  lower=rep(-1.0, n_weights))
# optimal weights and maximum variance
weight_s <- optim_run$par
-object_ive(weight_s, re_turns)
# plot first principal component loadings
barplot(weight_s, names.arg=names(weight_s), 
  xlab="", ylab="", 
  main="first principal component loadings")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Higher Order Principal Components}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{second principal component} can be calculated by maximizing its variance, under the constraint that it must be orthogonal to the \emph{first principal component}, 
      \vskip1ex
      Similarly, higher order \emph{principal components} can be calculated by maximizing their variances, under the constraint that they must be orthogonal to all the previous \emph{principal components}, 
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# pc1 weights and returns
weights_1 <- weight_s
pc_1 <- re_turns %*% weights_1
# redefine objective function
object_ive <- function(weight_s, re_turns) {
  portf_rets <- re_turns %*% weight_s
  -sum(portf_rets*portf_rets) + 
    1e7*(1 - sum(weight_s*weight_s))^2 + 
    1e7*sum(pc_1*portf_rets)^2
}  # end object_ive
# find second principal component weights
optim_run <- optim(par=weight_s,
                   fn=object_ive,
                   re_turns=re_turns,
                   method="L-BFGS-B",
                   upper=rep(1.0, n_weights),
                   lower=rep(-1.0, n_weights))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/pca_rates_load2.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# pc2 weights and returns
weights_2 <- optim_run$par
pc_2 <- re_turns %*% weights_2
sum(pc_1*pc_2)
# plot second principal component loadings
barplot(weights_2, names.arg=names(weights_2), 
        xlab="", ylab="", 
        main="second principal component loadings")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Eigenvalues of the Covariance Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The portfolio variance: $w^T \, \mathbb{C} \, w$ can be maximized under the constraint $w^T w = 1$, by maximizing the \emph{Lagrangian}:
      \begin{displaymath}
        \mathcal{L} = w^T \, \mathbb{C} \, w \, - \, \lambda \, (w^T w - 1)
      \end{displaymath}
      Where $\lambda$ is a \emph{Lagrange multiplier}, 
      \vskip1ex
      The weights corresponding to the maximum portfolio variance can be found by differentiating $\mathcal{L}$ with respect to $w$ and setting it to zero:
      \begin{displaymath}
        \mathbb{C} \, w = \lambda \, w
      \end{displaymath}
      The above is the \emph{eigenvalue} equation of the covariance matrix $\mathbb{C}$,
      \vskip1ex
      The optimal weights $w$ form an \emph{eigenvector}, and $\lambda$ is the \emph{eigenvalue} corresponding to the \emph{eigenvector} $w$, 
      \vskip1ex
      The \emph{eigenvalues} are the variances of the \emph{eigenvectors}, and their sum is equal to the sum of the return variances:
      \begin{displaymath}
        \sum_{i=1}^k \lambda_i = \sum_{i=1}^k r_i^T r_i
      \end{displaymath}
      The number of \emph{eigenvalues} is equal to the dimension of the covariance matrix,
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/pca_rates_eigenvalues.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# covariance matrix and variance vector of returns
cov_mat <- cov(re_turns)
vari_ance <- diag(cov_mat)
cor_mat <- cor(re_turns)
# calculate eigenvectors and eigenvalues
ei_gen <- eigen(cov_mat)
ei_gen$vectors
weights_1
weights_2
ei_gen$values[1]
var(pc_1)
(cov_mat %*% weights_1) / weights_1
ei_gen$values[2]
var(pc_2)
(cov_mat %*% weights_2) / weights_2
sum(vari_ance)
sum(ei_gen$values)
barplot(ei_gen$values, # plot eigenvalues
  names.arg=paste0("PC", 1:n_weights), 
  las=3, xlab="", ylab="", main="Principal Component Variances")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Principal Component Analysis of the Yield Curve}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Principal Component Analysis} (\emph{PCA}) is a \emph{dimensionality reduction} technique, that explains the returns of a large number of correlated time series as linear combinations of a smaller number of principal component time series,
      \vskip1ex
      The input time series are often scaled by their standard deviations, to improve the accuracy of \emph{PCA dimensionality reduction}, so that more information is retained by the first few \emph{principal component} time series,
      \vskip1ex
      If the input time series are not scaled, then \emph{PCA} analysis is equvalent to the \emph{eigen decomposition} of the covariance matrix, and if they are scaled, then \emph{PCA} analysis is equvalent to the \emph{eigen decomposition} of the correlation matrix,
      \vskip1ex
      The function \texttt{prcomp()} performs \emph{Principal Component Analysis} on a matrix of data (with the time series as columns), and returns the results as an object of class \texttt{prcomp}, 
      \vskip1ex
      The \texttt{prcomp()} argument \texttt{scale=TRUE} specifies that the input time series should be scaled by their standard deviations,
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/pca_rates_scree.png}
      A \emph{scree plot} is a bar plot of the volatilities of the \emph{principal components}, 
      <<echo=TRUE,eval=FALSE>>=
# perform principal component analysis PCA
pc_a <- prcomp(re_turns, scale=TRUE)
# plot standard deviations
barplot(pc_a$sdev, 
  names.arg=colnames(pc_a$rotation), 
  las=3, xlab="", ylab="", 
  main="Scree Plot: Volatilities of Principal Components 
  of Treasury rates")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Yield Curve Principal Component Loadings (Weights)}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Principal component} loadings are the weights of portfolios which have mutually orthogonal returns,
      \vskip1ex
      The \emph{principal component} portfolios represent the different orthogonal modes of the data variance, 
      \vskip1ex
      The first \emph{principal component} of the \emph{yield curve} is the correlated movement of all rates up and down,
      \vskip1ex
      The second \emph{principal component} is \emph{yield curve} steepening and flattening,
      \vskip1ex
      The third \emph{principal component} is the \emph{yield curve} butterfly movement,
      <<echo=(-(1:1)),eval=FALSE>>=
x11(width=6, height=7)
# principal component loadings (weights)
pc_a$rotation
# plot loading barplots in multiple panels
par(mfrow=c(3,2))
par(mar=c(2, 2, 2, 1), oma=c(0, 0, 0, 0))
for (or_der in 1:NCOL(pc_a$rotation)) {
  barplot(pc_a$rotation[, or_der], 
        las=3, xlab="", ylab="", main="")
  title(paste0("PC", or_der), line=-2.0, 
        col.main="red")
}  # end for
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_loadings_yield.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Yield Curve Principal Component Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The time series of the \emph{principal components} can be calculated by multiplying the loadings (weights) times the original data,
      \vskip1ex
      Higher order \emph{principal components} are gradually less volatile,
      <<echo=TRUE,eval=FALSE>>=
# principal component time series
pca_ts <- xts(re_turns %*% pc_a$rotation, 
                order.by=index(re_turns))
pca_ts <- cumsum(pca_ts)
# plot principal component time series in multiple panels
par(mfrow=c(3,2))
par(mar=c(2, 2, 0, 1), oma=c(0, 0, 0, 0))
ra_nge <- range(pca_ts)
for (or_der in 1:NCOL(pca_ts)) {
  plot.zoo(pca_ts[, or_der], 
           ylim=ra_nge, 
           xlab="", ylab="")
  title(paste0("PC", or_der), line=-2.0)
}  # end for
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_series_yield.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Recursive Data Objects}


%%%%%%%%%%%%%%%
\subsection{Lists}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      Lists are a type of vector that contain elements of different \emph{types},
      \vskip1ex
      Lists are recursive object types, meaning each list element can contain other vectors or lists,
      \vskip1ex
      The function \texttt{list()} creates a list from a list of vectors,
      \vskip1ex
      \texttt{list()} creates a named list from a list of symbol-value pairs,
      \vskip1ex
      The function \texttt{is.list()} returns \texttt{TRUE} if its argument is a list, and \texttt{FALSE} otherwise,
      \vskip1ex
      The function \texttt{unlist()} flattens a list into a vector that contains the atomic elements of the list (which typically causes coercion),
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# create a list with two elements
list_var <- list(c('a', 'b'), 1:4)
list_var
c(typeof(list_var), mode(list_var), class(list_var))
# lists are also vectors
c(is.vector(list_var), is.list(list_var))
NROW(list_var)
# create named list
list_var <- list(first=c('a', 'b'), second=1:4)
list_var
names(list_var)
unlist(list_var)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\begin{frame}[fragile,t]{Subsetting \subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      Lists can be subset (indexed) using:
      \begin{itemize}
        \item the \texttt{"["} operator (returns sublist),
        \item the \texttt{"[["} operator (returns an element),
        \item the \texttt{"\$"} operator (for named lists only),
      \end{itemize}
      \vskip1ex
      Partial name matching allows subsetting with partial name, as long as it can be resolved,
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
list_var[2]  # extract second element as sublist
list_var[[2]]  # extract second element
list_var[[2]][3]  # extract third element of second element
list_var[[c(2, 3)]]  # third element of second element
list_var$second  # extract second element
list_var$s  # extract second element - partial name matching
list_var$second[3]  # third element of second element
list_var <- list()  # empty list
list_var$a <- 1
list_var[2] <- 2
list_var
names(list_var)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\begin{frame}[fragile,t]{Coercing Vectors Into \subsecname \hskip0.5em Using \texttt{as.list()}}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The function \texttt{as.list()} coerces vectors and other objects into lists,
      \vskip1ex
      \texttt{as.list()} returns a list with the same elements as the vector,
      \vskip1ex
      \texttt{list()} called on a vector returns a single element equal to the vector,
      \vskip1ex
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
as.list(c(1,2,3))
list(c(1,2,3))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Data Frames}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      Data frames are \texttt{2-D} objects (like matrices), but their columns can be of different \emph{types},
      \vskip1ex
      Data frames can be thought of as lists of vectors of the same length,
      \vskip1ex
      The function \texttt{data.frame()} creates a \emph{data frame} from vectors assigned to column names,
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
data_frame <- data.frame(  # create a data frame
                      type=c('rose', 'daisy', 'tulip'),
                      color=c('red', 'white', 'yellow'),
                      price=c(1.5, 0.5, 1.0)
                    )  # end data.frame
data_frame
dim(data_frame)  # get dimension attribute
colnames(data_frame)  # get the colnames attribute
rownames(data_frame)  # get the rownames attribute
class(data_frame)  # get object class
typeof(data_frame)  # data frames are lists
is.data.frame(data_frame)

class(data_frame$type)  # get column class
class(data_frame$price)  # get column class
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\begin{frame}[fragile,t]{Subsetting \subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      Data frames can be subset in a similar way to lists and matrices,
      \vskip1ex
      Depending on how a data frame is subset, the result can be either a data frame or a vector,
      \vskip1ex
      Extracting a single column from a data frame produces a vector,
      \vskip1ex
      The data frame class attribute can be preserved by using the parameter \texttt{"drop=FALSE"},
      \vskip1ex
      Extracting a single row from a data frame produces a data frame,
      \vskip1ex
      The function \texttt{unlist()} applied to a single row extracted from a data frame coerces it to a vector,
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
data_frame[, 3]  # extract third column as vector
data_frame[[3]]  # extract third column as vector
data_frame[3]  # extract third column as data frame
data_frame[, 3, drop=FALSE]  # extract third column as data frame
data_frame[[3]][2]  # second element from third column
data_frame$price[2]  # second element from 'price' column
is.data.frame(data_frame[[3]]); is.vector(data_frame[[3]])
data_frame[2, ]  # extract second row
data_frame[2, ][3]  # third element from second column
data_frame[2, 3]  # third element from second column
unlist(data_frame[2, ])  # coerce to vector
is.data.frame(data_frame[2, ]); is.vector(data_frame[2, ])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\begin{frame}[fragile,t]{\subsecname \hskip0.5em and Factors}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      By default \texttt{data.frame()} coerces \texttt{character} vectors to \texttt{factors}, unless the \texttt{stringsAsFactors=FALSE} option is passed into \texttt{data.frame()},
      \vskip1ex
      The function \texttt{options()} sets global \emph{options}, that determine how \texttt{R} computes and displays its results,
      \vskip1ex
      If the global \texttt{option} \texttt{stringsAsFactors=FALSE} is set, then \texttt{character} vectors will not be coerced to \texttt{factors} in all subsequent \texttt{data frame} operations,
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
data_frame <- data.frame(  # create a data frame
                      type=c('rose', 'daisy', 'tulip'),
                      color=c('red', 'white', 'yellow'),
                      price=c(1.5, 0.5, 1.0),
                      row.names=c('flower1', 'flower2', 'flower3'),
                      stringsAsFactors=FALSE
                    )  # end data.frame
data_frame
class(data_frame$type)  # get column class
class(data_frame$price)  # get column class
# set option to not coerce character vectors to factors
options(stringsAsFactors=FALSE)
options("stringsAsFactors")
default.stringsAsFactors()
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\begin{frame}[fragile,t]{Exploring \subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{str()} displays the structure of an \texttt{R} object,
      \vskip1ex
      The functions \texttt{head()} and \texttt{tail()} display the first and last rows of an \texttt{R} object,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
str(data_frame)  # display the object structure
dim(cars)  # the cars data frame has 50 rows
head(cars, n=5)  # get first five rows
tail(cars, n=5)  # get last five rows
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\begin{frame}[fragile,t]{Sorting Vectors and \subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{sort()} returns a vector sorted into ascending order,
      \vskip1ex
      A permutation is a re-ordering of the elements of a vector, 
      \vskip1ex
      The permutation index specifies how the elements are re-ordered in a permutation,
      \vskip1ex
      The function \texttt{order()} calculates the permutation index to sort a given vector into ascending order,
      \vskip1ex
      Applying the function \texttt{order()} twice: \texttt{order(order())}, calculates the permutation index to sort the vector from ascending order into its unsorted (original) order,
      \vskip1ex
      So the permutation index produced by: \texttt{order(order())} is the reverse of the permutation index produced by: \texttt{order()},
      \vskip1ex
      \texttt{order()} can take several vectors as input, to break any ties,
      \vskip1ex
      Data frames can be sorted on any column,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# create a named vector
stu_dents <- sample(round(runif(5, min=1, max=10), digits=2))
names(stu_dents) <- c("Angie", "Chris", "Suzie", "Matt", "Liz")
# sort the vector into ascending order
sort(stu_dents)
# calculate index to sort into ascending order
order(stu_dents)
# sort the vector into ascending order
stu_dents[order(stu_dents)]
# calculate the sorted (ordered) vector
sort_ed <- stu_dents[order(stu_dents)]
# calculate index to sort into unsorted (original) order
order(order(stu_dents))
sort_ed[order(order(stu_dents))]
stu_dents
# create a data frame of stu_dents and their ranks
ra_nks <- c("first", "second", "third", "fourth", "fifth")
data.frame(students=stu_dents, rank=ra_nks[order(order(stu_dents))])
# permute data_frame of flowers on price column
order(data_frame$price)
# sort data_frame on price
data_frame[order(data_frame$price), ]
# sort data_frame on color
data_frame[order(data_frame$color), ]
order(c(2, 1:4))  # there's a tie
order(c(2, 1:4), 1:5)  # there's a tie
# read the Examples for sort()
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\begin{frame}[fragile,t]{Coercing \subsecname \hskip0.5em Into Matrices Using \texttt{as.matrix()}}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The function \texttt{as.matrix()} coerces vectors and data frames into matrices,
      \vskip1ex
      Coercing a data frame into a matrix causes coercion of \texttt{numeric} values into \texttt{character},
      \vskip1ex
      \texttt{as.matrix()} coerces vectors into single column matrices, as opposed to \texttt{matrix()}, which produces a matrix,
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
as.matrix(data_frame)
vec_tor <- sample(9)
matrix(vec_tor, ncol=3)
as.matrix(vec_tor, ncol=3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\begin{frame}[fragile,t]{Coercing Matrices Into \subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The generic function \texttt{as.data.frame()} coerces matrices and other objects into data frames,
      \vskip1ex
      The method \texttt{as.data.frame.matrix()} coerces only matrices into data frames,
      \vskip1ex
      \texttt{as.data.frame.matrix()} is about \texttt{50\%} faster than \texttt{as.data.frame()}, because it skips extra \texttt{R} code in \texttt{as.data.frame()} needed for argument validation, error checking, and method dispatch,
      \vskip1ex
      As a general rule, calling generic functions is slower than directly calling individual methods, because generic functions must execute extra \texttt{R} code for method dispatch,
      \vskip1ex
      The function \texttt{data.frame()} can also be used to coerce matrices into data frames, but is much slower than even \texttt{as.data.frame()},
      \vskip1ex
      \texttt{as.data.frame()} is about three times faster than \texttt{data.frame()}, because it doesn't require extra \texttt{R} code in \texttt{data.frame()} needed for handling different types of vectors, and for method dispatch,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:3))>>=
mat_rix <- matrix(5:10, nrow=2, ncol=3)  # create a matrix
rownames(mat_rix) <- c("row1", "row2")  # rownames attribute
colnames(mat_rix) <- c("col1", "col2", "col3")  # colnames attribute
library(microbenchmark)
# call method instead of generic function
as.data.frame.matrix(mat_rix)
# a few methods for generic function as.data.frame()
sample(methods(as.data.frame), size=4)
# function method is faster than generic function
summary(microbenchmark(
  as_data_frame_matrix=
    as.data.frame.matrix(mat_rix),
  as_data_frame=as.data.frame(mat_rix),
  data_frame=data.frame(mat_rix),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\begin{frame}[fragile,t]{Coercing Matrices Into Lists}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Matrices can be coerced into lists in at least two different ways,
      \vskip1ex
      Matrices can be first coerced into a data frame, and then into a list using function \texttt{as.list()},
      \vskip1ex
      Matrices can be directly coerced into a list using function \texttt{lapply()},
      \vskip1ex
      Using \texttt{lapply()} is the faster of the two methods, because \texttt{lapply()} is a \emph{compiled} function,
    \column{0.5\textwidth}
      <<echo=(-(1:1)),eval=FALSE>>=
library(microbenchmark)
# lapply is faster than coercion function
summary(microbenchmark(
  as_list=
    as.list(as.data.frame.matrix(mat_rix)),
  l_apply=
    lapply(seq_along(mat_rix[1, ]),
           function(in_dex) mat_rix[, in_dex]),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \texttt{iris} Data Frame}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The \texttt{iris} data frame is included in the \texttt{datasets} base package,
      \vskip1ex
      \texttt{iris} contains sepal and petal dimensions of 50 flowers from 3 species of iris,
      \vskip1ex
      The function \texttt{unique()} extracts unique elements of an object,
      \vskip1ex
      \texttt{sapply()} applies a function to a list or a vector of objects and returns a vector,
      \vskip1ex
      \texttt{sapply()} performs a loop over the list of objects, and can replace \texttt{"for"} loops in \texttt{R},
    \column{0.6\textwidth}
        <<echo=TRUE,eval=FALSE>>=
# ?iris  # get information on iris
dim(iris)
head(iris, 2)
colnames(iris)
unique(iris$Species)  # list of unique elements of iris
class(unique(iris$Species))
# find which columns of iris are numeric
sapply(iris, is.numeric)
# calculate means of iris columns
sapply(iris, mean)  # returns NA for Species
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \texttt{mtcars} Data Frame}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The \texttt{mtcars} data frame is included in the \texttt{datasets} base package, and contains design and performance data for 32 automobiles,
      \vskip1ex
    \column{0.6\textwidth}
        <<echo=TRUE,eval=FALSE>>=
# ?mtcars  # mtcars data from 1974 Motor Trend magazine
# mpg   Miles/(US) gallon
# qsec   1/4 mile time
# hp	 Gross horsepower
# wt	 Weight (lb/1000)
# cyl   Number of cylinders
dim(mtcars)
head(mtcars, 2)
colnames(mtcars)
head(rownames(mtcars), 3)
unique(mtcars$cyl)  # extract list of car cylinders
sapply(mtcars, mean)  # calculate means of mtcars columns
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \texttt{Cars93} Data Frame}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{Cars93} data frame is included in the \texttt{MASS} package, and contains design and performance data for 93 automobiles,
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data invisibly,
      \vskip1ex
      The parameter \texttt{breaks} is the number of cells of the histogram,
      \vskip1ex
      \texttt{"FD"} stands for the Freedman-Diaconis rule for calculating histogram breaks,
        <<Cars93_hist,fig.show='hide'>>=
library(MASS)
# ?Cars93  # get information on Cars93
dim(Cars93)
head(colnames(Cars93))
# head(Cars93, 2)
unique(Cars93$Type)  # extract list of car types
# sapply(Cars93, mean)  # calculate means of Cars93 columns
# plot histogram of Highway MPG using the Freedman-Diaconis rule
hist(Cars93$MPG.highway, col="lightblue1",
     main="Distance per Gallon 1993", xlab="Highway MPG", breaks="FD")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/Cars93_hist-1}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Data Management and Analysis}


%%%%%%%%%%%%%%%
\subsection{Bad Data}
\begin{frame}[fragile,t]{Types of \subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Possible sources of bad data are: imported data, class coercion, numeric overflow,
      \vskip1ex
      Types of bad data:
      \begin{itemize}
        \item \texttt{NA} (not available) is a logical constant indicating missing data,
        \item \texttt{NaN} means Not a Number data,
        \item \texttt{Inf} means numeric overflow - divide by zero,
      \end{itemize}
      \vskip1ex
      When a function produces \texttt{NA} or \texttt{NaN} values, then it also produces a \emph{warning} condition, but not an \emph{error},
      \vskip1ex
      \texttt{NA} or \texttt{NaN} values are not \emph{errors},
      \vskip1ex
      The functions \texttt{is.na()} and \texttt{is.nan()} test for \texttt{NA} and \texttt{NaN} values,
      \vskip1ex
      Many functions have a \texttt{na.rm} parameter to remove \texttt{NAs} from input data,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-1),eval=FALSE>>=
rm(list=ls())
as.numeric(c(1:3, "a"))  # NA from coercion
0/0  # NaN from ambiguous math
1/0  # Inf from divide by zero
is.na(c(NA, NaN, 0/0, 1/0))  # test for NA
is.nan(c(NA, NaN, 0/0, 1/0))  # test for NaN
NA*1:4  # create vector of Nas
# create vector with some NA values
da_ta <- c(1, 2, NA, 4, NA, 5)
da_ta
mean(da_ta)  # returns NA, when NAs are input
mean(da_ta, na.rm=TRUE)  # remove NAs from input data
da_ta[!is.na(da_ta)]  # delete the NA values
sum(!is.na(da_ta))  # count non-NA values
      @
  \end{columns}
\end{block}

\end{frame}


\begin{frame}[fragile,t]{Scrubbing \subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{complete.cases()} returns \texttt{TRUE} if a row has no \texttt{NA} values.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-1),eval=FALSE>>=
rm(list=ls())
# airquality data has some NAs
head(airquality)
dim(airquality)
# number of NAs
sum(!complete.cases(airquality))
# display rows containing NAs
head(airquality[!complete.cases(airquality), ])
      @
  \end{columns}
\end{block}

\end{frame}


\begin{frame}[fragile,t]{Scrubbing \subsecname \hskip0.5em (cont.)}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Rows containing bad data may be either removed or replaced with an estimated value.
      \vskip1ex
      The function \texttt{na.locf()} from package \emph{zoo} replaces NAs with most recent non-NA prior to it.
      \vskip1ex
      The function \texttt{na.locf.xts()} from package \emph{xts} is faster than \texttt{zoo::na.locf()}, but it only operates on time series of class \texttt{"xts"}.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-1),eval=FALSE>>=
rm(list=ls())
# remove rows containing NAs
good_air <- airquality[complete.cases(airquality), ]
dim(good_air)
head(good_air)  # NAs removed
library(zoo)  # load package zoo
# Replace NAs
good_air <- zoo::na.locf(airquality)
dim(good_air)
head(good_air)  # NAs replaced
# Create vector containing NA values
vec_tor <- sample(22)
vec_tor[sample(NROW(vec_tor), 4)] <- NA
# Replace NA values with the most recent non-NA values
zoo::na.locf(vec_tor)
# Replace NAs in xts time series
se_ries <- rutils::etf_env$price_s[, 1]
head(se_ries)
sum(is.na(se_ries))
library(quantmod)
series_zoo <- as.xts(zoo::na.locf(se_ries, fromLast=TRUE))
series_xts <- xts:::na.locf.xts(se_ries, fromLast=TRUE)
all.equal(series_zoo, series_xts, check.attributes=FALSE)
library(microbenchmark)
summary(microbenchmark(
  zoo=as.xts(zoo::na.locf(se_ries, fromLast=TRUE)),
  xts=xts:::na.locf.xts(se_ries, fromLast=TRUE),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{NULL} Values}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{NULL} represents a null object, and is a legitimate value, not bad data,
      \vskip1ex
      \texttt{NULL} is often returned by functions whose value is undefined,
      \vskip1ex
      \texttt{NULL} can also be used to initialize vectors,
      \vskip1ex
      \texttt{NULL} is not the same as \texttt{NA} values or zero-length (empty) vectors,
      \vskip1ex
      The functions \texttt{numeric()} and \texttt{character()} return empty (zero-length) vectors of the specified \emph{type},
      \vskip1ex
      The function \texttt{is.null()} tests for \texttt{NULL} values,
      \vskip1ex
      Very often variables are initialized to \texttt{NULL} before the start of iteration,
      \vskip1ex
      A more efficient way to perform iteration is by pre-allocating the vector,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# NULL values have no mode or type
c(mode(NULL), mode(NA))
c(typeof(NULL), typeof(NA))
c(length(NULL), length(NA))
# check for NULL values
is.null(NULL)
# NULL values are ignored when combined into a vector
c(1, 2, NULL, 4, 5)
# But NA value isn't ignored
c(1, 2, NA, 4, 5)
# vectors can be initialized to NULL
vec_tor <- NULL
is.null(vec_tor)
# grow the vector in a loop - very bad code!!!
for (in_dex in 1:5)
  vec_tor <- c(vec_tor, in_dex)
# initialize empty vector
vec_tor <- numeric()
# grow the vector in a loop - very bad code!!!
for (in_dex in 1:5)
  vec_tor <- c(vec_tor, in_dex)
# allocate vector
vec_tor <- numeric(5)
# assign to vector in a loop - good code
for (in_dex in 1:5)
  vec_tor[in_dex] <- runif(1)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Reading and Writing Data from Files}


%%%%%%%%%%%%%%%
\subsection{Writing Text Strings}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{cat()} concatenates strings and writes them to standard output or to files. 
      \vskip1ex
      \texttt{cat()} interprets its argument character string and its escape sequences (\texttt{"\textbackslash{}"}), but doesn't return a value,
      \vskip1ex
      The function \texttt{print()} doesn't interpret its argument, and simply prints it to standard output and invisibly returns it,
      \vskip1ex
      Typing the name of an object in \texttt{R} implicitly calls \texttt{print()} on that object,
      \vskip1ex
      The function \texttt{save()} writes objects to a binary file,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
cat("Enter\ttab")  # cat() interprets backslash escape sequences
print("Enter\ttab")

my_text <- print("hello")
my_text  # print() returns its argument

# create string
my_text <- "Title: My Text\nSome numbers: 1,2,3,...\nRprofile files contain code executed at R startup,\n"

cat(my_text, file="mytext.txt")  # write to text file

cat("Title: My Text",  # write several lines to text file
    "Some numbers: 1,2,3,...",
    "Rprofile files contain code executed at R startup,", 
    file="mytext.txt", sep="\n")

save(my_text, file="mytext.RData")  # write to binary file
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Displaying Numeric Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{print()} displays numeric data objects, with the number of digits given by the global option \texttt{"digits"}, 
      \vskip1ex
      The function \texttt{sprintf()} returns strings formatted from text strings and numeric data, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
print(pi)
print(pi, digits=10)
getOption("digits")
foo <- 12
bar <- "months"
sprintf("There are %i %s in the year", foo, bar)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reading Text from Files}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{scan()} reads text or data from a file and returns it as a vector or a list,
      \vskip1ex
      The function \texttt{readLines()} reads lines of text from a connection (file or console), and returns them as a vector of \texttt{character} strings,
      \vskip1ex
      The function \texttt{readline()} reads a single line from the console, and returns it as a \texttt{character} string,
      \vskip1ex
      The function \texttt{file.show()} reads text or data from a file and displays in editor,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# read text from file
scan(file="mytext.txt", what=character(), sep="\n")

# read lines from file
readLines(con="mytext.txt")

# read text from console
in_put <- readline("Enter a number: ")
class(in_put)
# coerce to numeric
in_put <- as.numeric(in_put)

# read text from file and display in editor:
# file.show("mytext.txt")
# file.show("mytext.txt", pager="")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reading and Writing Data Frames from \protect\emph{Text} Files}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functions \texttt{read.table()} and \texttt{write.table()} read and write data frames from text files. 
      \vskip1ex
      \texttt{write.table()} coerces objects to data frames before it writes them,
      \vskip1ex
      \texttt{read.table()} returns a data frame, and coerces non-numeric values to \texttt{factors} (unless the \texttt{stringsAsFactors=FALSE} option is set),
      \vskip1ex
      \texttt{read.table()} and \texttt{write.table()} can be used to read and write matrices from text files, but they have to be coerced back to matrices,
      \vskip1ex
      \texttt{read.table()} and \texttt{write.table()} are inefficient for very large data sets,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:4)),eval=FALSE>>=
setwd("C:/Develop/R/lecture_slides/data")
data_frame <- data.frame(type=c("rose", "daisy", "tulip"), color=c("red", "white", "yellow"), price=c(1.5, 0.5, 1.0), row.names=c("flower1", "flower2", "flower3"))  # end data.frame
mat_rix <- matrix(sample(1:12), ncol=3, dimnames=list(NULL, c("col1", "col2", "col3")))
rownames(mat_rix) <- paste("row", 1:NROW(mat_rix), sep="")
# write data frame to text file, and then read it back
write.table(data_frame, file="florist.txt")
data_read <- read.table(file="florist.txt")
data_read  # a data frame

# write matrix to text file, and then read it back
write.table(mat_rix, file="matrix.txt")
mat_read <- read.table(file="matrix.txt")
mat_read  # write.table() coerced matrix to data frame
class(mat_read)
# coerce from data frame back to matrix
mat_read <- as.matrix(mat_read)
class(mat_read)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Copying Data Frames Between the \protect\emph{clipboard} and \texttt{R}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Data frames stored in the \emph{clipboard} can be copied into \texttt{R} using the function \texttt{read.table()},
      \vskip1ex
      Data frames in \texttt{R} can be copied into the \emph{clipboard} using the function \texttt{write.table()},
      \vskip1ex
      This allows convenient copying of data frames between \texttt{R} and \emph{Excel},
      \vskip1ex
      Data frames can also be manipulated directly in the \texttt{R} spreadsheet-style data editor,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-(1:3)),eval=FALSE>>=
setwd("C:/Develop/R/lecture_slides/data")
data_frame <- data.frame(small=c(3, 5), medium=c(9, 11), large=c(15, 13))
data_frame <- read.table("mydata.txt", header=TRUE)
data_frame <- read.table("clipboard", header=TRUE)

write.table(x=data_frame, file="clipboard", sep="\t")

# wrapper function for copying data frame from clipboard into R
# by default, data is tab delimited, with a header
read_clip <- function(file="clipboard", sep="\t", 
                      header=TRUE, ...) {
  read.table(file=file, sep=sep, header=header, ...)
}  # end read_clip

data_frame <- read_clip()

# wrapper function for copying data frame from R into clipboard
# by default, data is tab delimited, with a header
write_clip <- function(data, row.names=FALSE, 
                       col.names=TRUE, ...) {
  write.table(x=data, file="clipboard", sep="\t", 
              row.names=row.names, col.names=col.names, ...)
}  # end write_clip

write_clip(data=data_frame)

# launch spreadsheet-style data editor
data_frame <- edit(data_frame)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reading and Writing Data Frames From \protect\emph{csv} Files}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The easiest way to share data between \texttt{R} and \emph{Excel} is through writing and reading \emph{csv()} files. 
      \vskip1ex
      The functions \texttt{read.csv()} and \texttt{write.csv()} read and write data frames from \emph{csv} format files. 
      \vskip1ex
      The functions \texttt{read.csv()} and \texttt{write.csv()} read and write data frames from \emph{csv} format files. 
      \vskip1ex
      These functions are \emph{wrappers} for \texttt{read.table()} and \texttt{write.table()},
      \vskip1ex
      \texttt{read.csv()} coerces non-numeric values to \texttt{factors}, unless the \texttt{stringsAsFactors=FALSE} option is set,
      \vskip1ex
      \texttt{read.csv()} reads row names as an extra column, unless the \texttt{row.names=1} argument is used,
      \vskip1ex
      The argument \texttt{"row.names"} accepts either the number or the name of the column containing the row names,
      \vskip1ex
      The \texttt{*.csv()} functions are very inefficient for large data sets,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# write data frame to CSV file, and then read it back
write.csv(data_frame, file="florist.csv")
data_read <- read.csv(file="florist.csv", 
                       stringsAsFactors=FALSE)
data_read  # the row names are read in as extra column
# restore row names
rownames(data_read) <- data_read[, 1]
data_read <- data_read[, -1]  # remove extra column
data_read
# read data frame, with row names from first column
data_read <- read.csv(file="florist.csv", row.names=1)
data_read
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reading and Writing Data Frames From \protect\emph{csv} Files (cont.)}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functions \texttt{read.csv()} and \texttt{write.csv()} can read and write data frames from \emph{csv} format files \emph{without using row names},
      \vskip1ex
      Row names can be omitted from the output file by calling \texttt{write.csv()} with the argument \texttt{row.names=FALSE}, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# write data frame to CSV file, without row names
write.csv(data_frame, row.names=FALSE, file="florist.csv")
data_read <- read.csv(file="florist.csv")
data_read  # a data frame without row names
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reading and Writing Matrices From \protect\emph{csv} Files}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functions \texttt{read.csv()} and \texttt{write.csv()} can read and write matrices from \emph{csv} format files. 
      \vskip1ex
      If row names can be omitted in the output file, then \texttt{write.csv()} can be called with argument \texttt{row.names=FALSE}, 
      \vskip1ex
      If the input file doesn't contain row names, then \texttt{read.csv()} can be called without the \texttt{"row.names"} argument,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# write matrix to csv file, and then read it back
write.csv(mat_rix, file="matrix.csv")
mat_read <- read.csv(file="matrix.csv", row.names=1)
mat_read  # read.csv() reads matrix as data frame
class(mat_read)
mat_read <- as.matrix(mat_read)  # coerce to matrix
identical(mat_rix, mat_read)
write.csv(mat_rix, row.names=FALSE, 
          file="matrix_ex_rows.csv")
mat_read <- read.csv(file="matrix_ex_rows.csv")
mat_read <- as.matrix(mat_read)
mat_read  # a matrix without row names
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reading and Writing Matrices (cont.)}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      There are several ways of reading and writing matrices from \emph{csv} files, with tradeoffs between simplicity, data size, and speed.
      \vskip1ex
      The function \texttt{write.matrix()} writes a matrix to a text file, without its row names.
      \vskip1ex
      \texttt{write.matrix()} is part of package \emph{MASS}.
      \vskip1ex
      The advantage of function \texttt{scan()} is its speed, but it doesn't handle row names easily.
      \vskip1ex
      Removing row names simplifies the reading and writing of matrices.
      \vskip1ex
      The function \texttt{readLines} reads whole lines and returns them as single strings.
      \vskip1ex
      The function \texttt{system.time()} calculates the execution time (in seconds) used to evaluate a given expression.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
setwd("C:/Develop/R/lecture_slides/data")
library(MASS)  # load package "MASS"
# write to CSV file by row - it's very SLOW!!!
MASS::write.matrix(mat_rix, 
  file="matrix.csv", sep=",")
# read using scan() and skip first line with colnames
mat_read <- scan(file="matrix.csv", 
  sep=",", skip=1, what=numeric())
# read colnames
col_names <- readLines(con="matrix.csv", n=1)
col_names  # this is a string!
# convert to char vector
col_names <- strsplit(col_names, 
  s=",")[[1]]
mat_read  # mat_read is a vector, not matrix!
# coerce by row to matrix
mat_read <- matrix(mat_read, 
  ncol=NROW(col_names), byrow=TRUE)
# restore colnames
colnames(mat_read) <- col_names
mat_read
# scan() is a little faster than read.csv()
library(microbenchmark)
summary(microbenchmark(
  read_csv=read.csv("matrix.csv"),
  scan=scan(file="matrix.csv", sep=",", 
    skip=1, what=numeric()),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reading Matrices Containing Bad Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Very often data that is read from external sources contains elements with bad data.
      \vskip1ex
      An example of bad data are \texttt{character} strings in \texttt{numeric} data.
      \vskip1ex
      Columns of numeric data that contain strings are coerced to \texttt{character} or \texttt{factor}, when they're read by \texttt{read.csv()}.
      \vskip1ex
      The function \texttt{as.numeric()} coerces complex data objects into \texttt{numeric} vectors, and removes all their \emph{attributes}.
      \vskip1ex
      \texttt{as.numeric()} coerces strings that don't represent numbers into \texttt{NA} values.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# read data from a csv file, including row names
mat_rix <- read.csv(file="matrix_bad.csv", 
  row.names=1, stringsAsFactors=FALSE)
mat_rix
class(mat_rix)
# columns with bad data are character or factor
sapply(mat_rix, class)
# copy row names
row_names <- row.names(mat_rix)
# sapply loop over columns and coerce to numeric
mat_rix <- sapply(mat_rix, as.numeric)
# restore row names
row.names(mat_rix) <- row_names
# replace NAs with zero
mat_rix[is.na(mat_rix)] <- 0
# matrix without NAs
mat_rix
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reading and Writing Time Series From \protect\emph{Text} Files}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{zoo} contains functions \texttt{read.zoo()} and \texttt{write.zoo()} for reading and writing \emph{zoo} series from \emph{text} and \emph{csv} files. 
      \vskip1ex
      \texttt{read.zoo()} and \texttt{write.zoo()} are \emph{wrappers} for \texttt{read.table()} and \texttt{write.table()},
      \vskip1ex
      By default these functions read and write data in \emph{space}-delimited format, but they can also read and write data to \emph{comma}-delimited \emph{csv} files by passing the parameter \texttt{sep=","}, 
      \vskip1ex
      Users may also directly use \texttt{read.table()} and \texttt{write.table()}, instead of \texttt{read.zoo()} and \texttt{write.zoo()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:4)),eval=FALSE>>=
setwd("C:/Develop/R/lecture_slides/data")
rm(list=ls())
set.seed(1121)  # reset random number generator
library(zoo)  # load package zoo
# create zoo with Date index
in_dex <- seq(from=as.Date("2013-06-15"), 
                  by="day", length.out=100)
zoo_series <- zoo(cumsum(rnorm(NROW(in_dex))), 
                  order.by=in_dex)
tail(zoo_series, 3)
# write zoo series to text file, and then read it back
write.zoo(zoo_series, file="zoo_series.txt")
zoo_series_read <- read.zoo("zoo_series.txt")  # read it back
tail(zoo_series, 3)
all.equal(zoo_series_read, zoo_series)
# Perform the same using write.table() and read.table()
# first coerce zoo_series into data frame
data_frame <- as.data.frame(zoo_series)
data_frame <- cbind(in_dex, data_frame)
# write zoo_series to text file using write.table
write.table(data_frame, file="zoo_series.txt", 
            row.names=FALSE, col.names=FALSE)
# read data frame from file
zoo_series_read <- read.table(file="zoo_series.txt", 
                              stringsAsFactors=FALSE)
sapply(zoo_series_read, class)  # a data frame
# coerce data frame into zoo_series
zoo_series_read <- zoo::zoo(
  drop(as.matrix(zoo_series_read[, -1])),
  order.by=as.Date(zoo_series_read[, 1]))
all.equal(zoo_series_read, zoo_series)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reading and Writing Time Series With \protect\emph{Date-time} Index}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the index of a \emph{zoo} series is a \emph{date-time}, then \texttt{write.zoo()} writes the date and time fields as separate columns with a \emph{space} between them. 
      \vskip1ex
      To properly read separate date and time columns from \emph{text} files, \texttt{read.zoo()} must be passed arguments \texttt{"index.column=list(1,2)"} and \texttt{"tz="America/New\_York"}.
      \vskip1ex
      Users may also directly use \texttt{read.table()} and \texttt{write.table()}, instead of \texttt{read.zoo()} and \texttt{write.zoo()}.
      \vskip1ex
      The function \texttt{as.POSIXct()} coerces \texttt{character} strings into \texttt{POSIXct} \emph{date-time} objects.
      <<echo=(-(1:2)),eval=FALSE>>=
set.seed(1121)  # reset random number generator
library(zoo)  # load package zoo
# create zoo with POSIXct date-time index
in_dex <- seq(from=as.POSIXct("2013-06-15"), 
                  by="hour", length.out=1000)
zoo_series <- zoo(cumsum(rnorm(NROW(in_dex))), 
                  order.by=in_dex)
# write zoo series to text file, and then read it back
write.zoo(zoo_series, file="zoo_series.txt")
zoo_series_read <- read.zoo("zoo_series.txt")  # read it back
# time field was read as a separate column
tail(zoo_series_read, 3)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# read and specify that second column is time field
zoo_series_read <- read.zoo(file="zoo_series.txt", 
                       index.column=list(1,2), 
                       tz="America/New_York")
all.equal(zoo_series_read, zoo_series)
# Perform the same using write.table() and read.table()
# first coerce zoo_series into data frame
data_frame <- as.data.frame(zoo_series)
data_frame <- cbind(in_dex, data_frame)
# write zoo_series to text file using write.table
write.table(data_frame, file="zoo_series.txt", 
            row.names=FALSE, col.names=FALSE)
# read data frame from file
zoo_series_read <- read.table(file="zoo_series.txt", 
                              stringsAsFactors=FALSE)
sapply(zoo_series_read, class)  # a data frame
# paste first two date columns together and coerce into as.POSIXct
date_col <- 1:2
in_dex <- do.call(paste, zoo_series_read[, date_col])
in_dex <- as.POSIXct(in_dex, tz="America/New_York")
# coerce data frame into zoo_series
zoo_series_read <- zoo::zoo(
  drop(as.matrix(zoo_series_read[, -date_col])),
  order.by=in_dex)
all.equal(zoo_series_read, zoo_series)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reading and Writing Time Series From \protect\emph{csv} Files}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Single column \emph{zoo} time series usually don't have a dimension attribute, and they don't have a column name, unlike multi-column \emph{zoo} time series, and this can cause hard to detect bugs,
      \vskip1ex
      It's best to always pass the argument \texttt{"col.names=TRUE"} to the function \texttt{write.zoo()}, to make sure it writes a column name for a single column \emph{zoo} time series,
      \vskip1ex
      Reading a \emph{csv} file containing a single column of data using the function \texttt{read.zoo()} produces a \emph{zoo} time series with a \texttt{NULL} dimension, unless the argument \texttt{"drop=FALSE"} is passed to \texttt{read.zoo()},
      \vskip1ex
      Very often \emph{csv} files contain custom \emph{date-time} formats, which need to be passed as parameters into \texttt{read.zoo()} for proper formatting,
      \vskip1ex
      The \texttt{"FUN"} argument of \texttt{read.zoo()} accepts a function for coercing columns of the input data into a \emph{date-time} object suitable for the \emph{zoo} index,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(zoo)  # load package zoo
# write zoo series to CSV file, and then read it back
write.zoo(zoo_series, file="zoo_series.csv", 
          sep=",", col.names=TRUE)
zoo_series <- read.zoo(file="zoo_series.csv", 
                  header=TRUE, sep=",", 
                  drop=FALSE, 
                  FUN=as.POSIXct, tz="America/New_York")
tail(zoo_series, 3)
# read zoo from CSV file, with custom date-time format
zoo_frame <- read.table(file="zoo_series2.csv", 
                        sep=",")
tail(zoo_frame, 3)  # date-time format mm/dd/yyyy hh:mm
zoo_series <- read.zoo(file="zoo_series2.csv", 
                  header=TRUE, sep=",", 
                  drop=FALSE, 
                  FUN=as.POSIXct, 
                  tz="America/New_York",
                  format="%m/%d/%Y %H:%M")
tail(zoo_series, 3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reading Time Series With \texttt{Numeric} \protect\emph{Date-time} Index}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the index of a \emph{zoo} series is \texttt{numeric} (representing the \emph{moment of time}, either as the number of days or seconds), then it must be coerced to a proper \emph{date-time} class.
      \vskip1ex
      The function \texttt{as.POSIXct.numeric()} coerces a \texttt{numeric} value representing the \emph{moment of time} into a \texttt{POSIXct} \emph{date-time}, equal to the \emph{clock time} in the local \emph{time zone}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# read zoo from CSV file, with numeric date-time
zoo_series <- read.csv(file="es_ohlc.csv", 
  sep=",")
zoo_series <- zoo::zoo(zoo_series[, -1],
  order.by=as.POSIXct.numeric(zoo_series[, 1], 
    tz="America/New_York", 
    origin="1970-01-01"))
head(zoo_series, 11)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Passing Arguments to the \texttt{save()} Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{save()} writes objects to a binary file,
      \vskip1ex
      Object names can be passed into \texttt{save()} either through the \texttt{"..."} argument, or the \texttt{"list"} argument,
      \vskip1ex
      Objects passed through the \texttt{"..."} argument are not evaluated, so they must be either object names or character strings,
      \vskip1ex
      Object names aren't surrounded by quotes \texttt{""}, while character strings that represent object names are surrounded by quotes \texttt{""},
      \vskip1ex
      Objects passed through the \texttt{"list"} argument are evaluated, so they may be variables containing character strings,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-1)>>=
rm(list=ls())  # remove all objects
var1 <- 1; var2 <- 2
ls()  # list all objects
ls()[1]  # list first object
args(save)  # list arguments of save function
# save "var1" to a binary file using string argument
save("var1", file="my_data.RData")
# save "var1" to a binary file using object name
save(var1, file="my_data.RData")
# save multiple objects
save(var1, var2, file="my_data.RData")
# save first object in list by passing to "..." argument
# ls()[1] is not evaluated
save(ls()[1], file="my_data.RData")
# save first object in list by passing to "list" argument
save(list=ls()[1], file="my_data.RData")
# save whole list by passing it to the "list" argument
save(list=ls(), file="my_data.RData")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reading and Writing Lists of Objects}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{load()} reads data from \texttt{*.RData} files, and \emph{invisibly} returns a vector of names of objects created in the workspace,
      \vskip1ex
      The vector of names can be used to manipulate the objects in loops, or to pass them to functions,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<>>=
rm(list=ls())  # remove all objects
# load objects from file
load_ed <- load(file="my_data.RData")
load_ed  # vector of loaded objects
ls()  # list objects
# assign new values to objects in  global environment
sapply(load_ed, function(sym_bol) {
  assign(sym_bol, runif(1), envir=globalenv())
})  # end sapply
ls()  # list objects
# assign new values to objects using for loop
for (sym_bol in load_ed) {
  assign(sym_bol, runif(1))
}  # end for
ls()  # list objects
# save vector of objects
save(list=load_ed, file="my_data.RData")
# remove only loaded objects
rm(list=load_ed)
# remove the object "load_ed"
rm(load_ed)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Saving Output of \texttt{R} to a File}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{sink()} diverts \texttt{R} \emph{text} output (excluding graphics) to a file, or ends the diversion,
      \vskip1ex
      Remember to call \texttt{sink()} to end the diversion!
      \vskip1ex
      The function \texttt{pdf()} diverts graphics output to a \emph{pdf} file (text output isn't diverted), in vector graphics format,
      \vskip1ex
      The functions \texttt{png()}, \texttt{jpeg()}, \texttt{bmp()}, and \texttt{tiff()} divert graphics output to graphics files (text output isn't diverted), 
      \vskip1ex
      The function \texttt{dev.off()} ends the diversion,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
sink("sinkdata.txt")# redirect text output to file

cat("Redirect text output from R\n")
print(runif(10))
cat("\nEnd data\nbye\n")

sink()  # turn redirect off

pdf("Rgraph.pdf", width=7, height=4)  # redirect graphics to pdf file

cat("Redirect data from R into pdf file\n")
my_var <- seq(-2*pi, 2*pi, len=100)
plot(x=my_var, y=sin(my_var), main="Sine wave", 
   xlab="", ylab="", type="l", lwd=2, col="red")
cat("\nEnd data\nbye\n")

dev.off()  # turn pdf output off

png("r_plot.png")  # redirect graphics output to png file

cat("Redirect graphics from R into png file\n")
plot(x=my_var, y=sin(my_var), main="Sine wave", 
 xlab="", ylab="", type="l", lwd=2, col="red")
cat("\nEnd data\nbye\n")

dev.off()  # turn png output off
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Homework Assignment}


%%%%%%%%%%%%%%%
\subsection{Homework Assignment}
\begin{frame}[t]{\subsecname}
\vspace{-1em}
\begin{block}{Required}
  \begin{itemize}[]
    \item Read all the lecture slides in \emph{FRE6871\_Lecture\_5.pdf}, and run all the code in \emph{FRE6871\_Lecture\_5.R}
  \end{itemize}
\end{block}
\begin{block}{Recommended}
  \begin{itemize}[]
    \item Read about \emph{PCA} in:\\
    \emph{pca-handout.pdf}\\
    \emph{pcaTutorial.pdf}\\
    \item Read about \emph{optimization methods}:\\
    \emph{Bolker Optimization Methods.pdf}\\
    \emph{Yollin Optimization.pdf}\\
    \emph{Boudt DEoptim Large Portfolio Optimization.pdf}\\
  \end{itemize}
\end{block}

\end{frame}


\end{document}
