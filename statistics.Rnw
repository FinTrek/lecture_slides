% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='scriptsize', fig.width=4, fig.height=4)
options(digits=3)
options(width=60, dev='pdf')
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
% bbold package for unitary vector or matrix symbol
\usepackage{bbold}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape,bg=red,fg=red}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[Probability and Statistics]{Probability and Statistics}
\subtitle{FRE6871 \& FRE7241, Spring 2017}
% \subject{Getting Started With R}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@poly.edu}
% \date{January 27, 2014}
\date{\today}
% \pgfdeclareimage[height=0.5cm]{university-logo}{engineering_long_white}
% \logo{\pgfuseimage{engineering_long_white}}


%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Probability and Statistics}


%%%%%%%%%%%%%%%
\subsection{Pseudo-Random Numbers}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Pseudo-random numbers are deterministic sequences of numbers which have some of the properties of random numbers, but they are not truly random numbers, 
      \vskip1ex
      Pseudo-random number generators depend on a \emph{seed} value, and produce the same sequence of numbers for a given \emph{seed} value, 
      \vskip1ex
      The function \texttt{set.seed()} initializes the random number generator by specifying the \emph{seed} value,
      \vskip1ex
      The choice of \emph{seed} value isn't important, and a given value is just good as any other one, 
      \vskip1ex
      The function \texttt{runif()} produces random numbers from the \emph{uniform} distribution,
      \vskip1ex
      The function \texttt{rnorm()} produces random numbers from the \emph{normal} distribution,
      \vskip1ex
      The function \texttt{dnorm()} calculates the normal probability density,
      \vskip1ex
      The function \texttt{pnorm()} calculates the cumulative \emph{normal} distribution,
      \vskip1ex
      The function \texttt{qnorm()} calculates the inverse cumulative \emph{normal} distribution,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # reset random number generator
runif(3)  # three numbers from uniform distribution
runif(3)  # produce another three numbers
set.seed(1121)  # reset random number generator
runif(3)  # produce another three numbers

# produce random number from standard normal distribution
rnorm(1)
# produce five random numbers from standard normal distribution
rnorm(5)
# produce five random numbers from the normal distribution
rnorm(n=5, mean=1, sd=2)  # match arguments by name
# calculate cumulative standard normal distribution
c(pnorm(-2), pnorm(2))
# calculate inverse cumulative standard normal distribution
c(qnorm(0.75), qnorm(0.25))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Logistic Map}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{logistic map} is a recurrence relation which produces a deterministic sequence of numbers:
      \begin{displaymath}
        x_{n+1} = r x_n (1 - x_n)
      \end{displaymath}
      If the \emph{seed} value $x_0$ is in the interval $(0, 1)$ and if $r=4$, then the sequence $x_n$ is also contained in the interval $(0, 1)$, 
      \vskip1ex
      The function \texttt{curve()} plots a function defined by its name,
      <<echo=TRUE,eval=FALSE>>=
# define logistic map function
log_map <- function(x, r=4) r*x*(1-x)
log_map(0.25, 4)
# plot logistic map
x11(width=6, height=5)
curve(expr=log_map, type="l", xlim=c(0, 1),
      xlab="x[n-1]", ylab="x[n]", lwd=2, col="blue", 
      main="logistic map")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/logistic_map.png}\\
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generating Pseudo-Random Numbers Using Logistic Map}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{logistic map} can be used to calculate sequences of pseudo-random numbers,
      \vskip1ex
      For most \emph{seed} values $x_0$ and $r=4$, the \emph{logistic map} produces a pseudo-random sequence, but it's not uniformly distributed, 
      \vskip1ex
      The inverse cosine function \texttt{acos()} transforms a \emph{logistic map} sequence into a uniformly distributed sequence, 
      \begin{displaymath}
        u_n = \arccos(1 - 2 x_n) / \pi
      \end{displaymath}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# calculate uniformly distributed pseudo-random
# sequence using logistic map function
uni_form <- function(see_d, len_gth=10) {
  # pre-allocate vector instead of "growing" it
  out_put <- numeric(len_gth)
  # initialize
  out_put[1] <- see_d
  # perform loop
  for (i in 2:len_gth) {
    out_put[i] <- 4*out_put[i-1]*(1-out_put[i-1])
  }  # end for
  acos(1-2*out_put)/pi
}  # end uni_form
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/logistic_map_density.png}\\
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
uni_form(see_d=0.1, len_gth=15)
plot(
  density(uni_form(see_d=runif(1), len_gth=1e5)), 
  xlab="", ylab="", lwd=2, col="blue", 
  main="uniform pseudo-random number density")
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generating Binomial Random Numbers}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      A \emph{binomial} trial is a coin flip, that results in either a success or failure,
      \vskip1ex
      The \emph{binomial} distribution specifies the probability of obtaining a certain number of successes in a sequence of independent \emph{binomial} trials,
      \vskip1ex
      Let $p$ be the probability of obtaining a success in a \emph{binomial} trial, and let $(1-p)$ be the probability of failure,
      \vskip1ex
      $p = 0.5$ corresponds to flipping an unbiased coin,
      \vskip1ex
      The probability of obtaining $k$ successes in $n$ independent \emph{binomial} trials is equal to:
      \begin{displaymath}
        {n \choose k} p^k (1-p)^{(n-k)}
      \end{displaymath}
      The function \texttt{rbinom()} produces random numbers from the \emph{binomial} distribution,
    \column{0.6\textwidth}
      \vspace{-1em}
        <<>>=
set.seed(1121)  # reset random number generator
# flip unbiased coin once, 20 times
rbinom(n=20, size=1, 0.5)
# number of heads after flipping twice, 20 times
rbinom(n=20, size=2, 0.5)
# number of heads after flipping thrice, 20 times
rbinom(n=20, size=3, 0.5)
# number of heads after flipping biased coin thrice, 20 times
rbinom(n=20, size=3, 0.8)
# number of heads after flipping biased coin thrice, 20 times
rbinom(n=20, size=3, 0.2)
# flip unbiased coin once, 20 times
sample(x=0:1, size=20, replace=TRUE)  # fast
as.numeric(runif(20) < 0.5)  # slower
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generating Random Samples and Permutations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{sample} is a subset of elements taken from a set of data elements,
      \vskip1ex
      The function \texttt{sample()} produces a random sample form a vector of data elements,
      \vskip1ex
      By default the \emph{size} of the sample (the \texttt{size} argument) is equal to the number of elements in the data vector,
      \vskip1ex
      So the call \texttt{sample(da\_ta)} produces a random permutation of all the elements of \texttt{da\_ta},
      \vskip1ex
      If \texttt{replace=TRUE}, then \texttt{sample()} produces samples with replacement,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<>>=
# permutation of five numbers
sample(x=5)
# permutation of four strings
sample(x=c("apple", "grape", "orange", "peach"))
# sample of size three
sample(x=5, size=3)
# sample with replacement
sample(x=5, replace=TRUE)
sample(  # sample of strings
  x=c("apple", "grape", "orange", "peach"),
  size=12,
  replace=TRUE)
# binomial sample: flip coin once, 20 times
sample(x=0:1, size=20, replace=TRUE)
# flip unbiased coin once, 20 times
as.numeric(runif(20) > 0.5)  # slower
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Statistical Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A data \emph{sample} is a set of observations of a \emph{random variable},
      \vskip1ex
      Let $\{x_1,\ldots ,x_n\}$ be a data \emph{sample} of a \emph{random variable} \texttt{x},
      \vskip1ex
      Let \texttt{x} follow a probability distribution with population mean equal to $\mu$ and population standard deviation equal to $\sigma$,
      \vskip1ex
      A \emph{statistic} is a function of a data \emph{sample}:  $f( x_1,\ldots ,x_n )$,
      \vskip1ex
      A \emph{statistic} is itself a \emph{random variable},
      \vskip1ex
      A statistical \emph{estimator} is a \emph{statistic} that provides an estimate of a \emph{distribution} parameter,
      \vskip1ex
      For example:
      \begin{displaymath}
        \bar{x}=\frac{1}{n}{\sum_{i=1}^n x_i}
      \end{displaymath}
      Is an \emph{estimator} of the \emph{mean} of the \emph{distribution},
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:2))>>=
rm(list=ls())
set.seed(1121)  # reset random number generator
# sample from Standard Normal Distribution
sam_ple <- rnorm(1000)

mean(sam_ple)  # sample mean

median(sam_ple)  # sample median

sd(sam_ple)  # sample standard deviation
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimators of Higher Moments}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimators of moments of a probability distribution, based on a \emph{sample} of data, are given by:
      \vskip1ex
      Sample mean: $\hat\mu=\frac{1}{n} \sum_{i=1}^n x_i$
      \vskip1ex
      Sample variance: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\hat\mu)^2$
      \vskip1ex
      With their expected values equal to the population mean and standard deviation:\\
      $\mathbb{E[\hat\mu]=\mu}$ \hskip0.5em and \hskip0.5em $\mathbb{E[\hat\sigma]=\sigma}$
      \vskip1ex
      The sample skewness (third moment) is equal to:
      \begin{displaymath}
        \hat{s}=\frac{n}{(n-1)(n-2)} \sum_{i=1}^n (\frac{x_i-\hat\mu}{\hat\sigma})^3
      \end{displaymath}
      The sample kurtosis (fourth moment) is equal to
      \begin{displaymath}
        \hat{k}=\frac{n(n+1)}{(n-1)(n-2)(n-3)} \sum_{i=1}^n (\frac{x_i-\hat\mu}{\hat\sigma})^4
      \end{displaymath}
      The normal distribution has zero skewness and kurtosis equal to 3,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-(1:1))>>=
rm(list=ls())
# DAX returns
ts_rets <- diff(log(EuStockMarkets[, 1]))
# number of observations
len_rets <- length(ts_rets)
# mean of DAX returns
mean_rets <- mean(ts_rets)
# standard deviation of DAX returns
sd_rets <- sd(ts_rets)
# skew of DAX returns
len_rets/((len_rets-1)*(len_rets-2))*
  sum(((ts_rets - mean_rets)/sd_rets)^3)
# kurtosis of DAX returns
len_rets*(len_rets+1)/((len_rets-1)^3)*
  sum(((ts_rets - mean_rets)/sd_rets)^4)
# random normal returns
ts_rets <- rnorm(len_rets, sd=2)
# mean and standard deviation of random normal returns
mean_rets <- mean(ts_rets)
sd_rets <- sd(ts_rets)
# skew of random normal returns
len_rets/((len_rets-1)*(len_rets-2))*
  sum(((ts_rets - mean_rets)/sd_rets)^3)
# kurtosis of random normal returns
len_rets*(len_rets+1)/((len_rets-1)^3)*
  sum(((ts_rets - mean_rets)/sd_rets)^4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Statistical estimators are themselves \emph{random variables},
      \vskip1ex
      The standard deviation of the estimator of the mean is equal to:
      \begin{displaymath}
        \sigma_{\mu} = \frac{\sigma}{\sqrt{n}}
      \end{displaymath}
      Where $\sigma$ is the population standard deviation,
      \vskip1ex
      The standard error is the estimator of the standard deviation, and for the mean estimator it is equal to:
      \begin{displaymath}
        SE_{\mu} = \frac{\hat\sigma}{\sqrt{n}}
      \end{displaymath}
      where: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\hat\mu)^2$ is the sample standard deviation (the estimator of the population standard deviation),
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
set.seed(1121)  # reset random number generator
# sample from Standard Normal Distribution
len_gth <- 1000
sam_ple <- rnorm(len_gth)
# sample mean
mean(sam_ple)
# sample standard deviation
sd(sam_ple)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Hypothesis Testing}


%%%%%%%%%%%%%%%
\subsection{Hypothesis Testing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Hypothesis Testing} is designed to test the validity of a \emph{null hypothesis},
      \vskip1ex
      A \emph{Hypothesis Test} consists of:
      \begin{itemize}
        \item \emph{null hypothesis},
        \item test \emph{statistic} (from sample),
        \item \emph{significance level} $\alpha$, determining whether to accept or reject the \emph{null hypothesis},
        \item \emph{p}-value (probability of observing the test statistic value, assuming the \emph{null hypothesis} is \texttt{TRUE}),
      \end{itemize}
      If the \emph{p}-value is less than the \emph{significance level} $\alpha$, then the \emph{null hypothesis} is rejected,
      \vskip1ex
      The objective of \emph{Hypothesis Testing} is to invalidate the \emph{null hypothesis},
      \vskip1ex
      In statistics we cannot \emph{prove} that a hypothesis is \texttt{TRUE}; we can only conclude that it's very unlikely to be \texttt{FALSE} (given the data),
    \column{0.5\textwidth}
      \vspace{-1em}
      <<>>=
### Perform two-tailed test that sample is
### from Standard Normal Distribution (mean=0, SD=1)
# generate vector of samples and store in data frame
test_frame <- data.frame(samples=rnorm(1000))

# significance level, two-tailed test, critical value=2*SD
signif_level <- 2*(1-pnorm(2))
signif_level
# get p-values for all the samples
test_frame$p_values <-
  sapply(test_frame$samples, pnorm)
test_frame$p_values <-
  2*(0.5-abs(test_frame$p_values-0.5))
# compare p_values to significance level
test_frame$result <-
  test_frame$p_values > signif_level
# number of null rejections
sum(!test_frame$result)
# show null rejections
head(test_frame[!test_frame$result, ])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Visualizing Hypothesis Testing Using Package \protect\emph{ggplot2}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      <<hyp_test_ggp2,echo=(-(1:2)),eval=FALSE,fig.show='hide'>>=
rm(list=ls())
par(oma=c(1, 1, 1, 1), mgp=c(2, 0.5, 0), mar=c(5, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
library(ggplot2)  # load ggplot2

qplot(  # simple ggplot2
    main="Standard Normal Distribution",
    c(-4, 4),
    stat="function",
    fun=dnorm,
    geom="line",
    xlab=NULL, ylab=NULL
    ) +  # end qplot

theme(  # modify plot theme
    plot.title=element_text(vjust=-1.0),
    plot.background=element_blank()
    ) +  # end theme

geom_vline(  # add vertical line
  aes(xintercept=c(-2.0, 2.0)),
  colour="red",
  linetype="dashed"
  )  # end geom_vline
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/hyp_test_ggp2-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Visualizing Hypothesis Testing Using \protect\emph{ggplot2} (cont.)}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      <<hyp_test_ggp2_2,echo=(-(1:2)),eval=FALSE,fig.show='hide'>>=
rm(list=ls())
par(oma=c(1, 1, 1, 1), mgp=c(2, 0.5, 0), mar=c(5, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
### create ggplot2 with shaded area
x_var <- -400:400/100
norm_frame <- data.frame(x_var=x_var,
                       d.norm=dnorm(x_var))
norm_frame$shade <- ifelse(
                  abs(norm_frame$x_var) >= 2,
                  norm_frame$d.norm, NA)
ggplot(  # main function
  data=norm_frame,
  mapping=aes(x=x_var, y=d.norm)
  ) +  # end ggplot
# plot line
  geom_line() +
# plot shaded area
  geom_ribbon(aes(ymin=0, ymax=shade), fill="red") +
# no axis labels
  xlab("") + ylab("") +
# add title
  ggtitle("Standard Normal Distribution") +
# modify plot theme
  theme(
        plot.title=element_text(vjust=-1.0),
        plot.background=element_blank()
  )  # end theme
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/hyp_test_ggp2_2-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\section{Univariate Statistical Analysis}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Shapiro-Wilk} Test of Normality}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Shapiro-Wilk} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1,\ldots ,x_n\}$ is from a normally distributed population,
      \vskip1ex
      The test statistic is:
      \begin{displaymath}
        W= \frac {(\sum_{i=1}^n a_i x_{(i)})^2} {\sum_{i=1}^n (x_i-\bar{x})^2}
      \end{displaymath}
      Where the: $\{a_1,\ldots ,a_n\}$ are proportional to the \emph{order statistics} of random variables from the normal distribution,
      \vskip1ex
      $x_{(k)}$ is the \emph{k}-th \emph{order statistic}, and is equal to the \emph{k}-th smallest value in the sample: $\{x_1,\ldots ,x_n\}$,
      \vskip1ex
      The \emph{Shapiro-Wilk} statistic follows its own distribution, and is less than or equal to one,
      \vskip1ex
      The \emph{Shapiro-Wilk} statistic is close to one for samples from normal distributions,
      \vskip1ex
      The \emph{p}-value for DAX returns is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and the DAX returns are not from a normally distributed population,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<>>=
# calculate DAX percentage returns
dax_rets <- diff(log(EuStockMarkets[, 1]))

# Shapiro-Wilk test for normal distribution
shapiro.test(rnorm(length(dax_rets)))

# Shapiro-Wilk test for DAX returns
shapiro.test(dax_rets)

# Shapiro-Wilk test for uniform distribution
shapiro.test(runif(length(dax_rets)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Jarque-Bera} Test of Normality}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Jarque-Bera} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1,\ldots ,x_n\}$ is from a normally distributed population,
      \vskip1ex
      The test statistic is:
      \begin{displaymath}
        JB= \frac{n}{6} (\hat{s}^2 + \frac{1}{4} (\hat{k} - 3)^2)
      \end{displaymath}
      Where the skewness and kurtosis are defined as:
      \begin{align*}
        \hat{s} = \frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^3
      &&
        \hat{k} = \frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^4
      \end{align*}
      The \emph{Jarque-Bera} statistic asymptotically follows the \emph{chi-squared} distribution with two degrees of freedom,
      \vskip1ex
      The \emph{Jarque-Bera} statistic is small for samples from normal distributions,
      \vskip1ex
      The \emph{p}-value for DAX returns is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and the DAX returns are not from a normally distributed population,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1))>>=
dax_rets <- diff(log(EuStockMarkets[, 1]))
library(tseries)  # load package tseries

# Jarque-Bera test for normal distribution
jarque.bera.test(rnorm(length(dax_rets)))

# Jarque-Bera test for DAX returns
jarque.bera.test(dax_rets)

# Jarque-Bera test for uniform distribution
jarque.bera.test(runif(length(dax_rets)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Kolmogorov-Smirnov} Test for Probability Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kolmogorov-Smirnov} test is designed to test the \emph{null hypothesis} that two samples: $\{x_1, \ldots , x_n\}$ and $\{y_1, \ldots , y_n\}$ were obtained from the same probability distribution, 
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} statistic is the maximum difference between two empirical cumulative distribution functions (cumulative frequencies):
      \begin{displaymath}
        D = \sup_i | P(x_i) - P(y_i) |
      \end{displaymath}
      The function \texttt{ks.test()} calculates the \emph{Kolmogorov-Smirnov} statistic and its \emph{p}-value, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<>>=
# KS test for normal distribution
ks.test(rnorm(100), rnorm)
# KS test for normal distribution
ks.test(runif(100), rnorm)
# KS test for two normal distributions
ks.test(rnorm(100), rnorm(100, mean=0.1))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Wilcoxon} Test for Distribution Mean}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Wilcoxon} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1, \ldots , x_n\}$ was obtained from a probability distribution with mean zero, 
      \vskip1ex
      The function \texttt{wilcox.test()} calculates the \emph{Wilcoxon} statistic and its \emph{p}-value, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<>>=
# Wilcoxon test for normal distribution
wilcox.test(rnorm(100))
# Wilcoxon test for two normal distributions
wilcox.test(rnorm(100), rnorm(100, mean=0.1))
# Wilcoxon test for two normal distributions
wilcox.test(rnorm(100), rnorm(100, mean=1.0))
# Wilcoxon test for a uniform versus normal distribution
wilcox.test(runif(100), rnorm(100))
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Regression Analysis}


%%%%%%%%%%%%%%%
\subsection{Formula Objects}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      Formulas in \texttt{R} are defined using the "\textasciitilde{}" operator followed by a series of terms separated by the \texttt{"+"} operator,
      \vskip1ex
      Formulas can be defined as separate objects, manipulated, and passed to functions,
      \vskip1ex
      The formula "\texttt{z} \textasciitilde{} \texttt{x}" means the response variable \texttt{z} is explained by the explanatory variable \texttt{x},
      \vskip1ex
      The formula "\texttt{z \textasciitilde{} x + y}" represents a linear model: \texttt{z = ax  + by + c},
      \vskip1ex
      The formula "\texttt{z \textasciitilde{} x - 1}" or "\texttt{z \textasciitilde{} x + 0}" represents a linear model with zero intercept: $z = ax$,
      \vskip1ex
      The function \texttt{update()} modifies existing \texttt{formulas},
      \vskip1ex
      The \texttt{"."} symbol represents either all the remaining data, or the variable that was in this part of the formula,
    \column{0.6\textwidth}
      \vspace{-1em}
      <<>>=
# formula of linear model with zero intercept
lin_formula <- z ~ x + y - 1
lin_formula

# collapsing a character vector into a text string
paste0("x", 1:5)
paste(paste0("x", 1:5), collapse="+")

# creating formula from text string
lin_formula <- as.formula(  # coerce text strings to formula
              paste("z ~ ",
                paste(paste0("x", 1:5), collapse="+")
                )  # end paste
            )  # end as.formula
class(lin_formula)
lin_formula
# modify the formula using "update"
update(lin_formula, log(.) ~ . + beta)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simple \protect\emph{Linear Regression}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A Simple Linear Regression is a linear model between a response variable \texttt{z} and a single explanatory variable \texttt{x}, defined by the formula:
      \begin{displaymath}
        z_i = \alpha + \beta x_i + \varepsilon_i
      \end{displaymath}
      The data consists of $n$ observations of the response and explanatory variables, with the index $i$ ranging from $1$ to $n$,
      \vskip1ex
      $\alpha$ and $\beta$ are the unknown regression coefficients,
      \vskip1ex
      $\varepsilon_i$ are the residuals, assumed to be normally distributed, independent, and stationary,
      \vskip1ex
      In the Ordinary Least Squares method (OLS), the regression parameters are estimated by minimizing the sum of squared residuals, also called the residual sum of squares (RSS):
      \begin{align*}
        RSS = \sum_{i=1}^n {\varepsilon_i^2} = \sum_{i=1}^n {(z_i - \alpha - \beta x_i)^2}\\ = (z - \alpha \mathbb{1} - \beta x)^T (z - \alpha \mathbb{1} - \beta x)
      \end{align*}
      $\mathbb{1}$ is the unit vector, and $\mathbb{1}^T x = x^T \mathbb{1} = \sum_{i=1}^n {x_i}$
    \column{0.5\textwidth}
      The regression coefficients can be found by equating the RSS derivatives to zero:
      \begin{align*}
        RSS_\alpha = -2 (z - \alpha \mathbb{1} - \beta x)^T \mathbb{1} = 0\\
        RSS_\beta = -2 (z - \alpha \mathbb{1} - \beta x)^T x = 0
      \end{align*}
      The solution for $\alpha$ is:
      \begin{align*}
        \alpha = z^T \mathbb{1} - \beta x^T \mathbb{1}
      \end{align*}
      The solution for $\beta$ is:
      \begin{flalign*}
        & (z - (z^T \mathbb{1} - \beta x^T \mathbb{1}) \mathbb{1} - \beta x)^T x = 0\\
        & (z^T x - (z^T \mathbb{1} - \beta x^T \mathbb{1}) \mathbb{1}^T x - \beta x^T x) = 0\\
        & (z^T x - (z^T \mathbb{1}) (x^T \mathbb{1}) + \beta (x^T \mathbb{1})^2 - \beta x^T x) = 0\\
        & \beta = \frac {z^T x - (x^T \mathbb{1}) (z^T \mathbb{1}) } {x^T x - (\mathbb{1}^T x)^2}
      \end{flalign*}
      If the response and explanatory variables have zero mean, then $\alpha=0$ and $\beta=\frac {z^T x} {x^T x}$,
      \vskip1ex
      It's then easy to see that $\beta$ is proportional to the correlation coefficient between the response and explanatory variables,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Linear Regression} with Multiple Explanatory Variables}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A Linear Regression model with $p$ explanatory variables $\{x_j\}$, is defined by the formula:
      \begin{displaymath}
        z_i = \alpha + \sum_{j=1}^{k} {\beta_j x_{i,j}} + \varepsilon_i
      \end{displaymath}
      Or in vector notation:
      \begin{displaymath}
        z = \alpha + \beta x + \varepsilon
      \end{displaymath}
      The response variable $z$ and the $p$ explanatory variables $\{x_j\}$ each contain $n$ observations,
      \vskip1ex
      The response variable $z$ is a vector of length $n$, and the explanatory variable $x$ is a $(n,p)$-dimensional matrix,
      \vskip1ex
      $\alpha$ and $\beta$ are the unknown regression coefficients, with $\alpha$ a scalar and $\beta$ a vector of length $p$,
      \vskip1ex
      $\varepsilon_i$ are the residuals, assumed to be normally distributed, independent, and stationary, with $\varepsilon$ a vector of length $p$,
    \column{0.5\textwidth}
      The OLS estimate for $\alpha$ is given by:
      \begin{align*}
        \alpha = z^T \mathbb{1} - \beta x^T \mathbb{1}
      \end{align*}
      If the variables are de-meaned, then the OLS estimate for $\beta$ is given by equating the RSS derivative to zero:
      \begin{flalign*}
        & RSS_\beta = - 2 (z - \beta x)^T x = 0\\
        & x^T z - \beta x^T x = 0\\
        & \beta = (x^T x)^{-1} x^T z
      \end{flalign*}
      The matrix $x^T x$ is the covariance matrix of the matrix $x$,
      \vskip1ex
      The covariance matrix $x^T x$ is invertible if the columns of $x$ are linearly independent,
      \vskip1ex
      The matrix $(x^T x)^{-1} x^T$ is known as the \emph{Moore-Penrose pseudoinverse} of the matrix $x$,
      \vskip1ex
      In the special case when the inverse matrix $x^{-1}$ does exist, then the \emph{pseudoinverse} matrix simplifies to the inverse: $(x^T x)^{-1} x^T = x^{-1} (x^T)^{-1} x^T = x^{-1}$
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Linear Regression} Using Function \texttt{lm()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let the data generating process for the response variable be given as: $z = \alpha_{lat} + \beta_{lat} x + \varepsilon_{lat}$
      \vskip1ex
      Where $\alpha_{lat}$ and $\beta_{lat}$ are latent (unknown) coefficients, and $\varepsilon_{lat}$ is an unknown vector of random noise (error terms),
      \vskip1ex
      The error terms are the difference between the measured values of the response minus the (unknown) actual response values,
      \vskip1ex
      The function \texttt{lm()} fits a linear model into a set of data, and returns an object of class \texttt{"lm"}, which is a list containing the results of fitting the model:
      \begin{itemize}
        \item call - the model formula,
        \item coefficients - the fitted model coefficients ($\alpha$, $\beta_j$),
        \item residuals - the model residuals (response minus fitted values),
      \end{itemize}
      The regression residuals are not the same as the error terms, because the regression coefficients are not equal to the coefficients of the data generating process,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-(1:1)),eval=TRUE>>=
set.seed(1121)  # initialize random number generator
# define explanatory variable
explana_tory <- rnorm(100, mean=2)
noise <- rnorm(100)
# response equals linear form plus error terms
res_ponse <- -3 + explana_tory + noise
# specify regression formula
reg_formula <- res_ponse ~ explana_tory
reg_model <- lm(reg_formula)  # perform regression
class(reg_model)  # regressions have class lm
attributes(reg_model)
eval(reg_model$call$formula)  # regression formula
reg_model$coefficients  # regression coefficients
coef(reg_model)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Linear Regression} Scatterplot}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The generic function \texttt{plot()} produces a scatterplot when it's called on the regression formula,
      \vskip1ex
      \texttt{abline()} plots a straight line corresponding to the regression coefficients, when it's called on the regression object,
      \vskip1ex
      The fitted (predicted) values are the values of the response variable obtained from applying the regression model to the explanatory variables,
        <<reg_scatter_plot,eval=FALSE,echo=(-(1:2)),fig.show='hide'>>=
par(oma=c(1, 2, 1, 0), mgp=c(2, 1, 0), mar=c(5, 1, 1, 1), cex.lab=0.8, cex.axis=1.0, cex.main=0.8, cex.sub=0.5)
x11(width=6, height=6)
plot(reg_formula)  # plot scatterplot using formula
title(main="Simple Regression", line=-1)
# add regression line
abline(reg_model, lwd=2, col="red")
# plot fitted (predicted) response values
points(x=explana_tory, y=reg_model$fitted.values,
       pch=16, col="blue")
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/reg_scatter_plot.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Linear Regression} Summary}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{summary.lm()} produces a list of regression model summary and diagnostic statistics:
      \begin{itemize}
        \item coefficients: matrix with estimated coefficients, their \emph{t}-statistics, and \emph{p}-values,
        \item r.squared: fraction of response variance explained by the model (correlation between response and explanatory),
        \item adj.r.squared: r.squared adjusted for higher model complexity,
        \item fstatistic: ratio of variance explained by model divided by unexplained variance,
      \end{itemize}
      The regression \emph{null} hypothesis is that the regression coefficients are \emph{zero},
      \vskip1ex
      The \emph{t}-statistic (\emph{t}-value) is the ratio of the estimated value divided by its standard error,
      \vskip1ex
      The \emph{p}-value is the probability of obtaining the observed value of the \emph{t}-statistic, or even more extreme values, under the \emph{null} hypothesis,
      \vskip1ex
      A small \emph{p}-value is often interpreted as meaning that the coefficients are very unlikely to be zero (given the data),
    \column{0.5\textwidth}
      \vspace{-1em}
        <<>>=
reg_model_sum <- summary(reg_model)  # copy regression summary
reg_model_sum  # print the summary to console
attributes(reg_model_sum)$names  # get summary elements
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Interpreting the Regression Statistics}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The regression \texttt{summary} is a list, and its elements can be accessed individually,
      \vskip1ex
      The standard errors of the regression are the standard deviations of the coefficients, given the residuals as the source of error,
      \vskip1ex
      The standard error of $\beta$ in a simple regression is given by: ${\sigma_\beta}^2 = \frac {1} {(n-2)} \frac {E[(\varepsilon^T x)^2]} {(x^T x)^2} = \frac {1} {(n-2)} \frac {E[\varepsilon^2]} {(x^T x)} = \frac {1} {(n-2)} \frac {{\sigma_\varepsilon}^2} {{\sigma_x}^2}$
      \vskip1ex
      The key assumption in the above formula for the standard error and the \emph{p}-value is that the residuals are normally distributed, independent, and stationary,
      \vskip1ex
      If the residuals are not normally distributed, independent, and stationary, then the standard error and the \emph{p}-value may be much bigger than reported by \texttt{summary.lm()}, and therefore the regression may not be statistically significant,
      \vskip1ex
      Market return time series are very far from normal, so the small \emph{p}-values shouldn't be automatically interpreted as meaning that the regression is statistically significant,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<>>=
reg_model_sum$coefficients
reg_model_sum$r.squared
reg_model_sum$adj.r.squared
reg_model_sum$fstatistic
# standard error of beta
reg_model_sum$
  coefficients["explana_tory", "Std. Error"]
sd(reg_model_sum$residuals)/sd(explana_tory)/
  sqrt(unname(reg_model_sum$fstatistic[3]))
anova(reg_model)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Weak Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the relationship between the response and explanatory variables is weak compared to the error terms (noise), then the regression will have low statistical significance,
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-(1:1))>>=
set.seed(1121)  # initialize random number generator
# high noise compared to coefficient
res_ponse <- 3 + 2*explana_tory + rnorm(30, sd=8)
reg_model <- lm(reg_formula)  # perform regression
# estimate of regression coefficient is not
# statistically significant
summary(reg_model)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Influence of Noise on Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
    \vspace{-2em}
      <<reg_noise,eval=FALSE,echo=(-(1:1)),fig.height=5.2,fig.show='hide'>>=
par(oma=c(1, 1, 1, 1), mgp=c(0, 0.5, 0), mar=c(1, 1, 1, 1), cex.lab=1.0, cex.axis=1.0, cex.main=1.0, cex.sub=1.0)
reg_stats <- function(std_dev) {  # noisy regression
  set.seed(1121)  # initialize number generator
# create explanatory and response variables
  explana_tory <- seq(from=0.1, to=3.0, by=0.1)
  res_ponse <- 3 + 0.2*explana_tory +
    rnorm(30, sd=std_dev)
# specify regression formula
  reg_formula <- res_ponse ~ explana_tory
# perform regression and get summary
  reg_model_sum <- summary(lm(reg_formula))
# extract regression statistics
  with(reg_model_sum, c(pval=coefficients[2, 4],
         adj_rsquared=adj.r.squared,
         fstat=fstatistic[1]))
}  # end reg_stats
# apply reg_stats() to vector of std dev values
vec_sd <- seq(from=0.1, to=0.5, by=0.1)
names(vec_sd) <- paste0("sd=", vec_sd)
mat_stats <- t(sapply(vec_sd, reg_stats))
# plot in loop
par(mfrow=c(NCOL(mat_stats), 1))
for (in_dex in 1:NCOL(mat_stats)) {
  plot(mat_stats[, in_dex], type="l",
       xaxt="n", xlab="", ylab="", main="")
  title(main=colnames(mat_stats)[in_dex], line=-1.0)
  axis(1, at=1:(NROW(mat_stats)),
       labels=rownames(mat_stats))
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/reg_noise-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Influence of Noise on Regression Another Method}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
    \vspace{-2em}
      <<eval=FALSE,echo=TRUE>>=
reg_stats <- function(da_ta) {  # get regression
# perform regression and get summary
  col_names <- colnames(da_ta)
  reg_formula <-
    paste(col_names[2], col_names[1], sep="~")
  reg_model_sum <- summary(lm(reg_formula,
                              data=da_ta))
# extract regression statistics
  with(reg_model_sum, c(pval=coefficients[2, 4],
         adj_rsquared=adj.r.squared,
         fstat=fstatistic[1]))
}  # end reg_stats
# apply reg_stats() to vector of std dev values
vec_sd <- seq(from=0.1, to=0.5, by=0.1)
names(vec_sd) <- paste0("sd=", vec_sd)
mat_stats <-
  t(sapply(vec_sd, function (std_dev) {
    set.seed(1121)  # initialize number generator
# create explanatory and response variables
    explana_tory <- seq(from=0.1, to=3.0, by=0.1)
    res_ponse <- 3 + 0.2*explana_tory +
      rnorm(30, sd=std_dev)
    reg_stats(data.frame(explana_tory, res_ponse))
    }))
# plot in loop
par(mfrow=c(NCOL(mat_stats), 1))
for (in_dex in 1:NCOL(mat_stats)) {
  plot(mat_stats[, in_dex], type="l",
       xaxt="n", xlab="", ylab="", main="")
  title(main=colnames(mat_stats)[in_dex], line=-1.0)
  axis(1, at=1:(NROW(mat_stats)),
       labels=rownames(mat_stats))
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/reg_noise-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Linear Regression} Diagnostic Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{plot()} produces diagnostic scatterplots for the residuals, when called on the regression object,
      \vskip1ex
      {\scriptsize
      The diagnostic scatterplots allow for visual inspection to determine the quality of the regression fit,
      \vskip1ex
      "Residuals vs Fitted" is a scatterplot of the residuals vs. the predicted responses,
      \vskip1ex
      "Scale-Location" is a scatterplot of the square root of the standardized residuals vs. the predicted responses,
      \vskip1ex
      The residuals should be randomly distributed around the horizontal line representing zero residual error,
      \vskip1ex
      A pattern in the residuals indicates that the model was not able to capture the relationship between the variables, or that the variables don't follow the statistical assumptions of the regression model,
      \vskip1ex
      "Normal Q-Q" is the standard Q-Q plot, and the points should fall on the diagonal line, indicating that the residuals are normally distributed,
      \vskip1ex
      "Residuals vs Leverage" is a scatterplot of the residuals vs. their leverage,
      \vskip1ex
      Leverage measures the amount by which the predicted response would change if the observed response were shifted by a small amount,
      \vskip1ex
      Cook's distance measures the influence of a single observation on the predicted values, and is proportional to the sum of the squared differences between predictions made with all observations and predictions made without the observation,
      \vskip1ex
      Points with large leverage, or a Cook's distance greater than 1 suggest the presence of an outlier or a poor model,
      }
    \column{0.5\textwidth}
      \vspace{-1em}
      <<plot_reg,eval=FALSE,echo=(-(1:2)),fig.show='hide'>>=
# set plot paramaters - margins and font scale
par(oma=c(1,0,1,0), mgp=c(2,1,0), mar=c(2,1,2,1), cex.lab=0.8, cex.axis=1.0, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2, 2))  # plot 2x2 panels
plot(reg_model)  # plot diagnostic scatterplots
plot(reg_model, which=2)  # plot just Q-Q
      @
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/plot_reg-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Durbin-Watson Test of Autocorrelation of Residuals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Durbin-Watson} test is designed to test the \emph{null hypothesis} that the autocorrelations of regression residuals are equal to zero,
      \vskip1ex
      The test statistic is:
      \begin{displaymath}
        DW = \frac {\sum_{i=2}^n (\varepsilon_i - \varepsilon_{i-1})^2} {\sum_{i=1}^n \varepsilon_i^2}
      \end{displaymath}
      Where $\varepsilon_i$ are the regression residuals,
      \vskip1ex
      The value of the \emph{Durbin-Watson} statistic \emph{DW} is close to zero for large positive autocorrelations, and close to four for large negative autocorrelations,
      \vskip1ex
      The \emph{DW} is close to two for autocorrelations close to zero,
      \vskip1ex
      The \emph{p}-value for the \texttt{reg\_model} regression is large, and we conclude that the \emph{null hypothesis} is \texttt{TRUE}, and the regression residuals are uncorrelated,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(lmtest)  # load lmtest
# perform Durbin-Watson test
dwtest(reg_model)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{devel: Autocorrelated Time Series Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Filtering or smoothing a time series containing an error terms over overlapping periods introduces autocorrelations in the error terms of the time series,
      \vskip1ex
      Autocorrelations in the error terms introduces autocorrelations of the regression residuals, causing the Durbin-Watson test to fail,
      \vskip1ex
      Autocorrelations in the error terms introduce autocorrelations of the regression residuals, causing the Durbin-Watson test to fail,
      \vskip1ex
      The failure of the Durbin-Watson test means that the \emph{standard errors} and \emph{p}-values calculated by the regression model are too small, and therefore the regression may not be statistically significant,
      \vskip1ex
      But the failure of the Durbin-Watson test doesn't reject the existence of a linear relationship between the response and explanatory variables, it just puts it in doubt,
      \vskip1ex
      Links:
      https://onlinecourses.science.psu.edu/stat510/node/72
      http://stats.stackexchange.com/questions/6469/simple-linear-model-with-autocorrelated-errors-in-r
      \vskip1ex
      Regression of non-stationary time series creates \emph{spurious} regressions,
      \vskip1ex
      The \emph{t}-statistics, \emph{p}-values, and \emph{R}-squared all indicate a statistically significant regression,
      \vskip1ex
      But the Durbin-Watson test shows residuals are autocorrelated, which invalidates the other tests,
      \vskip1ex
      The Q-Q plot also shows that residuals are \emph{not} normally distributed,
      \vspace{-1em}
        <<echo=(-(1:3)),eval=FALSE>>=
foo <- env_etf$re_turns[, c("VTI", "VEU")]
end_points <- endpoints(foo, on="months")
head(foo)
tail(foo)
class(foo)
dim(foo)
reg_model <- lm(paste(names(foo), collapse=" ~ "), data=foo)
reg_model_sum <- summary(reg_model)
reg_model_sum
dwtest(reg_model)

# filter over non-overlapping periods
bar <- names(foo)
foo <- merge(period.sum(foo[, 1], INDEX=end_points), period.sum(foo[, 2], INDEX=end_points))
foo <- foo[complete.cases(foo), ]
names(foo) <- bar

# filter over overlapping periods
foo <- rollsum(foo, k=11)


set.seed(1121)
library(lmtest)
# spurious regression in unit root time series
explana_tory <- cumsum(rnorm(100))  # unit root time series
res_ponse <- cumsum(rnorm(100))
reg_formula <- res_ponse ~ explana_tory
reg_model <- lm(reg_formula)  # perform regression
# summary indicates statistically significant regression
reg_model_sum <- summary(reg_model)
reg_model_sum$coefficients
reg_model_sum$r.squared
# Durbin-Watson test shows residuals are autocorrelated
dw_test <- dwtest(reg_model)
c(dw_test$statistic[[1]], dw_test$p.value)
      @
      \vspace{-2em}
        <<autocorr_reg,echo=(-(1:2)),eval=FALSE,fig.height=8,fig.show='hide'>>=
par(oma=c(15, 1, 1, 1), mgp=c(0, 0.5, 0), mar=c(1, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # set plot panels
plot(reg_formula, xlab="", ylab="")  # plot scatterplot using formula
title(main="Spurious Regression", line=-1)
# add regression line
abline(reg_model, lwd=2, col="red")
plot(reg_model, which=2, ask=FALSE)  # plot just Q-Q
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/autocorr_reg-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Omitted Variable Bias}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Omitted Variable Bias} occurs in a regression model that omits important predictors,
      \vskip1ex
      The parameter estimates are biased, even though the \emph{t}-statistics, \emph{p}-values, and \emph{R}-squared all indicate a statistically significant regression,
      \vskip1ex
      But the Durbin-Watson test shows residuals are autocorrelated, invalidating other tests,
      \vspace{-1em}
        <<echo=(-(1:1)),eval=FALSE>>=
library(lmtest)  # load lmtest
de_sign <- data.frame(  # design matrix
  explana_tory=1:30, omit_var=sin(0.2*1:30))
# response depends on both explanatory variables
res_ponse <- with(de_sign,
          0.2*explana_tory + omit_var + 0.2*rnorm(30))
# mis-specified regression only one explanatory
reg_model <- lm(res_ponse ~ explana_tory,
                data=de_sign)
reg_model_sum <- summary(reg_model)
reg_model_sum$coefficients
reg_model_sum$r.squared
# Durbin-Watson test shows residuals are autocorrelated
dwtest(reg_model)$p.value
      @
      \vspace{-2em}
        <<ovb_reg,echo=(-(1:2)),eval=FALSE,fig.height=8,fig.show='hide'>>=
par(oma=c(15, 1, 1, 1), mgp=c(0, 0.5, 0), mar=c(1, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # set plot panels
plot(reg_formula, data=de_sign)
abline(reg_model, lwd=2, col="red")
title(main="OVB Regression", line=-1)
plot(reg_model, which=2, ask=FALSE)  # plot just Q-Q
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/ovb_reg-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Spurious Time Series Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Regression of non-stationary time series creates \emph{spurious} regressions,
      \vskip1ex
      The \emph{t}-statistics, \emph{p}-values, and \emph{R}-squared all indicate a statistically significant regression,
      \vskip1ex
      But the Durbin-Watson test shows residuals are autocorrelated, which invalidates the other tests,
      \vskip1ex
      The Q-Q plot also shows that residuals are \emph{not} normally distributed,
      \vspace{-1em}
        <<echo=(-(1:3)),eval=FALSE>>=
set.seed(1121)
library(lmtest)
# spurious regression in unit root time series
explana_tory <- cumsum(rnorm(100))  # unit root time series
res_ponse <- cumsum(rnorm(100))
reg_formula <- res_ponse ~ explana_tory
reg_model <- lm(reg_formula)  # perform regression
# summary indicates statistically significant regression
reg_model_sum <- summary(reg_model)
reg_model_sum$coefficients
reg_model_sum$r.squared
# Durbin-Watson test shows residuals are autocorrelated
dw_test <- dwtest(reg_model)
c(dw_test$statistic[[1]], dw_test$p.value)
      @
      \vspace{-2em}
        <<spur_reg,echo=(-(1:2)),eval=FALSE,fig.height=8,fig.show='hide'>>=
par(oma=c(15, 1, 1, 1), mgp=c(0, 0.5, 0), mar=c(1, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # set plot panels
plot(reg_formula, xlab="", ylab="")  # plot scatterplot using formula
title(main="Spurious Regression", line=-1)
# add regression line
abline(reg_model, lwd=2, col="red")
plot(reg_model, which=2, ask=FALSE)  # plot just Q-Q
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/spur_reg-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Predictions From \protect\emph{Linear Regression} Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{predict()} is a generic function for forecasting based on a given model,
      \vskip1ex
      \texttt{predict.lm()} is the predict method for linear models (regressions),
      \vspace{-1em}
        <<predict_lm,echo=TRUE,fig.show='hide'>>=
explana_tory <- seq(from=0.1, to=3.0, by=0.1)  # explanatory variable
res_ponse <- 3 + 2*explana_tory + rnorm(30)
reg_formula <- res_ponse ~ explana_tory
reg_model <- lm(reg_formula)  # perform regression
new_data <- data.frame(explana_tory=0.1*31:40)
predict_lm <- predict(object=reg_model,
                      newdata=new_data, level=0.95,
                      interval="confidence")
predict_lm <- as.data.frame(predict_lm)
head(predict_lm, 2)
plot(reg_formula, xlim=c(1.0, 4.0),
     ylim=range(res_ponse, predict_lm),
     main="Regression predictions")
abline(reg_model, col="red")
with(predict_lm, {
  points(x=new_data$explana_tory, y=fit, pch=16, col="blue")
  lines(x=new_data$explana_tory, y=lwr, lwd=2, col="red")
  lines(x=new_data$explana_tory, y=upr, lwd=2, col="red")
})  # end with
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/predict_lm-1}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Classification}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Logistic} Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{logistic} function expresses the cumulative probability of a numerical variable ranging over the whole interval of real numbers: 
      \begin{displaymath}
        p(x) = \frac{1}{1 + \exp(-\lambda x)}
      \end{displaymath}
      Where $\lambda$ is the scale (dispersion) parameter,
      \vskip1ex
      The \emph{logistic} function can be inverted to obtain the \emph{Odds Ratio} (the ratio of probabilities for favorable to unfavorable outcomes): 
      \begin{displaymath}
        \frac{p(x)}{1 - p(x)} = \exp(\lambda x)
      \end{displaymath}
      The function \texttt{plogis()} gives the cumulative probability of the \emph{Logistic} distribution, 
        <<echo=(-(1:1)),eval=FALSE>>=
par(oma=c(1, 1, 1, 1), mar=c(2, 1, 1, 1), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
lamb_da <- c(0.5, 1, 1.5)
col_ors <- c("red", "black", "blue")
# plot three curves in loop
for (in_dex in 1:3) {
  curve(expr=plogis(x, scale=lamb_da[in_dex]),
        xlim=c(-4, 4), type="l", 
        xlab="", ylab="", lwd=2,
        col=col_ors[in_dex], add=(in_dex>1))
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/logistic_func.png}
      \vspace{-2em}
        <<echo=TRUE,eval=FALSE>>=
# add title
title(main="Logistic function", line=0.5)
# add legend
legend("topleft", title="Scale parameters", 
       paste("lambda", lamb_da, sep="="),
       inset=0.05, cex=0.8, lwd=2, 
       lty=c(1, 1, 1), col=col_ors)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing \protect\emph{Logistic} Regression Using the Function \texttt{glm()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Linear} regression isn't suitable when the response variable represents categorical data (\texttt{factor}), 
      \vskip1ex
      But \emph{logistic} regression (\emph{logit}) can be used to model data with a categorical response variable, 
      \vskip1ex
      The function \texttt{glm()} fits generalized linear models, including \emph{logistic} regressions, 
      \vskip1ex
      \texttt{glm()} can fit two different types of response variables: categorical data (\texttt{factors}) from individual observations, or counts of categorical data (\texttt{integers}) from groups of observations, 
      \vskip1ex
      The family object \texttt{binomial(link="logit")} specifies a binomial distribution of residuals in the \emph{logistic} regression model, 
      <<echo=TRUE,eval=FALSE>>=
# simulate categorical data
sco_re <- sort(runif(100))
thresh_old <- 0.5  # probability threshold
ac_tive <- ((sco_re + rnorm(100, sd=0.1)) > thresh_old)
# Wilcoxon test for sco_re predictor
wilcox.test(sco_re[ac_tive], sco_re[!ac_tive])
# perform logit regression
log_it <- glm(ac_tive ~ sco_re, family=binomial(logit))
summary(log_it)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/logistic_density.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
plot(x=sco_re, y=log_it$fitted.values, type="l", lwd=3, 
     main="Category densities and logistic function", 
     xlab="score", ylab="probability")
den_sity <- density(sco_re[ac_tive])
den_sity$y <- den_sity$y/max(den_sity$y)
lines(den_sity, col="red")
polygon(c(min(den_sity$x), den_sity$x, max(den_sity$x)), c(min(den_sity$y), den_sity$y, min(den_sity$y)), col=rgb(1, 0, 0, 0.2), border=NA)
den_sity <- density(sco_re[!ac_tive])
den_sity$y <- den_sity$y/max(den_sity$y)
lines(den_sity, col="blue")
polygon(c(min(den_sity$x), den_sity$x, max(den_sity$x)), c(min(den_sity$y), den_sity$y, min(den_sity$y)), col=rgb(0, 0, 1, 0.2), border=NA)
# add legend
legend(x="top", bty="n", lty=c(1, NA, NA), lwd=c(3, NA, NA), pch=c(NA, 15, 15), 
       legend=c("logistic fit", "active", "non-active"),
       col=c("black", "red", "blue"), 
       text.col=c("black", "red", "blue"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{ISLR} With Datasets for Machine Learning}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{ISLR} contains datasets used in the book \emph{"Introduction to Statistical Learning"}:\
      \fullcite{islbook}
    \column{0.5\textwidth}
      <<echo=TRUE,eval=FALSE>>=
library(ISLR)  # load package ISLR
# get documentation for package tseries
packageDescription("ISLR")  # get short description

help(package="ISLR")  # load help page

library(ISLR)  # load package ISLR

data(package="ISLR")  # list all datasets in ISLR

ls("package:ISLR")  # list all objects in ISLR

detach("package:ISLR")  # remove ISLR from search path
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \texttt{Default} Dataset}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{Default} dataset is a data frame in package \emph{ISLR}, with credit default data, 
      \vskip1ex
      The \texttt{Default} data frame contains two columns of binary categorical data (\texttt{factors}): \texttt{default} and \texttt{student}, and two columns of numerical data: \texttt{balance} and \texttt{income}, 
      \vskip1ex
      The columns \texttt{student}, \texttt{balance}, and \texttt{income} can be used as \emph{predictors} to predict the \texttt{default} column, 
      <<echo=TRUE,eval=FALSE>>=
library(ISLR)  # load package ISLR
# load credit default data
attach(Default)
summary(Default)
sapply(Default, class)
dim(Default); head(Default)
x_lim <- range(balance)
y_lim <- range(income)
# plot data points for non-defaulters
default_ed <- (default=="Yes")
plot(income ~ balance, 
     main="Default dataset from package ISLR", 
     xlim=x_lim, ylim=y_lim, 
     data=Default[!default_ed, ], 
     pch=4, col="blue")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/islr_default_data.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot data points for defaulters
points(income ~ balance, 
       data=Default[default_ed, ], 
       pch=4, col="red")
# add legend
legend(x="topright", bty="n", 
       legend=c("non-defaulters", "defaulters"),
       col=c("blue", "red"), lty=1, pch=4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Boxplots of the \texttt{Default} Dataset}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{Box Plot} (box-and-whisker plot) is a graphical display of a distribution of values, 
      \vskip1ex
      The \emph{box} represents the upper and lower quartiles, \\
      the vertical lines (whiskers) represent values beyond the quartiles, \\
      and open circles represent values beyond the nominal range (outliers),
      \vskip1ex
      The function \texttt{boxplot()} plots a box-and-whisker plot for a distribution of values,
      \vskip1ex
      \texttt{boxplot()} has two \texttt{methods}: one for \texttt{formula} objects (involving categorical variables), and another for \texttt{data frames},
      \vskip1ex
      The \emph{Wilcoxon} test shows that the \texttt{balance} column provides a strong separation between defaulters and non-defaulters, but the \texttt{income} column doesn't, 
      <<echo=TRUE,eval=FALSE>>=
default_ed <- (default=="Yes")
# Wilcoxon test for balance predictor
wilcox.test(balance[default_ed], balance[!default_ed])
# Wilcoxon test for income predictor
wilcox.test(income[default_ed], income[!default_ed])
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/islr_default_boxplot.png}
      \vspace{-2em}
      <<echo=(-(1:2)),eval=FALSE>>=
library(ISLR)  # load package ISLR
attach(Default)  # load credit default data
par(mfrow=c(1,2))  # set plot panels
# balance boxplot
boxplot(formula=balance ~ default, 
        col="lightgrey", 
        main="balance", xlab="default")
# income boxplot
boxplot(formula=income ~ default, 
        col="lightgrey", 
        main="income", xlab="default")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Modeling Credit Defaults Using \protect\emph{Logistic} Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{balance} column can be used to calculate the probability of default using \emph{logistic} regression, 
      \vskip1ex
      The residuals in \emph{logistic} regression are the differences betweeen the actual response values (\texttt{0} and \texttt{1}), and the calculated probabilities of default, 
      \vskip1ex
      The \emph{logit} residuals are not normally distributed, so the data is fitted using the \emph{maximum-likelihood} method, instead of least squares, 
      \vskip1ex
      The family object \texttt{binomial(link="logit")} specifies a binomial distribution of residuals in the \emph{logistic} regression model, 
      <<echo=TRUE,eval=FALSE>>=
# fit logistic regression model
log_it <- glm(default ~ balance, 
              family=binomial(logit))
summary(log_it)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/islr_logistic_reg.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
plot(x=balance, y=default_ed, 
     main="Logistic regression of credit defaults", col="orange", 
     xlab="credit balance", ylab="defaults")
or_der <- order(balance)
lines(x=balance[or_der], y=log_it$fitted.values[or_der], 
      col="blue", lwd=2)
legend(x="topleft", inset=0.1, 
       legend=c("defaults", "logit fitted values"),
       col=c("orange", "blue"), lty=c(NA, 1), pch=c(1, NA), lwd=c(3, 3))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Modeling Cumulative Defaults Using \protect\emph{Logistic} Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The cumulative count of defaults with respect to a single predictor can be modelled as a \emph{logistic} function, using the function \texttt{glm()}, 
      \vskip1ex
      The response variable should be specified as a matrix with two columns, one containing the number of defaults, and other the number of non-defaults, 
      <<echo=(-(1:2)),eval=FALSE>>=
library(ISLR)  # load package ISLR
attach(Default)  # load credit default data
# calculate cumulative defaults
default_ed <- (default=="Yes")
to_tal <- sum(default_ed)
default_s <- sapply(balance, function(ba_lance) {
    sum(default_ed[balance <= ba_lance])
})  # end sapply
# perform logit regression
log_it <- glm(
  cbind(default_s, to_tal-default_s) ~ 
    balance, 
  family=binomial(logit))
summary(log_it)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/islr_logistic_count.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
plot(x=balance, y=default_s/to_tal, col="orange", lwd=1, 
     main="Cumulative defaults versus balance", 
     xlab="credit balance", ylab="cumulative defaults")
or_der <- order(balance)
lines(x=balance[or_der], y=log_it$fitted.values[or_der], 
      col="blue", lwd=2)
legend(x="topleft", inset=0.1, 
       legend=c("cumulative defaults", "fitted values"),
       col=c("orange", "blue"), lty=c(NA, 1), pch=c(1, NA), lwd=c(3, 3))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Multifactor \protect\emph{Logistic} Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Logistic} regression calculates the probability of categorical variables, from the \emph{Odds Ratio} of continuous explanatory variables: 
      \begin{displaymath}
        p = \frac{1}{1 + \exp(- \lambda_0 - \sum_{i=1}^n \lambda_i x_i)}
      \end{displaymath}
      The \emph{generic} function \texttt{summary()} produces a list of regression model summary and diagnostic statistics:
      \begin{itemize}
        \item coefficients: matrix with estimated coefficients, their \emph{z}-values, and \emph{p}-values, 
        \item \emph{Null} deviance: measures the differences betweeen the response values and the probabilities calculated using only the intercept, 
        \item \emph{Residual} deviance: measures the differences betweeen the response values and the model probabilities, 
      \end{itemize}
      The \texttt{balance} and \texttt{student} columns are statistically significant, but the \texttt{income} column is not, 
    \column{0.5\textwidth}
      \vspace{-2em}
      <<echo=(-(1:3)),eval=TRUE>>=
library(ISLR)  # load package ISLR
attach(Default)  # load credit default data
# fit multifactor logistic regression model
col_names <- colnames(Default)
for_mula <- as.formula(paste(col_names[1], 
  paste(col_names[-1], collapse="+"), sep=" ~ "))
log_it <- glm(for_mula, data=Default, 
              family=binomial(logit))
summary(log_it)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Confounding Variables in Multifactor \protect\emph{Logistic} Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{student} column is a confounding variable since it's correlated with the \texttt{balance} column, 
      \vskip1ex
      Students are less likely to default than non-students with the same \texttt{balance}, 
      \vskip1ex
      But on average students have higher \texttt{balances} than non-students, which makes them more likely to default, 
      \vskip1ex
      That's why the multifactor regression coefficient for \texttt{student} is negative, while the single factor coefficient for \texttt{student} is positive, 
      <<echo=(-(1:2)),eval=FALSE>>=
library(ISLR)  # load package ISLR
attach(Default)  # load credit default data
default_ed <- (default=="Yes")
stu_dent <- (student=="Yes")
# calculate cumulative defaults
default_s <- sapply(balance, 
  function(ba_lance) {
    c(stu_dent=sum(default_ed[stu_dent & (balance <= ba_lance)]), 
      non_student=sum(default_ed[(!stu_dent) & (balance <= ba_lance)]))
})  # end sapply
to_tal <- c(sum(default_ed[stu_dent]), sum(default_ed[!stu_dent]))
default_s <- t(default_s / to_tal)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/islr_student_boxplot.png}
      \vspace{-2em}
      <<echo=(-(1:2)),eval=FALSE>>=
library(ISLR)  # load package ISLR
attach(Default)  # load credit default data
# plot cumulative defaults
par(mfrow=c(1,2))  # set plot panels
or_der <- order(balance)
plot(x=balance[or_der], y=default_s[or_der, 1], 
     col="red", t="l", lwd=2, 
     main="Cumulative defaults of\n students and non-students", 
     xlab="credit balance", ylab="")
lines(x=balance[or_der], y=default_s[or_der, 2], 
      col="blue", lwd=2)
legend(x="topleft", bty="n", 
       legend=c("students", "non-students"),
       col=c("red", "blue"), text.col=c("red", "blue"), 
       lwd=c(3, 3))
# balance boxplot for student factor
boxplot(formula=balance ~ student, 
        col="lightgrey", 
        main="balance", xlab="student")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Forecasting of Credit Defaults using Logistic Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{predict()} is a generic function for forecasting based on a given model,
      \vskip1ex
      \emph{Logistic} regression calculates the probability of categorical variables, based on continuous explanatory variables, 
      \vskip1ex
      A \emph{type I error} is the incorrect rejection of a \texttt{TRUE} case (i.e. a "false positive"), 
      \vskip1ex
      That is, a \emph{type I error} is when there is no default, but it's classified as a default, 
      \vskip1ex
      A \emph{type II error} is the incorrect acceptance of a \texttt{FALSE} case (i.e. a "false negative"), 
      \vskip1ex
      That is, a \emph{type II error} is when there is a default, but it's classified as no default, 
      \vskip1ex
      A confusion matrix is a table that summarizes the performance of a classification model on a set of test data for which the true values are known, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# fit full logistic regression model
for_mula <- default ~ balance
for_mula <- as.formula(paste(col_names[1], 
  paste(col_names[-1], collapse="+"), sep=" ~ "))
log_it <- glm(for_mula, data=Default, 
              family=binomial(logit))
fore_casts <- predict(log_it, type="response")
length(fore_casts)
fore_casts[1:10]
thresh_old <- 0.5  # probability threshold
# calculate confusion matrix
table((fore_casts>thresh_old), default_ed)
sum(default_ed)

# fit logistic regression over training data
sam_ple <- sample(x=1:NROW(Default), size=NROW(Default)/2)
train_data <- Default[sam_ple, ]
log_it <- glm(for_mula, data=train_data, 
              family=binomial(link="logit"))

# forecast over test data
test_data <- Default[-sam_ple, ]
fore_casts <- predict(log_it, newdata=test_data, type="response")
# calculate confusion matrix
table((fore_casts>thresh_old), test_data$default=="Yes")
detach(Default)
      @
  \end{columns}
\end{block}

\end{frame}


\end{document}
