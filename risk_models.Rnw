% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size="scriptsize", fig.width=4, fig.height=4)
options(width=60, dev="pdf")
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
% \usepackage{mathtools}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[Risk Analysis and Model Construction]{Risk Analysis and Model Construction}
\subtitle{FRE6871 \& FRE7241, Fall 2017}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{\today}



%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle


%%%%%%%%%%%%%%%
\section{Modeling and Fitting Asset Returns}


%%%%%%%%%%%%%%%
\subsection{Higher Moments of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimators of moments of a probability distribution are given by:
      \vskip1ex
      Sample mean: $\bar{x}=\frac{1}{n} \sum_{i=1}^n x_i$
      \vskip1ex
      Sample variance: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$
      \vskip1ex
      With their expected values equal to the population mean and standard deviation:\\
      $\mathbb{E}[\bar{x}]=\mu$ \hskip0.5em and \hskip0.5em $\mathbb{E}[\hat\sigma]=\sigma$
      \vskip1ex
      The sample skewness (third moment):
      \begin{displaymath}
        \hat{s}=\frac{n}{(n-1)(n-2)} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^3
      \end{displaymath}
      The sample kurtosis (fourth moment):
      \begin{displaymath}
        \hat{k}=\frac{n(n+1)}{(n-1)(n-2)(n-3)} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^4
      \end{displaymath}
      The normal distribution has zero skewness and kurtosis equal to 3,
      \vskip1ex
      Stock returns typically have negative skewness and kurtosis much greater than 3,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# VTI percentage returns
re_turns <- rutils::diff_xts(log(quantmod::Cl(rutils::env_etf$VTI)))
# number of observations
n_row <- NROW(re_turns)
# mean of VTI returns
mean_rets <- mean(re_turns)
# standard deviation of VTI returns
sd_rets <- sd(re_turns)
# skew of VTI returns
n_row/((n_row-1)*(n_row-2))*
  sum(((re_turns - mean_rets)/sd_rets)^3)
# kurtosis of VTI returns
n_row*(n_row+1)/((n_row-1)^3)*
  sum(((re_turns - mean_rets)/sd_rets)^4)
# random normal returns
re_turns <- rnorm(n_row, sd=sd_rets)
# mean and standard deviation of random normal returns
mean_rets <- mean(re_turns)
sd_rets <- sd(re_turns)
# skew of random normal returns
n_row/((n_row-1)*(n_row-2))*
  sum(((re_turns - mean_rets)/sd_rets)^3)
# kurtosis of random normal returns
n_row*(n_row+1)/((n_row-1)^3)*
  sum(((re_turns - mean_rets)/sd_rets)^4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Statistical estimators are functions of samples (which are random variables), and therefore are themselves \emph{random variables},
      \vskip1ex
      The \emph{standard error} (SE) of an estimator is defined as its \emph{standard deviation} (not to be confused with the \emph{population standard deviation} of the underlying random variable),
      \vskip1ex
      For example, the \emph{standard error} of the estimator of the mean is equal to:
      \begin{displaymath}
        \sigma_{\mu} = \frac{\sigma}{\sqrt{n}}
      \end{displaymath}
      Where $\sigma$ is the \emph{population standard deviation} (which is usually unkown),
      \vskip1ex
      The \emph{estimator} of this \emph{standard error} is equal to:
      \begin{displaymath}
        SE_{\mu} = \frac{\hat\sigma}{\sqrt{n}}
      \end{displaymath}
      where: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$ is the sample standard deviation (the estimator of the population standard deviation),
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
set.seed(1121)  # reset random number generator
# sample from Standard Normal Distribution
n_row <- 1000
sam_ple <- rnorm(n_row)
# sample mean
mean(sam_ple)
# sample standard deviation
sd(sam_ple)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Robust Measures of Dispersion and Skewness}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability), defined using the median (instead of the mean):
      \begin{displaymath}
        MAD = median(abs(C_i - median(C_i)))
      \end{displaymath}
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$, 
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation, 
      \vskip1ex
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance, 
      \vskip1ex
      For normally distributed data the \emph{MAD} has a larger standard error than the standard deviation, 
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
re_turns <- rnorm(1000)
sd(re_turns)
mad(re_turns)
median(abs(re_turns - median(re_turns)))
median(abs(re_turns - median(re_turns)))/qnorm(0.75)
# bootstrap of sd and mad estimators
boot_strap <- sapply(1:10000, function(x) {
  boot_sample <-
    re_turns[sample.int(n_row, replace=TRUE)]
  c(sd=sd(boot_sample),
    mad=mad(boot_sample))
})  # end sapply
boot_strap <- t(boot_strap)
# parallel bootstrap under Windows
library(parallel)  # load package parallel
num_cores <- detectCores() - 1  # number of cores
clus_ter <- makeCluster(num_cores)  # initialize compute cluster
boot_strap <- parLapply(clus_ter, 1:10000, 
  function(x, re_turns) {
    boot_sample <- 
      re_turns[sample.int(NROW(re_turns), replace=TRUE)]
    c(sd=sd(boot_sample),
      mad=mad(boot_sample))
  }, re_turns=re_turns)  # end parLapply
# parallel bootstrap under Mac-OSX or Linux
boot_strap <- mclapply(1:10000, 
  function(x) {
    boot_sample <- 
      re_turns[sample.int(NROW(re_turns), replace=TRUE)]
    c(sd=sd(boot_sample),
      mad=mad(boot_sample))
  }, mc.cores=num_cores)  # end mclapply
stopCluster(clus_ter)  # stop R processes over cluster
# analyze bootstrapped variance
boot_strap <- rutils::do_call(rbind, boot_strap)
head(boot_strap)
sum(is.na(boot_strap))
# means and standard errors from bootstrap
apply(boot_strap, MARGIN=2, 
      function(x) c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Normal (Gaussian)} Probability Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Normal (Gaussian)} probability density function is given by:
      \begin{displaymath}
        N(x, \mu, \sigma) = \frac{e^{-(x-\mu)^2/2\sigma^2}}{\sigma\sqrt{2 \pi}}
      \end{displaymath}
      The \emph{Standard Normal} distribution $N(0, 1)$ is a special case of the \emph{Normal} $N(\mu, \sigma)$ with $\mu=0$ and $\sigma=1$,
      \vskip1ex
      The function \texttt{dnorm()} calculates the \emph{Normal} probability density,
      <<echo=TRUE,eval=FALSE>>=
x_var <- seq(-5, 7, length=100)
y_var <- dnorm(x_var, mean=1.0, sd=2.0)
plot(x_var, y_var, type="l", lty="solid",
     xlab="", ylab="")
title(main="Normal Density Function", line=0.5)
star_t <- 3; fin_ish <- 5  # set lower and upper bounds
# set polygon base
are_a <- ((x_var >= star_t) & (x_var <= fin_ish))
polygon(c(star_t, x_var[are_a], fin_ish),  # draw polygon
        c(-1, y_var[are_a], -1), col="red")
      @
    \column{0.5\textwidth}
    \includegraphics[width=0.5\paperwidth]{figure/norm_dist}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Normal (Gaussian)} Probability Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Plots of several \emph{Normal} distributions with different values of $\sigma$, using the function \texttt{curve()} for plotting functions given by their name,
      <<norm_dist_mult_curves,eval=FALSE,echo=(-(1:1)),fig.show="hide">>=
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
sig_mas <- c(0.5, 1, 1.5, 2)  # sigma values
# create plot colors
col_ors <- c("red", "black", "blue", "green")
# create legend labels
lab_els <- paste("sigma", sig_mas, sep="=")
for (in_dex in 1:4) {  # plot four curves
curve(expr=dnorm(x, sd=sig_mas[in_dex]),
      type="l", xlim=c(-4, 4),
      xlab="", ylab="", lwd=2,
      col=col_ors[in_dex],
      add=as.logical(in_dex-1))
}  # end for
# add title
title(main="Normal Distributions", line=0.5)
# add legend
legend("topright", inset=0.05, title="Sigmas",
       lab_els, cex=0.8, lwd=2, lty=c(1, 1, 1, 1),
       col=col_ors)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/norm_dist_mult_curves-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Student's \protect\emph{t-distribution}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $z_{1},\ldots , z_{\nu}$ be independent standard normal random variables, with sample mean: $\bar{z}=\frac{1}{\nu} \sum_{i=1}^{\nu} z_i$ ($\mathbb{E}[\bar{z}]=\mu$) and sample variance: $\hat\sigma^2=\frac{1}{\nu-1} \sum_{i=1}^{\nu} (z_i-\bar{z})^2$
      \vskip1ex
      Then the random variable (\emph{t-ratio}):
      \begin{displaymath}
        t = \frac{\bar{z} - \mu}{\hat\sigma / \sqrt{\nu}}
      \end{displaymath}
      Follows the \emph{t-distribution} with $\nu$ degrees of freedom, with the probability density function:
      \begin{displaymath}
        P(x) = \frac{\Gamma((\nu+1)/2)}{\sqrt{\pi \nu}\,\Gamma(\nu/2)}\, (1 + x^2/\nu)^{-(\nu+1)/2}
      \end{displaymath}
      \vspace{-1em}
        <<eval=FALSE,echo=(-(1:2))>>=
x11(width=6, height=5)
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
d_free <- c(3, 6, 9)  # df values
col_ors <- c("black", "red", "blue", "green")
lab_els <- c("normal", paste("df", d_free, sep="="))
# plot a Normal probability distribution
curve(expr=dnorm, type="l", xlim=c(-4, 4),
      xlab="", ylab="", lwd=2)
for (in_dex in 1:3) {  # plot three t-distributions
curve(expr=dt(x, df=d_free[in_dex]),
      type="l", xlab="", ylab="", lwd=2,
      col=col_ors[in_dex+1], add=TRUE)
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/t_dist_mult.png}\\
      \vspace{-1em}
        <<eval=FALSE,echo=TRUE>>=
# add title
title(main="t-distributions", line=0.5)
# add legend
legend("topright", inset=0.05,
       title="Degrees\n of freedom", lab_els,
       cex=0.8, lwd=6, lty=c(1, 1, 1, 1),
       col=col_ors)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Log-likelihood Function of Student's \protect\emph{t-distribution}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The non-standardized Student's \emph{t-distribution} is given by:
      \begin{displaymath}
        P(x) = \frac{\Gamma((\nu+1)/2)}{\sqrt{\pi \nu} \, \sigma \, \Gamma(\nu/2)} \, (1 + (\frac{x - \mu}{\sigma})^2/\nu)^{-(\nu+1)/2}
      \end{displaymath}
      Where $\nu$ is the degrees of freedom, $\mu$ is the location parameter, and $\sigma$ is the scale parameter,  
      \vskip1ex
      The corresponding negative of \emph{log-likelihood} function can be written as:
      \begin{multline*}
        P(x) = -log(\frac{\Gamma((\nu+1)/2)}{\sqrt{\pi \nu} \, \Gamma(\nu/2)}) + log(\sigma ) + \\
        \frac{\nu+1}{2} \, log(1 + (\frac{x - \mu}{\sigma})^2/\nu)
      \end{multline*}
      $\mathcal{L}(\theta|\bar{x})$ is a function of the parameters of a statistical model $(\theta)$, given a sample of observed values $(\bar{x})$, taken under the model's probability distribution $P(x|\theta)$:
      \begin{displaymath}
        \mathcal{L}(\theta|x) = \prod_{i=1}^{n} P(x_i|\theta)
      \end{displaymath}
      The \emph{likelihood} function measures how \emph{likely} are the parameters of a statistical model, given a sample of observed values $(\bar{x})$,
      \vskip1ex
      The \emph{maximum-likelihood} estimate (\emph{MLE}) of the model's parameters are those that maximize the \emph{likelihood} function:
      \begin{displaymath}
        \theta_{MLE} = \operatorname*{arg\,max}_{\theta} {\mathcal{L}(\theta|x)}
      \end{displaymath}
      In practice the logarithm of the \emph{likelihood} $\log(\mathcal{L})$ is maximized, instead of the \emph{likelihood} itself,
      \vskip1ex
      The function \texttt{outer()} calculates the \emph{outer} product of two matrices, and by default multiplies the elements of its arguments,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# objective function is log-likelihood
object_ive <- function(pa_r, free_dom, sam_ple) {
  sum(
    -log(gamma((free_dom+1)/2) / 
      (sqrt(pi*free_dom) * gamma(free_dom/2))) + 
    log(pa_r[2]) + 
    (free_dom+1)/2 * log(1 + ((sam_ple - pa_r[1])/
                            pa_r[2])^2/free_dom))
}  # end object_ive
# simpler objective function
object_ive <- function(pa_r, free_dom, sam_ple) {
  -sum(log(dt(x=(sam_ple-pa_r[1])/pa_r[2], 
              df=free_dom)/pa_r[2]))
}  # end object_ive
# demonstrate equivalence of the two methods
object_ive(c(0, 1), 2, 2:5)
-sum(log(dt(x=2:5, df=2)))
object_ive(c(1, 0.5), 2, 2:5)
-sum(log(dt(x=(2:5-1)/0.5, df=2)/0.5))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fitting Asset Returns into Student's \protect\emph{t-distribution}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{fitdistr()} from package \emph{MASS} fits a univariate distribution to a sample of data, by performing \emph{maximum likelihood} optimization,
      \vskip1ex
      The function \texttt{fitdistr()} performs a \emph{maximum likelihood} optimization to find the non-standardized Student's \emph{t-distribution} location and scale parameters,
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# VTI percentage returns
re_turns <- rutils::diff_xts(log(quantmod::Cl(rutils::env_etf$VTI)))
# initial parameters
par_init <- c(mean=0, scale=0.01)
# fit distribution using optim()
optim_fit <- optim(par=par_init,
  fn=object_ive, # log-likelihood function
  sam_ple=re_turns, 
  free_dom=2, # degrees of freedom
  method="L-BFGS-B", # quasi-Newton method
  upper=c(1, 0.1), # upper constraint
  lower=c(-1, 1e-7)) # lower constraint
# optimal parameters
lo_cation <- optim_fit$par["mean"]
sc_ale <- optim_fit$par["scale"]
# fit distribution using MASS::fitdistr()
optim_fit <- MASS::fitdistr(re_turns, 
  densfun="t", df=2, lower=c(-1, 1e-7))
optim_fit$estimate
optim_fit$sd
lo_cation <- optim_fit$estimate[1]
sc_ale <- optim_fit$estimate[2]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting the Fitted Student's \protect\emph{t-distribution}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{hist()} calculates and plots a histogram, and returns its data invisibly,
      \vskip1ex
      The parameter \texttt{breaks} is the number of cells of the histogram,
      <<echo=(-(1:1)),eval=FALSE>>=
x11(width=6, height=5)
# plot histogram of VTI returns
histo_gram <- hist(re_turns, col="lightgrey", 
  xlab="returns", breaks=100, xlim=c(-0.05, 0.05), 
  ylab="frequency", freq=FALSE, 
  main="VTI returns histogram")
lines(density(re_turns, adjust=1.5), 
      lwd=3, col="blue")
# plot the Normal probability distribution
curve(expr=dnorm(x, mean=mean(re_turns), 
  sd=sd(re_turns)), add=TRUE, type="l", 
  xlab="", ylab="", lwd=3, col="green")
# plot t-distribution function
curve(expr=dt((x-lo_cation)/sc_ale, df=2)/sc_ale, 
      type="l", xlab="", ylab="", lwd=3,
      col="red", add=TRUE)
# add legend
legend("topright", inset=0.05, 
  leg=c("density", "t-distr", "normal"),
  lwd=6, lty=c(1, 1, 1),
  col=c("blue", "red", "green"))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/t_dist_rets.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Chi-squared} Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $z_1, \ldots , z_k$ be independent standard \emph{Normal} random variables,
      \vskip1ex
      Then the random variable $X = \sum_{i=1}^k z_i^2$ is distributed according to the \emph{Chi-squared} distribution with \texttt{k} degrees of freedom: $X \sim \chi_k^2$, and its probability density function is given by:
      \begin{displaymath}
        P(x) = \frac{x^{k/2-1}\,e^{-x/2}}{2^{k/2}\, \Gamma(k/2)}
      \end{displaymath}
        <<eval=FALSE,echo=(-(1:2))>>=
x11(width=6, height=5)
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
d_free <- c(2, 5, 8, 11)  # df values
# create plot colors
col_ors <- c("red", "black", "blue", "green")
# create legend labels
lab_els <- paste("df", d_free, sep="=")
for (in_dex in 1:4) {  # plot four curves
curve(expr=dchisq(x, df=d_free[in_dex]),
      type="l", xlim=c(0, 20), ylim=c(0, 0.3),
      xlab="", ylab="", lwd=2,
      col=col_ors[in_dex],
      add=as.logical(in_dex-1))
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/chisq_dist_mult.png}\\
      \vspace{-1em}
        <<eval=FALSE,echo=TRUE>>=
# add title
title(main="Chi-squared Distributions", line=0.5)
# add legend
legend("topright", inset=0.05,
       title="Degrees of freedom", lab_els,
       cex=0.8, lwd=6, lty=c(1, 1, 1, 1),
       col=col_ors)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Kolmogorov-Smirnov} Test for Probability Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kolmogorov-Smirnov} test is designed to test the \emph{null hypothesis} that two samples: $\{x_1, \ldots , x_n\}$ and $\{y_1, \ldots , y_n\}$ were obtained from the same probability distribution,
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} statistic is the maximum difference between two empirical cumulative distribution functions (cumulative frequencies):
      \begin{displaymath}
        D = \sup_i | P(x_i) - P(y_i) |
      \end{displaymath}
      The function \texttt{ks.test()} calculates the \emph{Kolmogorov-Smirnov} statistic and its \emph{p}-value,
      \vskip1ex
      The second argument is either a \texttt{numeric} vector of data values, or a name of a cumulative distribution function,
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} test can be used as a \emph{goodness of fit} test, to test if a set of observations fits a probability distribution,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<>>=
# KS test for normal distribution
ks.test(rnorm(100), pnorm)
# KS test for uniform distribution
ks.test(runif(100), pnorm)
# KS test for two similar normal distributions
ks.test(rnorm(100), rnorm(100, mean=0.1))
# KS test for two different normal distributions
ks.test(rnorm(100), rnorm(100, mean=1.0))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Goodness of Fit Tests}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Goodness of fit} tests are designed to test if a set of observations fits a probability distribution,
      \vskip1ex
      The \emph{Chi-squared} test is designed to test if a frequency distribution (contingency table) fits the specified probability distribution,
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} test is designed to test if two samples were obtained from the same probability distribution,
      \vskip1ex
      The function \texttt{ks.test()} calculates the \emph{Kolmogorov-Smirnov} statistic and its \emph{p}-value,
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# calculate cumulative probabilities and then difference them
prob_s <- pt((histo_gram$breaks-lo_cation)/sc_ale, df=2)
prob_s <- diff(prob_s)
# perform Chi-squared test
chisq.test(histo_gram$counts, p=prob_s, 
  rescale.p=TRUE, simulate.p.value=TRUE)

# calculate sample from t-distribution
sam_ple <- lo_cation + sc_ale*rt(NROW(re_turns), df=2)
# perform Kolmogorov-Smirnov test
ks.test(as.numeric(re_turns), sam_ple)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Performing Aggregations Over Time Series}


%%%%%%%%%%%%%%%
\subsection{Aggregations Over Look-back Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A time \emph{period} is defined as the time between two neighboring points in time, 
      \vskip1ex
      A time \emph{interval} is defined as the time spanned by one or more neighboring time \emph{periods}, 
      \vskip1ex
      A \emph{look-back interval} is a time \emph{interval} for performing aggregations over the past, starting from a \emph{startpoint} and ending at an \emph{endpoint}, 
      \vskip1ex
      The \emph{startpoints} are the \emph{endpoints} lagged by the interval width (number of periods in the interval), 
      \vskip1ex
      The look-back \emph{intervals} may or may not \emph{overlap} with their neighboring intervals, 
    \column{0.5\textwidth}
      A rolling aggregation is specified by a vector of look-back \emph{intervals} at each point in time, 
      \vskip1ex
      An example of a rolling aggregation are moving average prices, 
      \vskip1ex
      An interval aggregation is specified by a vector of look-back \emph{intervals} attached at \emph{endpoints} spanning multiple time \emph{periods}, 
      \vskip1ex
      An example of a non-overlapping interval aggregation are monthly asset returns, 
      \vskip1ex
      An example of an overlapping interval aggregation are trailing 12-month asset returns calculated monthly, 
  \end{columns}
    \vspace{-2em}
    \includegraphics[width=0.9\paperwidth]{figure/rolling_intervals.png}\\
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using \texttt{sapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      Aggregations performed over time series can be extremely slow if done improperly, therefore it's very important to find the fastest methods of performing aggregations, 
      \vskip1ex
      The \texttt{sapply()} functional allows performing aggregations over the look-back \emph{intervals}, 
      \vskip1ex
      The \texttt{sapply()} functional by default returns a vector or matrix, not an \emph{xts} series,
      \vskip1ex
      The vector or matrix returned by \texttt{sapply()} therefore needs to be coerced into an \emph{xts} series,
      \vskip1ex
      The variable \texttt{look\_back} is the size of the look-back interval, equal to the number of data points used for applying the aggregation function (including the current point), 
    \column{0.6\textwidth}
      \vspace{-1em}
      <<TRUE,eval=FALSE>>=
price_s <- Cl(rutils::env_etf$VTI)
end_points <- seq_along(price_s)  # define end points
len_gth <- NROW(end_points)
look_back <- 22  # number of data points per look-back interval
# start_points are multi-period lag of end_points
start_points <- c(rep_len(1, look_back-1), 
    end_points[1:(len_gth-look_back+1)])
# define list of look-back intervals for aggregations over past
look_backs <- lapply(seq_along(end_points), 
  function(in_dex) {
    start_points[in_dex]:end_points[in_dex]
})  # end lapply
# define aggregation function
agg_regate <- function(x_ts) c(max=max(x_ts), min=min(x_ts))
# perform aggregations over look_backs list
agg_regations <- sapply(look_backs, 
    function(look_back) agg_regate(price_s[look_back])
)  # end sapply
# coerce agg_regations into matrix and transpose it
if (is.vector(agg_regations))
  agg_regations <- t(agg_regations)
agg_regations <- t(agg_regations)
# coerce agg_regations into xts series
agg_regations <- xts(agg_regations, 
                     order.by=index(price_s[end_points]))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using \texttt{lapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The \texttt{lapply()} functional allows performing aggregations over the look-back \emph{intervals}, 
      \vskip1ex
      The \texttt{lapply()} functional by default returns a list, not an \emph{xts} series,
      \vskip1ex
      If \texttt{lapply()} returns a list of \emph{xts} series, then this list can be collapsed into a single \emph{xts} series using the function \texttt{do\_call\_rbind()} from package \emph{rutils}, 
      \vskip1ex
      The function \texttt{chart\_Series()} from package \emph{quantmod} can produce a variety of time series plots, 
      \vskip1ex
      \texttt{chart\_Series()} plots can be modified by modifying \emph{plot objects} or \emph{theme objects},
      \vskip1ex
      A plot \emph{theme object} is a list containing parameters that determine the plot appearance (colors, size, fonts),
      \vskip1ex
      The function \texttt{chart\_theme()} returns the theme object, 
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(HighFreq)  # load package HighFreq
# perform aggregations over look_backs list
agg_regations <- lapply(look_backs, 
    function(look_back) agg_regate(price_s[look_back])
)  # end lapply
# rbind list into single xts or matrix
agg_regations <- rutils::do_call_rbind(agg_regations)
# convert into xts
agg_regations <- xts::xts(agg_regations, 
    order.by=index(price_s))
agg_regations <- cbind(agg_regations, price_s)
# plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red", "green")
x11()
chart_Series(agg_regations, theme=plot_theme, 
             name="price aggregations")
legend("top", legend=colnames(agg_regations), 
  bg="white", lty=c(1, 1, 1), lwd=c(6, 6, 6), 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining Functionals for Rolling Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The functional \texttt{roll\_agg()} performs rolling aggregations of its function argument \texttt{FUN}, over an \emph{xts} series (\texttt{x\_ts}), and a look-back interval (\texttt{look\_back}),
      \vskip1ex
      The argument \texttt{FUN} is an aggregation function over a subset of \texttt{x\_ts} series, 
      \vskip1ex
      The dots \texttt{"..."} argument is passed into \texttt{FUN} as additional arguments,
      \vskip1ex
      The argument \texttt{look\_back} is equal to the number of periods of \texttt{x\_ts} series which are passed to the aggregation function \texttt{FUN}, 
      \vskip1ex
      The functional \texttt{roll\_agg()} calls \texttt{lapply()}, which loops over the length of series \texttt{x\_ts}, 
      \vskip1ex
      Note that two different intervals may be used with \texttt{roll\_agg()},
      \vskip1ex
      The first interval is the argument \texttt{look\_back}, 
      \vskip1ex
      A second interval may be one of the variables bound to the dots \texttt{"..."} argument, and passed to the aggregation function \texttt{FUN} (for example, an \emph{EWMA} window), 
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# define functional for rolling aggregations
roll_agg <- function(x_ts, look_back, FUN, ...) {
# define end points at every period
  end_points <- seq_along(x_ts)
  len_gth <- NROW(end_points)
# define starting points as lag of end_points
  start_points <- c(rep_len(1, look_back-1), 
    end_points[1:(len_gth-look_back+1)])
# define list of look-back intervals for aggregations over past
look_backs <- lapply(seq_along(end_points), 
  function(in_dex) {
    start_points[in_dex]:end_points[in_dex]
})  # end lapply
# perform aggregations over look_backs list
  agg_regations <- lapply(look_backs, 
    function(look_back) FUN(x_ts[look_back], ...)
  )  # end lapply
# rbind list into single xts or matrix
  agg_regations <- rutils::do_call_rbind(agg_regations)
# coerce agg_regations into xts series
  if (!is.xts(agg_regations))
    agg_regations <- xts(agg_regations, order.by=index(x_ts))
  agg_regations
}  # end roll_agg
# define aggregation function
agg_regate <- function(x_ts)
  c(max=max(x_ts), min=min(x_ts))
# perform aggregations over rolling interval
agg_regations <- roll_agg(price_s, look_back=look_back, 
                    FUN=agg_regate)
class(agg_regations)
dim(agg_regations)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Benchmarking Speed of Rolling Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The speed of rolling aggregations using \texttt{apply()} loops can be greatly increased by simplifying the aggregation function, 
      \vskip1ex
      For example, an aggregation function that returns a vector is over \texttt{13} times faster than a function that returns an \emph{xts} object, 
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# define aggregation function that returns a vector
agg_vector <- function(x_ts)
  c(max=max(x_ts), min=min(x_ts))
# define aggregation function that returns an xts
agg_xts <- function(x_ts)
  xts(t(c(max=max(x_ts), min=min(x_ts))), 
      order.by=end(x_ts))
# benchmark the speed of aggregation functions
library(microbenchmark)
summary(microbenchmark(
  agg_vector=roll_agg(price_s, look_back=look_back,
                    FUN=agg_vector),
  agg_xts=roll_agg(price_s, look_back=look_back,
                    FUN=agg_xts),
  times=10))[, c(1, 4, 5)]
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Benchmarking Functionals for Rolling Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.45\textwidth}
      Several packages contain functionals designed for performing rolling aggregations:
      \begin{itemize}
        \item \texttt{rollapply.zoo()} from package \emph{zoo},
        \item \texttt{rollapply.xts()} from package \emph{xts},
        \item \texttt{apply.rolling()} from package \emph{PerformanceAnalytics},
      \end{itemize}
      These functionals don't require specifying the \emph{endpoints}, and instead calculate the \emph{endpoints} from the rolling interval width, 
      \vskip1ex
      These functionals can only apply functions that return a single value, not a vector, 
      \vskip1ex
      These functionals return an \emph{xts} series with leading \texttt{NA} values at points before the rolling interval can fit over the data, 
      \vskip1ex
      The argument \texttt{align="right"} of \texttt{rollapply()} determines that aggregations are taken from the past,
      \vskip1ex
      The functional \texttt{rollapply.xts} is the fastest, about as fast as performing an \texttt{lapply()} loop directly, 
    \column{0.55\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# define aggregation function that returns a single value
agg_regate <- function(x_ts)  max(x_ts)
# perform aggregations over a rolling interval
agg_regations <- xts:::rollapply.xts(price_s, width=look_back, 
                    FUN=agg_regate, align="right")
# perform aggregations over a rolling interval
library(PerformanceAnalytics)  # load package PerformanceAnalytics
agg_regations <- apply.rolling(price_s, 
                    width=look_back, FUN=agg_regate)
# benchmark the speed of the functionals
library(microbenchmark)
summary(microbenchmark(
  roll_agg=roll_agg(price_s, look_back=look_back,
                    FUN=max),
  roll_xts=xts:::rollapply.xts(price_s, width=look_back, 
                       FUN=max, align="right"), 
  apply_rolling=apply.rolling(price_s, 
                              width=look_back, FUN=max), 
  times=10))[, c(1, 4, 5)]
@
  \end{columns}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%
\subsection{Rolling Aggregations Using \protect\emph{Vectorized} Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The generic functions \texttt{cumsum()}, \texttt{cummax()}, and \texttt{cummin()} return the cumulative sums, minima, and maxima of \emph{vectors} and \emph{time series} objects,
      \vskip1ex
      The methods for these functions are implemented as \emph{vectorized compiled} functions, and are therefore much faster than \texttt{apply()} loops, 
      \vskip1ex
      The \texttt{cumsum()} function can be used to efficiently calculate the rolling sum of an an \emph{xts} series, 
      \vskip1ex
      Using the function \texttt{cumsum()} is over \texttt{25} times faster than using \texttt{apply()} loops, 
      \vskip1ex
      But rolling standard deviations and higher moments can't be easily calculated using \texttt{cumsum()}, 
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# rolling sum using cumsum()
roll_sum <- function(x_ts, look_back) {
  cum_sum <- cumsum(na.omit(x_ts))
  out_put <- cum_sum - lag(x=cum_sum, k=look_back)
  out_put[1:look_back, ] <- cum_sum[1:look_back, ]
  colnames(out_put) <- paste0(colnames(x_ts), "_stdev")
  out_put
}  # end roll_sum
agg_regations <- roll_sum(price_s, look_back=look_back)
# define list of look-back intervals for aggregations over past
look_backs <- lapply(seq_along(end_points), 
  function(in_dex) {
    start_points[in_dex]:end_points[in_dex]
})  # end lapply
# perform rolling aggregations using apply loop
agg_regations <- sapply(look_backs, 
    function(look_back) sum(price_s[look_back])
)  # end sapply
head(agg_regations)
tail(agg_regations)
# benchmark the speed of both methods
library(microbenchmark)
summary(microbenchmark(
  roll_sum=roll_sum(price_s, look_back=look_back),
  s_apply=sapply(look_backs, 
    function(look_back) sum(price_s[look_back])), 
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using Package \protect\emph{TTR}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{TTR} contains functions for calculating rolling aggregations over \emph{vectors} and \emph{time series} objects: 
      \begin{itemize}
        \item \texttt{runSum()} for rolling sums,
        \item \texttt{runMin()} and \texttt{runMax()} for rolling minima and maxima, 
        \item \texttt{runSD()} for rolling standard deviations,
        \item \texttt{runMedian()} and \texttt{runMAD()} for rolling medians and Median Absolute Deviations (MAD), 
        \item \texttt{runCor()} for rolling correlations,
      \end{itemize}
      The rolling \emph{TTR} functions are much faster than performing \texttt{apply()} loops, because they are \emph{compiled} functions (compiled from \texttt{C++} or \texttt{Fortran} code),
      \vskip1ex
      But the rolling \emph{TTR} functions are a little slower than using \emph{vectorized compiled} functions such as \texttt{cumsum()}, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# library(TTR)  # load package TTR
# benchmark the speed of TTR::runSum
library(microbenchmark)
summary(microbenchmark(
  cum_sum=cumsum(coredata(price_s)), 
  roll_sum=rutils::roll_sum(price_s, win_dow=look_back),
  run_sum=TTR::runSum(price_s, n=look_back),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling \protect\emph{Weighted} Aggregations Using Package \protect\emph{RcppRoll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{RcppRoll} contains functions for calculating \emph{weighted} rolling aggregations over \emph{vectors} and \emph{time series} objects: 
      \begin{itemize}
        \item \texttt{roll\_sum()} for \emph{weighted} rolling sums,
        \item \texttt{roll\_min()} and \texttt{roll\_max()} for \emph{weighted} rolling minima and maxima, 
        \item \texttt{roll\_sd()} for \emph{weighted} rolling standard deviations,
        \item \texttt{roll\_median()} for \emph{weighted} rolling medians, 
      \end{itemize}
      The \emph{RcppRoll} functions accept \emph{xts} objects, but they return matrices, not \emph{xts} objects, 
      \vskip1ex
      The rolling \emph{RcppRoll} functions are much faster than performing \texttt{apply()} loops, because they are \emph{compiled} functions (compiled from \texttt{C++} code),
      \vskip1ex
      But the rolling \emph{RcppRoll} functions are a little slower than using \emph{vectorized compiled} functions such as \texttt{cumsum()}, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(RcppRoll)  # load package RcppRoll
wid_th <- 22  # number of data points per look-back interval
# calculate rolling sum using rutils
prices_mean <- 
  rutils::roll_sum(price_s, win_dow=wid_th)
# calculate rolling sum using RcppRoll
prices_mean <- RcppRoll::roll_sum(price_s, 
                    align="right", n=wid_th)
# benchmark the speed of RcppRoll::roll_sum
library(microbenchmark)
summary(microbenchmark(
  cum_sum=cumsum(coredata(price_s)), 
  rcpp_roll_sum=RcppRoll::roll_sum(price_s, n=wid_th),
  roll_sum=rutils::roll_sum(price_s, win_dow=wid_th),
  times=10))[, c(1, 4, 5)]
# calculate EWMA sum using RcppRoll
weight_s <- exp(0.1*1:wid_th)
prices_mean <- RcppRoll::roll_mean(price_s, 
      align="right", n=wid_th, weights=weight_s)
prices_mean <- cbind(price_s, 
  rbind(coredata(price_s[1:(look_back-1), ]), prices_mean))
colnames(prices_mean) <- c("SPY", "SPY EWMA")
# plot EWMA prices with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red")
x11()
chart_Series(prices_mean, theme=plot_theme, 
             name="EWMA prices")
legend("top", legend=colnames(prices_mean), 
       bg="white", lty=c(1, 1), lwd=c(6, 6), 
       col=plot_theme$col$line.col, bty="n")
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using Package \protect\emph{caTools}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{caTools} contains functions for calculating rolling interval aggregations over a \texttt{vector} of data:
      \begin{itemize}
        \item \texttt{runmin} and \texttt{runmax} for rolling minima and maxima, 
        \item \texttt{runsd()} for rolling standard deviations,
        \item \texttt{runmad()} for rolling Median Absolute Deviations (MAD),
        \item \texttt{runquantile()} for rolling quantiles,
      \end{itemize}
      Time series need to be coerced to \emph{vectors} before they are passed to \emph{caTools} functions,
      \vskip1ex
      The rolling \emph{caTools} functions are very fast because they are \emph{compiled} functions (compiled from \texttt{C++} code),
      \vskip1ex
      The argument \texttt{"endrule"} determines how the end values of the data are treated,
      \vskip1ex
      The argument \texttt{"align"} determines whether the interval is centered (default), left-aligned or right-aligned, with \texttt{align="center"} the fastest option,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
library(caTools)  # load package "caTools"
# get documentation for package "caTools"
packageDescription("caTools")  # get short description
help(package="caTools")  # load help page
data(package="caTools")  # list all datasets in "caTools"
ls("package:caTools")  # list all objects in "caTools"
detach("package:caTools")  # remove caTools from search path
# median filter
look_back <- 11
price_s <- Cl(HighFreq::SPY["2012-02-01/2012-04-01"])
med_ian <- runmed(x=price_s, k=look_back)
# vector of rolling volatility
vol_at <- runsd(x=price_s, k=look_back, 
                endrule="constant", align="center")
# vector of rolling quantiles
quan_tiles <- runquantile(x=price_s, 
                  k=look_back, probs=0.9, 
                  endrule="constant", 
                  align="center")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining Equally Spaced \protect\emph{Endpoints} of a Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Endpoints} are a vector of indices that divide a time series into non-overlapping intervals,  
      \vskip1ex
      \emph{Endpoints} may be specified as integers or as date-time objects, 
      \vspace{-1em}
      <<chart_Series_endp,echo=TRUE,eval=FALSE>>=
library(HighFreq)  # load package HighFreq
# extract daily closing VTI prices
price_s <- Cl(rutils::env_etf$VTI)
# define number of data points per interval
look_back <- 22
# number of look_backs that fit over price_s
n_row <- NROW(price_s)
num_agg <- n_row %/% look_back
# if n_row==look_back*num_agg then whole number 
# of look_backs fit over price_s
end_points <- (1:num_agg)*look_back
# if (n_row > look_back*num_agg) 
# then stub interval at beginning
end_points <- 
  n_row-look_back*num_agg + (0:num_agg)*look_back
# stub interval at end
end_points <- c((1:num_agg)*look_back, n_row)
# plot data and endpoints as vertical lines
plot_theme <- chart_theme()
plot_theme$col$line.col <- "blue"
chart_Series(price_s, theme=plot_theme, 
  name="prices with endpoints as vertical lines")
abline(v=end_points, col="red")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/chart_Series_endp.png}\\
      \vskip1ex
      \emph{Endpoints} may be equally spaced, with a fixed number of data points between neighboring \emph{endpoints}, 
      \vskip1ex
      \emph{Endpoints} start at \texttt{0} to allow the same number of data points in each equally spaced interval, 
      \vskip1ex
      If all the data points don't fit into a whole number of intervals, then a stub interval is needed to fit the remaining data points, 
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Determining Calendar \protect\emph{Endpoints} of \protect\emph{xts} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{endpoints()} from package \emph{xts} extracts the indices of the last observations in each calendar period of time of an \emph{xts} series,
      \vskip1ex
      For example:\\ \-\ \texttt{endpoints(x, on="hours")}\\
      extracts the indices of the last observations in each hour,
      \vskip1ex
      The \emph{endpoints} calculated by \texttt{endpoints()} aren't always equally spaced, and aren't the same as those calculated from fixed intervals, 
      \vskip1ex
      For example, the last observations in each day aren't equally spaced due to weekends and holidays, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# indices of last observations in each hour
end_points <- xts::endpoints(price_s, on="hours")
head(end_points)
# extract the last observations in each hour
head(price_s[end_points, ])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Non-overlapping Aggregations Using \texttt{sapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The \texttt{apply()} functionals allow for applying a function over intervals of an \emph{xts} series defined by a vector of \emph{endpoints},
      \vskip1ex
      The \texttt{sapply()} functional by default returns a vector or matrix, not an \emph{xts} series,
      \vskip1ex
      The vector or matrix returned by \texttt{sapply()} therefore needs to be coerced into an \emph{xts} series,
      \vskip1ex
      The function \texttt{chart\_Series()} from package \emph{quantmod} can produce a variety of time series plots, 
      \vskip1ex
      \texttt{chart\_Series()} plots can be modified by modifying \emph{plot objects} or \emph{theme objects},
      \vskip1ex
      A plot \emph{theme object} is a list containing parameters that determine the plot appearance (colors, size, fonts),
      \vskip1ex
      The function \texttt{chart\_theme()} returns the theme object, 
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
end_points <- # define end_points with beginning stub
  n_row-look_back*num_agg + (0:num_agg)*look_back
len_gth <- NROW(end_points)
# start_points are single-period lag of end_points
start_points <- c(1, end_points[1:(len_gth-1)]+1)
# define list of look-back intervals for aggregations over past
look_backs <- lapply(seq_along(end_points), 
  function(in_dex) {
    start_points[in_dex]:end_points[in_dex]
})  # end lapply
look_backs[[1]]
look_backs[[2]]
# perform sapply() loop over look_backs list
agg_regations <- sapply(look_backs, 
    function(look_back) {
      x_ts <- price_s[look_back]
      c(max=max(x_ts), min=min(x_ts))
  })  # end sapply
# coerce agg_regations into matrix and transpose it
if (is.vector(agg_regations))
  agg_regations <- t(agg_regations)
agg_regations <- t(agg_regations)
# coerce agg_regations into xts series
agg_regations <- xts(agg_regations, 
    order.by=index(price_s[end_points]))
head(agg_regations)
# plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("red", "green")
chart_Series(agg_regations, theme=plot_theme, 
             name="price aggregations")
legend("top", legend=colnames(agg_regations), 
  bg="white", lty=c(1, 1), lwd=c(6, 6), 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Non-overlapping Aggregations Using \texttt{lapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The \texttt{apply()} functionals allow for applying a function over intervals of an \emph{xts} series defined by a vector of \emph{endpoints},
      \vskip1ex
      The \texttt{lapply()} functional by default returns a list, not an \emph{xts} series,
      \vskip1ex
      If \texttt{lapply()} returns a list of \emph{xts} series, then this list can be collapsed into a single \emph{xts} series using the function \texttt{do\_call\_rbind()} from package \emph{rutils}, 
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# perform lapply() loop over look_backs list
agg_regations <- lapply(look_backs, 
    function(look_back) {
      x_ts <- price_s[look_back]
      c(max=max(x_ts), min=min(x_ts))
    })  # end lapply
# rbind list into single xts or matrix
agg_regations <- rutils::do_call_rbind(agg_regations)
# coerce agg_regations into xts series
agg_regations <- xts(agg_regations, 
    order.by=index(price_s[end_points]))
head(agg_regations)
# plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("red", "green")
chart_Series(agg_regations, theme=plot_theme, 
             name="price aggregations")
legend("top", legend=colnames(agg_regations), 
  bg="white", lty=c(1, 1), lwd=c(6, 6), 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Interval Aggregations Using \texttt{period.apply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The functional \texttt{period.apply()} from package \emph{xts} performs \emph{aggregations} over non-overlapping intervals of an \emph{xts} series defined by a vector of \emph{endpoints}, 
      \vskip1ex
      Internally \texttt{period.apply()} performs an \texttt{sapply()} loop, and is therefore about as fast as an \texttt{sapply()} loop, 
      \vskip1ex
      The package \emph{xts} also has several specialized functionals for aggregating data over \emph{endpoints}:
      \begin{itemize}
        \item \texttt{period.sum()} calculate the sum for each period,
        \item \texttt{period.max()} calculate the maximum for each period,
        \item \texttt{period.min()} calculate the minimum for each period,
        \item \texttt{period.prod()} calculate the product for each period,
      \end{itemize}
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# define functional for rolling aggregations over end_points
roll_agg <- function(x_ts, end_points, FUN, ...) {
  len_gth <- NROW(end_points)
# start_points are single-period lag of end_points
  start_points <- c(1, end_points[1:(len_gth-1)]+1)
# perform aggregations over look_backs list
  agg_regations <- lapply(look_backs, 
    function(look_back) FUN(x_ts[look_back], ...))  # end lapply
# rbind list into single xts or matrix
  agg_regations <- rutils::do_call_rbind(agg_regations)
  if (!is.xts(agg_regations))
    agg_regations <-  # coerce agg_regations into xts series
    xts(agg_regations, order.by=index(x_ts[end_points]))
  agg_regations
}  # end roll_agg
# apply sum() over end_points
agg_regations <- 
  roll_agg(price_s, end_points=end_points, FUN=sum)
agg_regations <- 
  period.apply(price_s, INDEX=end_points, FUN=sum)
# benchmark the speed of aggregation functions
summary(microbenchmark(
  roll_agg=roll_agg(price_s, end_points=end_points, FUN=sum),
  period_apply=period.apply(price_s, INDEX=end_points, FUN=sum),
  times=10))[, c(1, 4, 5)]
agg_regations <- period.sum(price_s, INDEX=end_points)
head(agg_regations)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Aggregations of \protect\emph{xts} Over Calendar Periods}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{xts} has convenience wrapper functionals for \texttt{period.apply()}, that apply functions over calendar periods:
      \begin{itemize}
        \item \texttt{apply.daily()} applies functions over daily periods,
        \item \texttt{apply.weekly()} applies functions over weekly periods,
        \item \texttt{apply.monthly()} applies functions over monthly periods,
        \item \texttt{apply.quarterly()} applies functions over quarterly periods,
        \item \texttt{apply.yearly()} applies functions over yearly periods,
      \end{itemize}
      These functionals don't require specifying a vector of \emph{endpoints}, because they determine the \emph{endpoints} from the calendar periods, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# load package HighFreq
library(HighFreq)
# extract closing minutely prices
price_s <- Cl(HighFreq::SPY["2012-02-01/2012-04-01"])
# apply "mean" over daily periods
agg_regations <- apply.daily(price_s, FUN=sum)
head(agg_regations)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Aggregations Over Overlapping Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The functional \texttt{period.apply()} performs aggregations over \emph{non-overlapping} intervals, 
      \vskip1ex
      But it's often necessary to perform aggregations over \emph{overlapping} intervals, defined by a vector of \emph{endpoints} and a \emph{look-back interval}, 
      \vskip1ex
      The \emph{startpoints} are defined as the \emph{endpoints} lagged by the interval width (number of periods in the \emph{look-back interval}), 
      \vskip1ex
      Each point in time has an associated \emph{look-back interval}, which starts at a certain number of periods in the past (\emph{start\_point}) and ends at that point (\emph{end\_point}), 
      \vskip1ex
      The variable \texttt{look\_back} is equal to the number of end points in the \emph{look-back interval}, while (\texttt{look\_back - 1}) is equal to the number of intervals in the look-back, 
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(HighFreq)  # load package HighFreq
end_points <- # define end_points with beginning stub
  n_row-look_back*num_agg + (0:num_agg)*look_back
len_gth <- NROW(end_points)
num_points <- 4  # number of end points in look-back interval
# start_points are multi-period lag of end_points
start_points <- c(rep_len(1, num_points-1), 
  end_points[1:(len_gth-num_points+1)])
# define list of look-back intervals for aggregations over past
look_backs <- lapply(seq_along(end_points), 
  function(in_dex) {
    start_points[in_dex]:end_points[in_dex]
})  # end lapply
# perform lapply() loop over look_backs list
agg_regations <- lapply(look_backs, 
    function(look_back) {
      x_ts <- price_s[look_back]
      c(max=max(x_ts), min=min(x_ts))
    })  # end lapply
# rbind list into single xts or matrix
agg_regations <- rutils::do_call_rbind(agg_regations)
# coerce agg_regations into xts series
agg_regations <- xts(agg_regations, 
    order.by=index(price_s[end_points]))
# plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("red", "green")
chart_Series(agg_regations, theme=plot_theme, 
             name="price aggregations")
legend("top", legend=colnames(agg_regations), 
  bg="white", lty=c(1, 1), lwd=c(6, 6), 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Extending Interval Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Interval aggregations produce values only at the \emph{endpoints}, but they can be carried forward in time using the function \texttt{na.locf()} from package \emph{zoo},
      <<echo=(-(1:2)),eval=FALSE>>=
library(HighFreq)  # load package HighFreq
agg_regations <- cbind(price_s, agg_regations)
tail(agg_regations, 22)
agg_regations <- na.omit(zoo::na.locf(agg_regations))
# plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red", "green")
chart_Series(agg_regations, theme=plot_theme, 
             name="price aggregations")
legend("top", legend=colnames(agg_regations), 
  bg="white", lty=c(1, 1, 1), lwd=c(6, 6, 6), 
  col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/agg_interval_carryfwd.png}\\
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Interval Aggregations of \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The method \texttt{aggregate.zoo()} performs aggregations of \emph{zoo} series over non-overlapping intervals defined by a vector of aggregation groups (minutes, hours, days, etc.), 
      \vskip1ex
      For example, \texttt{aggregate.zoo()} can calculate the average monthly returns, 
      <<echo=(-(1:3)),eval=FALSE>>=
set.seed(1121)  # reset random number generator
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
library(zoo)  # load package zoo
# create zoo time series of random returns
in_dex <- Sys.Date() + 0:365
zoo_series <- 
  zoo(rnorm(NROW(in_dex)), order.by=in_dex)
# create monthly dates
dates_agg <- as.Date(as.yearmon(index(zoo_series)))
# perform monthly mean aggregation
zoo_agg <- aggregate(zoo_series, by=dates_agg, 
                     FUN=mean)
# merge with original zoo - union of dates
zoo_agg <- cbind(zoo_series, zoo_agg)
# replace NA's using locf
zoo_agg <- na.locf(zoo_agg)
# extract aggregated zoo
zoo_agg <- zoo_agg[index(zoo_series), 2]
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/zoo_agg-1}
      \vspace{-7em}
      <<zoo_agg,echo=(-(1:1)),eval=FALSE,fig.show="hide">>=
# library(HighFreq)  # load package HighFreq
# plot original and aggregated cumulative returns
plot(cumsum(zoo_series), xlab="", ylab="")
lines(cumsum(zoo_agg), lwd=2, col="red")
# add legend
legend("topright", inset=0.05, cex=0.8, 
       title="Aggregated Prices", 
       leg=c("orig prices", "agg prices"), 
       lwd=2, bg="white", col=c("black", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Interpolating \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{zoo} has two functions for replacing \texttt{NA} values using interpolation:
      \begin{itemize}
        \item \texttt{na.approx()} performs linear interpolation,
        \item \texttt{na.spline()} performs spline interpolation,
      \end{itemize}
      \vspace{-1em}
      <<zoo_interpol,echo=(-(1:1)),eval=FALSE,fig.show="hide">>=
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# perform monthly mean aggregation
zoo_agg <- aggregate(zoo_series, by=dates_agg, 
                     FUN=mean)
# merge with original zoo - union of dates
zoo_agg <- cbind(zoo_series, zoo_agg)
# replace NA's using linear interpolation
zoo_agg <- na.approx(zoo_agg)
# extract interpolated zoo
zoo_agg <- zoo_agg[index(zoo_series), 2]
# plot original and interpolated zoo
plot(cumsum(zoo_series), xlab="", ylab="")
lines(cumsum(zoo_agg), lwd=2, col="red")
# add legend
legend("topright", inset=0.05, cex=0.8, title="Interpolated Prices", 
       leg=c("orig prices", "interpol prices"), lwd=2, bg="white", 
       col=c("black", "red"))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/zoo_interpol-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Over \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{zoo} has several functions for rolling calculations:
      \begin{itemize}
        \item \texttt{rollapply()} performing aggregations over a rolling (sliding) interval,
        \item \texttt{rollmean()} calculating rolling means,
        \item \texttt{rollmedian()} calculating rolling median,
        \item \texttt{rollmax()} calculating rolling max,
      \end{itemize}
      \vspace{-1em}
      <<zoo_roll,echo=(-(1:1)),eval=FALSE,fig.show="hide">>=
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# "mean" aggregation over interval with width=11
zoo_mean <- rollapply(zoo_series, width=11, 
                      FUN=mean, align="right")
# merge with original zoo - union of dates
zoo_mean <- cbind(zoo_series, zoo_mean)
# replace NA's using na.locf
zoo_mean <- na.locf(zoo_mean, fromLast=TRUE)
# extract mean zoo
zoo_mean <- zoo_mean[index(zoo_series), 2]
# plot original and interpolated zoo
plot(cumsum(zoo_series), xlab="", ylab="")
lines(cumsum(zoo_mean), lwd=2, col="red")
# add legend
legend("topright", inset=0.05, cex=0.8, title="Mean Prices", 
       leg=c("orig prices", "mean prices"), lwd=2, bg="white", 
       col=c("black", "red"))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/zoo_roll-1}
      \vspace{-3em}
      The argument \texttt{align="right"} determines that aggregations are taken from the past,
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Estimating and Modeling Volatility and Skew}


%%%%%%%%%%%%%%%
\subsection{Estimating Rolling Variance Using \texttt{sapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Heteroskedasticity} refers to statistical distributions whose variance changes with time,
      \vskip1ex
      Empirical \emph{time series} of returns are \emph{heteroskedastic} because their variance changes with time,
      \vskip1ex
      The rolling realized variance of a \emph{time series} is a vector given by the estimator:
      \begin{align*}
        \sigma_i^2=\frac{1}{k-1} \sum_{j=0}^{k-1} (r_{i-j}-\bar{r_i})^2\\
        \bar{r_i}=\frac{1}{k}{\sum_{j=0}^{k-1} r_{i-j}}
      \end{align*}
      Where $k$ is the \emph{look-back interval} for performing aggregations over the past, 
      \vskip1ex
      It's not possible to calculate the rolling variance in \texttt{R} using vectorized functions, so it must be calculated using an \texttt{apply()} loop,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# VTI percentage returns
re_turns <- rutils::diff_xts(log(quantmod::Cl(rutils::env_etf$VTI)))
# define end points
end_points <- seq_along(re_turns)
len_gth <- NROW(end_points)
look_back <- 51
# start_points are multi-period lag of end_points
start_points <- c(rep_len(1, look_back-1), 
    end_points[1:(len_gth-look_back+1)])
# define list of look-back intervals for aggregations over past
look_backs <- lapply(seq_along(end_points), 
  function(in_dex) {
    start_points[in_dex]:end_points[in_dex]
})  # end lapply
# calculate realized VTI variance in sapply() loop
vari_ance <- sapply(look_backs, 
  function(look_back) {
    ret_s <- re_turns[look_back]
    sum((ret_s - mean(ret_s))^2)
}) / (look_back-1)  # end sapply
tail(vari_ance)
class(vari_ance)
# coerce vari_ance into xts
vari_ance <- xts(vari_ance, order.by=index(re_turns))
colnames(vari_ance) <- "VTI.variance"
head(vari_ance)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating Rolling Variance Using Package \protect\emph{roll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{roll} contains functions for calculating \emph{weighted} rolling aggregations over \emph{vectors} and \emph{time series} objects: 
      \begin{itemize}
        \item \texttt{roll\_var()} for \emph{weighted} rolling variance,
        \item \texttt{roll\_scale()} for rolling scaling and centering of time series, 
        \item \texttt{roll\_pcr()} for rolling principal component regressions of time series,
      \end{itemize}
      The \emph{roll} functions are about \texttt{1,000} times faster than \texttt{apply()} loops!
      \vskip1ex
      The \emph{roll} functions are extremely fast because they perform calculations in \emph{parallel} in compiled \texttt{C++} code, using package \emph{Rcpp},
      \vskip1ex
      The \emph{roll} functions accept \emph{xts} time series, and they return \emph{xts}, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# calculate VTI variance using package roll
library(roll)  # load roll
vari_ance <- 
  roll::roll_var(re_turns, width=look_back)
colnames(vari_ance) <- "VTI.variance"
head(vari_ance)
sum(is.na(vari_ance))
vari_ance[1:(look_back-1)] <- 0
# benchmark calculation of rolling variance
library(microbenchmark)
summary(microbenchmark(
  roll_sapply=sapply(look_backs, function(look_back) {
    ret_s <- re_turns[look_back]
    sum((ret_s - mean(ret_s))^2)
  }),
  ro_ll=roll::roll_var(re_turns, width=look_back),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling \protect\emph{EWMA} Realized Variance Estimator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Time-varying volatility can be more accurately estimated using an \emph{Exponentially Weighted Moving Average} (\emph{EWMA}) variance estimator, 
      \vskip1ex
      If the \emph{time series} has zero \emph{expected} mean, then the \emph{EWMA} \emph{realized} variance estimator can be written approxiamtely as:
      \begin{displaymath}
        \sigma_i^2 = (1-\lambda) {r_i}^2 + \lambda \sigma_{i-1}^2 = (1-\lambda) \sum_{j=0}^{\infty} \lambda^j {r_{i-j}}^2
      \end{displaymath}
      $\sigma_i^2$ is the weighted \emph{realized} variance, equal to the weighted average of the point realized variance for period $i$ and the past \emph{realized} variance, 
      \vskip1ex
      The parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with smaller values of $\lambda$ producing faster decay, giving more weight to recent realized variance, and vice versa, 
      \vskip1ex
      The function \texttt{filter()} calculates the convolution of a vector or time series with a vector of filter coefficients (weights), 
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vol_ewma.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# calculate EWMA VTI variance using filter()
wid_th <- 51
weight_s <- exp(-0.1*1:wid_th)
weight_s <- weight_s/sum(weight_s)
vari_ance <- stats::filter(re_turns^2, 
    filter=weight_s, sides=1)
vari_ance[1:(wid_th-1)] <- vari_ance[wid_th]
class(vari_ance)
vari_ance <- as.numeric(vari_ance)
x_ts <- xts:::xts(sqrt(vari_ance), order.by=index(re_turns))
# plot EWMA standard deviation
chart_Series(x_ts, 
  name="EWMA standard deviation")
dygraphs::dygraph(x_ts, main="EWMA standard deviation")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating \protect\emph{EWMA} Variance Using Package \protect\emph{roll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the \emph{time series} has non-zero \emph{expected} mean, then the rolling \emph{EWMA} variance is a vector given by the estimator:
      \begin{align*}
        \sigma_i^2=\frac{1}{k-1} \sum_{j=0}^{k-1} {w_j (r_{i-j}-\bar{r_i})^2}\\
        \bar{r_i}=\frac{1}{k}{\sum_{j=0}^{k-1} {w_j r_{i-j}}}
      \end{align*}
      Where $w_j$ is the vector of weights:
      \begin{displaymath}
        w_j = \frac{\lambda^j}{\sum_{j=0}^{k-1} \lambda^j}
      \end{displaymath}
      The function \texttt{roll\_var()} from package \emph{roll} calculates the rolling \emph{EWMA} variance,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# calculate VTI variance using package roll
library(roll)  # load roll
vari_ance <- roll::roll_var(re_turns, 
  weights=rev(weight_s), width=wid_th)
colnames(vari_ance) <- "VTI.variance"
class(vari_ance)
head(vari_ance)
sum(is.na(vari_ance))
vari_ance[1:(wid_th-1)] <- 0
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Variance calculated over non-overlapping intervals has very statistically significant autocorrelations,
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=4)
par(mar=c(4, 3, 1, 1), oma=c(0, 0, 0, 0))
# VTI percentage returns
re_turns <- rutils::diff_xts(log(quantmod::Cl(rutils::env_etf$VTI)))
# calculate VTI variance using package roll
look_back <- 22
vari_ance <- 
  roll::roll_var(re_turns, width=look_back)
vari_ance[1:(look_back-1)] <- 0
colnames(vari_ance) <- "VTI.variance"
# number of look_backs that fit over re_turns
n_row <- NROW(re_turns)
num_agg <- n_row %/% look_back
end_points <- # define end_points with beginning stub
  n_row-look_back*num_agg + (0:num_agg)*look_back
len_gth <- NROW(end_points)
# subset vari_ance to end_points
vari_ance <- vari_ance[end_points]
# improved autocorrelation function
acf_plus(coredata(vari_ance), lag=10, main="")
title(main="acf of variance", line=-1)
# partial autocorrelation
pacf(coredata(vari_ance), lag=10, main="", ylab=NA)
title(main="pacf of variance", line=-1)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/acf_var.png}\\
      \includegraphics[width=0.5\paperwidth]{figure/pacf_var.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Volatility Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH(1,1)} model is a volatility model defined by two coupled equations:
      \begin{align*}
        r_i = \mu + \sigma_{i-1} \varepsilon_i \\
        \sigma_i^2 = \omega + \alpha r_i^2 + \beta \sigma_{i-1}^2
      \end{align*}
      Where $\sigma_i^2$ is the time-dependent variance, equal to the weighted average of the point \emph{realized} variance ${r_{i-1}}^2$, and the past variance $\sigma_{i-1}^2$, 
      \vskip1ex
      The return process $r_i$ follows a normal distribution with time-dependent variance $\sigma_i^2$,
      \vskip1ex
      The parameter $\alpha$ is the weight associated with recent realized variance updates, and $\beta$ is the weight associated with the past variance, 
      \vskip1ex
      The parameter $\omega$ determines the long-term average level of variance, which is given by:
      \begin{displaymath}
        \sigma^2 = \frac{\omega}{1 - \alpha - \beta}
      \end{displaymath}
      The sum of $\alpha$ plus $\beta$ should be less than \texttt{1}, otherwise the volatility is explosive,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# define GARCH parameters
om_ega <- 0.01 ; al_pha <- 0.2
be_ta <- 0.2 ; len_gth <- 1000
re_turns <- numeric(len_gth)
vari_ance <- numeric(len_gth)
vari_ance[1] <- om_ega/(1-al_pha-be_ta)
re_turns[1] <- rnorm(1, sd=sqrt(vari_ance[1]))
# simulate GARCH model
set.seed(1121)  # reset random numbers
for (i in 2:len_gth) {
  re_turns[i] <- rnorm(n=1, sd=sqrt(vari_ance[i-1]))
  vari_ance[i] <- om_ega + al_pha*re_turns[i]^2 + 
    be_ta*vari_ance[i-1]
}  # end for
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Volatility Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH(1,1)} volatility model exhibits sharp spikes in the volatility, followed by a quick decay of volatility,
      \vskip1ex
      But the decay of volatility in the \emph{GARCH} model is faster than what is observed in practice,
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=4)
par(mar=c(3, 3, 1, 1), oma=c(0, 0, 0, 0))
# plot GARCH cumulative returns
plot(cumsum(re_turns/100), t="l", 
  lwd=2, col="orange", xlab="", ylab="",
  main="GARCH cumulative returns")
date_s <- seq.Date(from=Sys.Date()-len_gth+1, 
  to=Sys.Date(), length.out=len_gth)
x_ts <- xts:::xts(cumsum(re_turns/100), order.by=date_s)
dygraphs::dygraph(x_ts, main="GARCH cumulative returns")
# plot GARCH standard deviation
plot(sqrt(vari_ance), t="l", 
  col="orange", xlab="", ylab="",
  main="GARCH standard deviation")
x_ts <- xts:::xts(sqrt(vari_ance), order.by=date_s)
dygraphs::dygraph(x_ts, main="GARCH standard deviation")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/garch_returns.png}\\
      \includegraphics[width=0.5\paperwidth]{figure/garch_stdev.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Model Properties}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The parameter $\alpha$ is the weight of the squared realized returns in the variance, 
      \vskip1ex
      Greater values of $\alpha$ produce a stronger feedback between the realized returns and variance, causing stronger variance spikes and higher kurtosis,
      <<echo=TRUE,eval=FALSE>>=
# define GARCH parameters
om_ega <- 0.0001 ; al_pha <- 0.5
be_ta <- 0.1 ; len_gth <- 10000
re_turns <- numeric(len_gth)
vari_ance <- numeric(len_gth)
vari_ance[1] <- om_ega/(1-al_pha-be_ta)
re_turns[1] <- rnorm(1, sd=sqrt(vari_ance[1]))
# simulate GARCH model
set.seed(1121)  # reset random numbers
for (i in 2:len_gth) {
  re_turns[i] <- rnorm(n=1, sd=sqrt(vari_ance[i-1]))
  vari_ance[i] <- om_ega + al_pha*re_turns[i]^2 + 
    be_ta*vari_ance[i-1]
}  # end for
# calculate kurtosis of GARCH returns
moments::moment(re_turns, order=4) / 
  moments::moment(re_turns, order=2)^2
# perform Jarque-Bera test of normality
tseries::jarque.bera.test(re_turns)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/garch_hist.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot histogram of GARCH returns
histo_gram <- hist(re_turns, col="lightgrey",
  xlab="returns", breaks=200, xlim=c(-0.05, 0.05),
  ylab="frequency", freq=FALSE,
  main="GARCH returns histogram")
lines(density(re_turns, adjust=1.5),
      lwd=3, col="blue")
optim_fit <- MASS::fitdistr(re_turns, 
  densfun="t", df=2, lower=c(-1, 1e-7))
lo_cation <- optim_fit$estimate[1]
sc_ale <- optim_fit$estimate[2]
curve(expr=dt((x-lo_cation)/sc_ale, df=2)/sc_ale,
  type="l", xlab="", ylab="", lwd=3,
  col="red", add=TRUE)
legend("topright", inset=0.05,
       leg=c("density", "t-distr w/ 2 dof"),
       lwd=6, lty=c(1, 1),
       col=c("blue", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Model Calibration}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{GARCH} models can be calibrated on returns using the \emph{maximum-likelihood} method, but it's a complex optimization procedure,
      \vskip1ex
      The package \emph{fGarch} contains functions for applying \emph{GARCH} models,
      \vskip1ex
      The function \texttt{garchFit()} calibrates a \emph{GARCH} model on a time series of returns,
      \vskip1ex
      The function \texttt{garchFit()} returns an \texttt{S4} object of class \emph{fGARCH}, with multiple slots containing the \emph{GARCH} model outputs and diagnostic information,
      <<echo=(-(1:2)),eval=FALSE>>=
# use fixed notation instead of exponential notation
options(scipen=999)
library(fGarch)
# fit returns into GARCH
garch_fit <- fGarch::garchFit(data=re_turns)
# fitted GARCH parameters
round(garch_fit@fit$coef, 5)
# actual GARCH parameters
round(c(mu=mean(re_turns), omega=om_ega, 
  alpha=al_pha, beta=be_ta), 5)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/garch_fGarch_fitted.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# plot GARCH fitted standard deviation
plot.zoo(sqrt(garch_fit@fit$series$h), t="l", 
  col="orange", xlab="", ylab="",
  main="GARCH fitted standard deviation")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Model Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{garchSpec()} from package \emph{fGarch} specifies a \emph{GARCH} model,
      \vskip1ex
      The function \texttt{garchSim()} simulates a \emph{GARCH} model,
      <<echo=TRUE,eval=FALSE>>=
# specify GARCH model
garch_spec <- fGarch::garchSpec(
  model=list(omega=om_ega, alpha=al_pha, beta=be_ta))
# simulate GARCH model
garch_sim <- 
  fGarch::garchSim(spec=garch_spec, n=len_gth)
re_turns <- as.numeric(garch_sim)
# calculate kurtosis of GARCH returns
moments::moment(re_turns, order=4) / 
  moments::moment(re_turns, order=2)^2
# perform Jarque-Bera test of normality
tseries::jarque.bera.test(re_turns)
# plot histogram of GARCH returns
histo_gram <- hist(re_turns, col="lightgrey",
  xlab="returns", breaks=200, xlim=c(-0.05, 0.05),
  ylab="frequency", freq=FALSE,
  main="GARCH returns histogram")
lines(density(re_turns, adjust=1.5),
      lwd=3, col="blue")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/garch_fGarch_hist.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# fir t-distribution into GARCH returns
optim_fit <- MASS::fitdistr(re_turns, 
  densfun="t", df=2, lower=c(-1, 1e-7))
lo_cation <- optim_fit$estimate[1]
sc_ale <- optim_fit$estimate[2]
curve(expr=dt((x-lo_cation)/sc_ale, df=2)/sc_ale,
  type="l", xlab="", ylab="", lwd=3,
  col="red", add=TRUE)
legend("topright", inset=0.05,
       leg=c("density", "t-distr w/ 2 dof"),
       lwd=6, lty=c(1, 1),
       col=c("blue", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: \protect\emph{GARCH} Model Forecasting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH(1,1)} model is a volatility forecasting model defined as:
      \begin{displaymath}
        \hspace{-0.5em}\hat\sigma_i^2 = \omega + \alpha {\Delta r_{i-1}}^2 + \beta \hat\sigma_{i-1}^2
      \end{displaymath}
      $\hat\sigma_i^2$ is the \emph{forecasted} variance, equal to the weighted average of the point \emph{realized} variance ${\Delta r_{i-1}}^2$, and the past variance forecast $\hat\sigma_{i-1}^2$, 
      \vskip1ex
      The parameter $\omega$ is related to the long-term average level of variance, $\alpha$ is the weight associated with recent realized variance updates, and $\beta$ is the weight associated with the past variance forecasts, 
      \vskip1ex
      In contrast to the \emph{EWMA} model, The \emph{GARCH} model doesn't depend on the realized variance in the current period $i$, but only on the past period $(i-1)$, 
      \vskip1ex
      The rolling \emph{RcppRoll} functions are much faster than performing \texttt{apply()} loops, because they are \emph{compiled} functions (compiled from \texttt{C++} code),
      \vskip1ex
      But the rolling \emph{RcppRoll} functions are a little slower than using \emph{vectorized compiled} functions such as \texttt{cumsum()}, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(HighFreq)  # load HighFreq
# calculate variance for each period
vari_ance <- 252*(24*60*60)^2*
  HighFreq::run_variance(oh_lc=env_etf$VTI)
# calculate EWMA VTI variance using RcppRoll
library(RcppRoll)  # load RcppRoll
wid_th <- 51
weight_s <- exp(0.1*1:wid_th)
var_ewma <- RcppRoll::roll_mean(vari_ance, 
    align="right", n=wid_th, weights=weight_s)
var_ewma <- xts(var_ewma, 
    order.by=index(env_etf$VTI[-(1:(wid_th-1)), ]))
colnames(var_ewma) <- "VTI variance"
# plot EWMA variance with custom line colors
x11()
chart_Series(env_etf$VTI["2010-01/2010-10"], 
   name="VTI EWMA variance with May 6, 2010 Flash Crash")
# add variance in extra panel
add_TA(var_ewma["2010-01/2010-10"], col="black")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Simulating Mixture of Volatilities}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Geometric Brownian motion} is the basic model for the time evolution of asset prices \emph{S(t)}, 
      \begin{displaymath}
        d \ln S = ( \mu - \frac{\sigma^2}{2} ) dt + \sigma \sqrt{dt} \epsilon
      \end{displaymath}
      Where $\epsilon$ is a random process following the standard normal distribution, 
      \vskip1ex
      The convexity correction $-\frac{\sigma^2}{2}$ ensures that the expected value of prices grows as $\mu t$, (in accordance with Ito's lemma), 
      \vskip1ex
      The prices at any point in time follow the \emph{Log-normal} distribution, which is the exponential of the normal (\emph{Gaussian}) distribution, 
      <<echo=TRUE,eval=FALSE>>=
# extract time index from VTI
in_dex <- index(rutils::env_etf$VTI)
n_row <- NROW(in_dex)
half_length <- n_row %/% 2
# simulate returns with random volatility
set.seed(1121)  # reset random number generator
# specify random volatility with sd=1 and sd=4
vol_at <- 0.01*sample(c(rep(1, half_length), rep(4, n_row-half_length)))
re_turns <- vol_at*rnorm(n_row) - vol_at^2/2
re_turns <- re_turns/sd(re_turns)
re_turns <- xts(re_turns, order.by=in_dex)
# calculate moments
sapply(1:4, FUN=moments::moment, x=re_turns)
# fit distribution using MASS::fitdistr()
optim_fit <- 
  MASS::fitdistr(re_turns, densfun="t", df=2)
# plot returns histogram using PerformanceAnalytics
col_ors <- c("lightgray", "blue", "green", "red")
x11()
PerformanceAnalytics::chart.Histogram(re_turns, 
  main="", ylim=c(0.0, 0.8), col=col_ors[1:3], 
  methods = c("add.density", "add.normal"))
curve(expr=dt((x-optim_fit$estimate[1])/
  optim_fit$estimate[2], df=2)/optim_fit$estimate[2], 
      type="l", xlab="", ylab="", lwd=2,
      col=col_ors[4], add=TRUE)
# add title
title(main="Mixture of volatilities returns histogram", cex.main=1.3, line=-1)
# add legend
lab_els <- c("density", "normal", "t-distr")
legend("topright", inset=0.05, lab_els,
       lwd=2, lty=c(1, 1, 1),
       col=col_ors[2:4])
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/vol_mix.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating Volatility of Intraday Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{close-to-close} estimator depends on \texttt{close} prices specified over the aggregation intervals: 
      \begin{displaymath}
        \hat\sigma^2 = \frac{1}{n-1} \sum_{i=1}^{n} (\log(\frac{C_i}{C_{i-1}})-\bar{r})^2
      \end{displaymath}
      \vspace{-1em}
      \begin{displaymath}
        \bar{r} = \frac{1}{n} \sum_{i=1}^{n} \log(\frac{C_i}{C_{i-1}})
      \end{displaymath}
      Volatility estimates for intraday time series depend both on the units of returns (per second, minute, day, etc.), and on the aggregation interval (secondly, minutely, daily, etc.), 
      \vskip1ex
      A minutely time interval is equal to \texttt{60} seconds, a daily time interval is equal to \texttt{86,400=24*60*60} seconds, etc.), 
      \vskip1ex
      For example, it's possible to measure returns in minutely intervals in units per second, 
      \vskip1ex
      The estimated volatility is directly proportional to the measurement units, 
      \vskip1ex
      For example, the volatility estimated from per minute returns is \texttt{60} times the volatility estimated from per second returns, 
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=TRUE>>=
library(HighFreq)  # load HighFreq
# minutely SPY returns (unit per minute) single day
re_turns <- rutils::diff_xts(log(SPY["2012-02-13", 4]))
# minutely SPY volatility (unit per minute)
sd(re_turns)
# minutely SPY returns (unit per second)
re_turns <- rutils::diff_xts(log(SPY["2012-02-13", 4])) / 
  c(1, diff(.index(SPY["2012-02-13"])))
# minutely SPY volatility scaled to unit per minute
60*sd(re_turns)
# minutely SPY returns multiple days no overnight scaling
re_turns <- rutils::diff_xts(log(SPY[, 4]))
# minutely SPY volatility (unit per minute)
sd(re_turns)
# minutely SPY returns (unit per second)
re_turns <- rutils::diff_xts(log(SPY[, 4])) / 
  c(1, diff(.index(SPY)))
# minutely SPY volatility scaled to unit per minute
60*sd(re_turns)
table(c(1, diff(.index(SPY))))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Volatility as Function of Aggregation Interval}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Return volatility depends on the length of the aggregation time interval approximately as the \emph{square root} of the interval:
      \begin{displaymath}
        \hat\sigma \propto {\Delta t} ^ {H/2}
      \end{displaymath}
      Where $\Delta t$ is the length of the aggregation interval, and \texttt{H} is the \emph{Hurst} exponent,
      \vskip1ex
      If prices follow \texttt{geometric Brownian motion} then the volatility is exactly proportional to the \emph{square root} of the interval length (\texttt{H=1}), 
      \vskip1ex
      If prices are \texttt{mean-reverting} then the volatility grows slower than the \emph{square root} of the interval length (\texttt{H<1}), 
      \vskip1ex
      If prices are \texttt{trending} then the volatility grows faster than the \emph{square root} of the interval length (\texttt{H>1}), 
      \vskip1ex
      The length of the daily time interval is often approximated to be equal to \texttt{390=6.5*60} minutes, since the trading session is equal to \texttt{6.5} hours, and daily volatility is dominated by that of the trading session, 
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-(1:1)),eval=TRUE>>=
library(HighFreq)  # load HighFreq
# daily OHLC SPY prices
SPY_daily <- 
  rutils::to_period(oh_lc=SPY, period="days")
# daily SPY returns and volatility
sd(rutils::diff_xts(log(SPY_daily[, 4])))
# minutely SPY returns (unit per minute)
re_turns <- rutils::diff_xts(log(SPY[, 4]))
# minutely SPY volatility scaled to daily interval
sqrt(6.5*60)*sd(re_turns)

# minutely SPY returns (unit per second)
re_turns <- rutils::diff_xts(log(SPY[, 4])) / 
  c(1, diff(.index(SPY)))
# minutely SPY volatility scaled to daily aggregation interval
60*sqrt(6.5*60)*sd(re_turns)

# daily SPY volatility
# including extra time over weekends and holidays
24*60*60*sd(rutils::diff_xts(log(SPY_daily[, 4])) / 
            c(1, diff(.index(SPY_daily))))
table(c(1, diff(.index(SPY_daily))))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Range} Volatility Estimators of \texttt{OHLC} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Range} volatility estimators utilize the \texttt{high} and \texttt{low} prices, and therefore have lower standard error than the standard \emph{close-to-close} estimator, 
      \vskip1ex
      The \emph{Garman-Klass} estimator uses the \emph{low-to-high} price range, but it underestimates volatility because it doesn't account for \emph{close-to-open} price jumps:
      \begin{displaymath}
        \hat\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (0.5\log(\frac{H_i}{L_i})^2 - (2\log2-1)\log(\frac{C_i}{O_i})^2)
      \end{displaymath}
      The \emph{Yang-Zhang} estimator is the most efficient (has the lowest standard error) among unbiased estimators, and also accounts for \emph{close-to-open} price jumps: 
      \vspace{-1em}
      \begin{multline*}
        \hspace{-1em}\hat\sigma^2 = \frac{1}{n-1} \sum_{i=1}^{n} (\log(\frac{O_i}{C_{i-1}})-\bar{r}_{co})^2 + \\
        0.134(\log(\frac{C_i}{O_i})-\bar{r}_{oc})^2 + \\
        \frac{0.866}{n} \sum_{i=1}^{n} (\log(\frac{H_i}{O_i})\log(\frac{H_i}{C_i}) + \log(\frac{L_i}{O_i})\log(\frac{L_i}{C_i}))
      \end{multline*}
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-(1:1)),eval=FALSE>>=
library(HighFreq)  # load HighFreq
# daily SPY volatility from minutely prices using package TTR
library(TTR)
sqrt((6.5*60)*mean(na.omit(
  TTR::volatility(SPY, N=1, 
                  calc="yang.zhang"))^2))
# SPY volatility using package HighFreq
60*sqrt((6.5*60)*agg_regate(oh_lc=SPY, 
            weight_ed=FALSE, mo_ment="run_variance", 
            calc_method="yang_zhang"))
      @
      \vspace{-1em}
      Theoretically, the \emph{Yang-Zhang} (\emph{YZ}) and \emph{Garman-Klass-Yang-Zhang} (\emph{GKYZ}) range variance estimators are unbiased and have up to seven times smaller standard errors than the standard close-to-close estimator, 
      \vskip1ex
      But in practice, prices are not observed continuously, so the price range is underestimated, and so is the variance when using the \emph{YZ} and \emph{GKYZ} range estimators, 
      \vskip1ex
      Therefore in practice the \emph{YZ} and \emph{GKYZ} range estimators underestimate volatility, 
      \vskip1ex
      In addition, their standard errors are reduced less than by the theoretical amount, for the same reason, 
      \vskip1ex
      The \emph{Garman-Klass-Yang-Zhang} estimator is another very efficient and unbiased estimator, and also accounts for \emph{close-to-open} price jumps: 
      \vspace{-1em}
      \begin{multline*}
        \hat\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} ((\log(\frac{O_i}{C_{i-1}})-\bar{r})^2 + \\
        0.5\log(\frac{H_i}{L_i})^2 - (2\log2-1)(\log(\frac{C_i}{O_i})^2))
      \end{multline*}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Comparing \protect\emph{Range} Volatility Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Range} volatility estimators follow the standard \emph{Close-to-Close} estimator, except in intervals of high intra-period volatility,
      <<echo=TRUE,eval=FALSE>>=
library(HighFreq)  # load HighFreq
# calculate variance
var_close <- 
  HighFreq::run_variance(oh_lc=env_etf$VTI, 
                         calc_method="close")
var_yang_zhang <- 
  HighFreq::run_variance(oh_lc=env_etf$VTI)
vari_ance <- 
  252*(24*60*60)^2*cbind(var_close, var_yang_zhang)
colnames(vari_ance) <- 
  c("close var", "Yang-Zhang var")
# plot
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red")
x11()
chart_Series(vari_ance["2011-06/2011-12"], 
  theme=plot_theme, name="Close and YZ variances")
legend("top", legend=colnames(vari_ance),
       bg="white", lty=c(1, 1), lwd=c(6, 6),
       col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/close_YZ_vol.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Alternative \protect\emph{Range} Volatility Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An alternative \emph{Range} volatility estimator can be created by calculating the logarithm of the range, (as opposed to the range percentage, or the logarithm of the price ratios), 
      \begin{displaymath}
        \hat\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} \log(\frac{H_i - L_i}{H_i + L_i})^2
      \end{displaymath}
      The range logarithm fits better into the normal distribution than the range percentage, 
      <<echo=TRUE,eval=FALSE>>=
re_turns <- 
  ifelse(env_etf$VTI[, 2] > env_etf$VTI[, 3], 
  log((env_etf$VTI[, 2] - env_etf$VTI[, 3]) / 
    (env_etf$VTI[, 2] + env_etf$VTI[, 3])), 0)
# perform normality tests
shapiro.test(coredata(re_turns))
tseries::jarque.bera.test(re_turns)
# fit distribution using MASS::fitdistr()
optim_fit <- MASS::fitdistr(re_turns, 
                  densfun="t", df=2)
optim_fit$estimate; optim_fit$sd
# calculate moments of standardized returns
sapply(3:4, moments::moment, 
       x=(re_turns - mean(re_turns))/sd(re_turns))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/log_range.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot histogram of VTI returns
col_ors <- c("lightgray", "blue", "green", "red")
PerformanceAnalytics::chart.Histogram(re_turns, 
  main="", xlim=c(-7, -3), col=col_ors[1:3], 
  methods = c("add.density", "add.normal"))
curve(expr=dt((x-optim_fit$estimate[1])/
  optim_fit$estimate[2], df=2)/optim_fit$estimate[2], 
      type="l", xlab="", ylab="", lwd=2,
      col=col_ors[4], add=TRUE)
# add title and legend
title(main="VTI logarithm of range", 
      cex.main=1.3, line=-1)
legend("topright", inset=0.05, 
  legend=c("density", "normal", "t-distr"),
  lwd=2, lty=c(1, 1, 1), col=col_ors[2:4])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelations of Alternative \protect\emph{Range} Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The logarithm of the range exhibits very significant autocorrelations, unlike the range percentage, 
      <<echo=TRUE,eval=FALSE>>=
# VTI range variance partial autocorrelations
pacf(re_turns^2, lag=10, xlab=NA, ylab=NA, 
     main="PACF of VTI log range")
chart_Series(re_turns^2, 
             name="VTI log of range squared")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pacf_log_range.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Volatility Estimators Using Bootstrap}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard errors of estimators can be calculated using a \emph{bootstrap} simulation,
      \vskip1ex
      The \emph{bootstrap} procedure generates new data by randomly sampling with replacement from the observed data set,
      \vskip1ex
      The \emph{bootstrapped} data is then used to re-calculate the estimator many times, producing a vector of values,
      \vskip1ex
      The \emph{bootstrapped} estimator values can then be used to calculate the probability distribution of the estimator and its standard error,
      \vskip1ex
      Bootstrapping doesn't provide accurate estimates for estimators that are sensitive to the ordering and correlations in the data, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# standard errors of TTR variance estimators using bootstrap
boot_strap <- sapply(1:100, function(x) {
# create random OHLC
  oh_lc <- HighFreq::random_ohlc()
# calculate variance estimate
  sqrt((6.5*60)*mean(na.omit(
    TTR::volatility(oh_lc, N=1, 
                    calc="yang.zhang"))^2))
})  # end sapply
# analyze bootstrapped variance
head(boot_strap)
sum(is.na(boot_strap))
apply(boot_strap, MARGIN=2, mean)
apply(boot_strap, MARGIN=2, sd)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelations of \protect\emph{Close-to-Close} and \protect\emph{Range} Variances}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard \emph{Close-to-Close} estimator exhibits very significant autocorrelations, but the \emph{Range} estimators are not autocorrelated, 
      \vskip1ex
      That is because the time series of squared intra-period ranges is not autocorrelated, 
      <<echo=(-(1:2)),eval=FALSE>>=
par(oma=c(1, 1, 1, 1), mar=c(2, 2, 1, 1), mgp=c(0, 0.5, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # set plot panels
# Close variance estimator partial autocorrelations
pacf(var_close, lag=10, xlab=NA, ylab=NA)
title(main="VTI close variance partial autocorrelations")

# Range variance estimator partial autocorrelations
pacf(var_yang_zhang, lag=10, xlab=NA, ylab=NA)
title(main="VTI YZ variance partial autocorrelations")

# Squared range partial autocorrelations
re_turns <- log(rutils::env_etf$VTI[,2] /
                  rutils::env_etf$VTI[,3])
pacf(re_turns^2, lag=10, xlab=NA, ylab=NA)
title(main="VTI squared range partial autocorrelations")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/var_pacf.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Package \protect\emph{PerformanceAnalytics} for Risk and Return Analysis}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{PerformanceAnalytics}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The package \emph{PerformanceAnalytics} contains functions and data sets for performance and risk analysis,
      \vskip1ex
      The function \texttt{data()} loads external data or lists data sets in a package,
      \vskip1ex
      \texttt{managers} is an \emph{xts} time series containing monthly percentage returns of six asset managers (HAM1 through HAM6), the EDHEC Long-Short Equity hedge fund index, the \texttt{S\&P 500}, and US Treasury 10-year bond and 3-month bill total returns,
    \column{0.6\textwidth}
      \vspace{-1em}
      <<eval=FALSE>>=
library(PerformanceAnalytics)  # load package "PerformanceAnalytics"
# get documentation for package "PerformanceAnalytics"
packageDescription("PerformanceAnalytics")  # get short description
help(package="PerformanceAnalytics")  # load help page
data(package="PerformanceAnalytics")  # list all datasets in "PerformanceAnalytics"
ls("package:PerformanceAnalytics")  # list all objects in "PerformanceAnalytics"
detach("package:PerformanceAnalytics")  # remove PerformanceAnalytics from search path
      @
      \vspace{-1em}
      <<echo=(-1)>>=
library(PerformanceAnalytics)  # load package "PerformanceAnalytics"
perf_data <- 
  unclass(data(
    package="PerformanceAnalytics"))$results[, -(1:2)]
apply(perf_data, 1, paste, collapse=" - ")
data(managers)  # load "managers" data set
class(managers)
dim(managers)
head(managers, 3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{CumReturns} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{chart.CumReturns()} plots the cumulative returns of a time series of returns,
      <<cum_returns,echo=TRUE,eval=FALSE,fig.width=7,fig.height=6,fig.show="hide">>=
# load package "PerformanceAnalytics"
library(PerformanceAnalytics)
data(managers)  # load "managers" data set
ham_1 <- managers[, c("HAM1", "EDHEC LS EQ", 
                      "SP500 TR")]

chart.CumReturns(ham_1, lwd=2, ylab="", 
        legend.loc="topleft", main="")
# add title
title(main="Managers cumulative returns", 
      line=-1)
      @
    \column{0.5\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/cum_returns-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{PerformanceSummary} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{charts.PerformanceSummary()} plots three charts: cumulative returns, return bars, and drawdowns,
      <<performance_summary,echo=(-(1:2)),eval=FALSE,fig.height=6,fig.show="hide">>=
library(PerformanceAnalytics)  # load package "PerformanceAnalytics"
data(managers)  # load "managers" data set
charts.PerformanceSummary(ham_1, 
  main="", lwd=2, ylog=TRUE)
      @
    \column{0.5\textwidth}
    \vspace{-3em}
      \includegraphics[width=0.5\paperwidth]{figure/performance_summary-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{ETF \texttt{CumReturns} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{chart.CumReturns()} plots the cumulative returns of a time series of returns,
      <<etf_cum_returns,echo=(-1),eval=FALSE,fig.width=7,fig.height=6,fig.show="hide">>=
library(PerformanceAnalytics)  # load package "PerformanceAnalytics"
chart.CumReturns(
  env_etf$re_turns[, c("XLF", "DBC", "IEF")], lwd=2, 
  ylab="", legend.loc="topleft", main="")
# add title
title(main="ETF cumulative returns", line=-1)
      @
    \column{0.5\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/etf_cum_returns-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Drawdown Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
    \vspace{-1em}
      <<drawdown_plot,eval=FALSE,echo=(-(1:1)),fig.width=7,fig.height=6,fig.show="hide">>=
options(width=200)
library(PerformanceAnalytics)
chart.Drawdown(env_etf$re_turns[, "VTI"], ylab="", 
               main="VTI drawdowns")
      @
      \vskip27ex
      <<eval=FALSE,echo=(-(1:2))>>=
options(width=200)
library(PerformanceAnalytics)
table.Drawdowns(env_etf$re_turns[, "VTI"])
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/drawdown_plot-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Return Distribution Histogram}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<returns_hist,echo=(-1),eval=FALSE,fig.width=5,fig.height=5,fig.show="hide">>=
library(PerformanceAnalytics)
chart.Histogram(env_etf$re_turns[, 1], main="", 
  xlim=c(-0.06, 0.06), 
  methods = c("add.density", "add.normal"))
# add title
title(main=paste(colnames(env_etf$re_turns[, 1]), 
                 "density"), line=-1)
      @
    \column{0.5\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/returns_hist-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Return Boxplots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<returns_box,echo=(-1),eval=FALSE,fig.width=6,fig.height=6,fig.show="hide">>=
library(PerformanceAnalytics)
chart.Boxplot(env_etf$re_turns[, 
  c("VTI", "IEF", "IVW", "VYM", "IWB", "DBC", "VXX")])
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \hspace*{-8em}\includegraphics[width=0.65\paperwidth]{figure/returns_box-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Return Distribution Statistics}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-1),eval=FALSE>>=
library(PerformanceAnalytics)
tail(table.Stats(env_etf$re_turns[, 
  c("VTI", "IEF", "DBC", "VXX")]), 4)
risk_return <- table.Stats(env_etf$re_turns)
class(risk_return)
# Transpose the data frame
risk_return <- as.data.frame(t(risk_return))
      @
      \vspace{-1em}
      <<returns_scatter,echo=(-1),eval=FALSE,fig.width=5,fig.height=5,fig.show="hide">>=
# plot scatterplot
plot(Kurtosis ~ Skewness, data=risk_return,
     main="Kurtosis vs Skewness")
# add labels
text(x=risk_return$Skewness, y=risk_return$Kurtosis, 
          labels=rownames(risk_return), 
          pos=1, cex=0.8)
      @
    \column{0.5\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/returns_scatter-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Return Statistics Ranking}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.45\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# add skew_kurt column
risk_return$skew_kurt <- 
  risk_return$Skewness/risk_return$Kurtosis
# sort on skew_kurt
risk_return <- risk_return[
  order(risk_return$skew_kurt, 
        decreasing=TRUE), ]
# add names column
risk_return$Name <- 
  etf_list[rownames(risk_return), ]$Name
      @
    \column{0.55\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
risk_return[, c("Name", "Skewness", "Kurtosis")]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk vs. Return Scatterplot}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<risk_return_scatter,echo=(-1),eval=FALSE,fig.width=5,fig.height=5,fig.show="hide">>=
library(PerformanceAnalytics)
chart.RiskReturnScatter(
  env_etf$re_turns[, colnames(env_etf$re_turns)!="VXX"], 
  Rf=0.01/12)
      @
    \column{0.5\textwidth}
    \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/risk_return_scatter-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk-adjusted Returns Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sharpe} ratio measures the excess returns per unit of risk, and is equal to the excess returns (over a risk-free return) divided by the standard deviation of the returns:
      \begin{displaymath}
        S_{r}=\frac{E[R-R_f]}{\sigma}
      \end{displaymath}
      The \emph{Sortino} ratio is equal to the excess returns divided by the \emph{downside deviation} (standard deviation of returns below a target rate of return),
      \begin{displaymath}
        S_{r}=\frac{E[R-R_t]}{\sqrt{\sum_{i=1}^{k} ([R_i-R_t]_{-})^2}}
      \end{displaymath}
      The \emph{Calmar} ratio is equal to the excess returns divided by the maximum drawdown of the returns:
      \begin{displaymath}
        C_{r}=\frac{E[R-R_f]}{DD}
      \end{displaymath}
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-1)>>=
library(PerformanceAnalytics)
vti_ief <- env_etf$re_turns[, c("VTI", "IEF")]
SharpeRatio(vti_ief)

SortinoRatio(vti_ief)

CalmarRatio(vti_ief)
tail(table.Stats(vti_ief), 4)
      @
  \end{columns}
\end{block}

\end{frame}


\end{document}
