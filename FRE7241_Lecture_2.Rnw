% FRE7241_Lecture_2
% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='scriptsize', fig.width=4, fig.height=4)
options(width=60, dev='pdf')
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{array}
\usepackage{multirow}
% mathtools package for math symbols
\usepackage{mathtools}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
\usepackage{listings}
\usepackage{xcolor}
\definecolor{anti_flashwhite}{rgb}{0.95, 0.95, 0.96}
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[FRE7241 Lecture\#2]{FRE7241 Algorithmic Portfolio Management}
\subtitle{Lecture\#2, Spring 2019}

\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{April 16, 2019}
% \date{\today}
% \pgfdeclareimage[height=0.5cm]{university-logo}{engineering_long_white}
% \logo{\pgfuseimage{engineering_long_white}}


%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle


%%%%%%%%%%%%%%%
\section{Performing Aggregations Over Time Series}


%%%%%%%%%%%%%%%
\subsection{Defining Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A time \emph{period} is the time between two neighboring points in time.
      \vskip1ex
      A time \emph{interval} is the time spanned by one or more time \emph{periods}.
      \vskip1ex
      A \emph{look-back interval} is a time \emph{interval} for performing aggregations over the past, starting from a \emph{startpoint} and ending at an \emph{endpoint}.
      \vskip1ex
      The \emph{startpoints} are the \emph{endpoints} lagged by the \emph{look-back interval}.
      \vskip1ex
      The look-back \emph{intervals} may or may not \emph{overlap} with their neighboring intervals.
    \column{0.5\textwidth}
      A \emph{rolling aggregation} is specified by \emph{endpoints} at each point in time.
      \vskip1ex
      An example of a rolling aggregation are moving average prices.
      \vskip1ex
      An \emph{interval aggregation} is specified by \emph{endpoints} separated by many time \emph{periods}.
      \vskip1ex
      Examples of interval aggregations are monthly asset returns, or trailing 12-month asset returns calculated every month.
  \end{columns}
    \vspace{-2em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_overlapping.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining \protect\emph{Rolling} Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{rolling aggregation} is specified by \emph{endpoints} at each point in time.
      \vskip1ex
      The \emph{startpoints} are the \emph{endpoints} lagged by the \emph{look-back interval}.
      \vskip1ex
      An example of a rolling aggregation are moving average prices.
    \column{0.5\textwidth}
      <<echo=TRUE,eval=FALSE>>=
oh_lc <- rutils::etf_env$VTI
# Number of data points
n_rows <- NROW(oh_lc["2018-06/"])
# Define end_points at each point in time
end_points <- 1:n_rows
# Number of data points in look_back interval
look_back <- 22
# start_points are end_points lagged by look_back
start_points <- c(rep_len(1, look_back-1), 
    end_points[1:(n_rows-look_back+1)])
# Or
start_points <- (end_points-look_back+1)
start_points <- ifelse(start_points > 0, 
  start_points, 1)
      @
  \end{columns}
    \vspace{-2em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_rolling.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining Equally Spaced \protect\emph{Endpoints} of a Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The neighboring \emph{endpoints} may be separated by a fixed number of time periods, equal to \texttt{n\_points}.
      \vskip1ex
      If the total number of data points is not an integer multiple of \texttt{n\_points}, then a stub interval must be added either at the beginning or at the end of \emph{endpoints}.
      \vskip1ex
      The function \texttt{xts::endpoints()} extracts the indices of the last observations in each calendar period of an \emph{xts} series.
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Number of data points
price_s <- Cl(oh_lc["2018/"])
n_rows <- NROW(price_s)
# Number of periods between endpoints
n_points <- 22
# Number of n_points that fit over n_rows
n_agg <- n_rows %/% n_points
# if n_rows==n_points*n_agg then whole number 
end_points <- (1:n_agg)*n_points
# else stub interval at beginning
end_points <- 
  n_rows-n_points*n_agg + (0:n_agg)*n_points
# else stub interval at end
end_points <- c((1:n_agg)*n_points, n_rows)
# Or use xts::endpoints()
end_points <- xts::endpoints(price_s, on="months")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/intervals_end_points.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot data and endpoints as vertical lines
plot.xts(price_s, col="blue", lwd=2, xlab="", ylab="",
         main="Prices with Endpoints as Vertical Lines")
addEventLines(xts(rep("endpoint", NROW(end_points)), index(price_s)[end_points]), 
              col="red", lwd=2, pos=4)
# Or
plot_theme <- chart_theme()
plot_theme$col$line.col <- "blue"
chart_Series(price_s, theme=plot_theme, 
  name="prices with endpoints as vertical lines")
abline(v=end_points, col="red", lwd=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining \protect\emph{Overlapping} Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Overlapping} time intervals can be defined if the \emph{startpoints} are equal to the \emph{endpoints} lagged by the \emph{look-back interval}.
      \vskip1ex
      An example of an overlapping interval aggregation are trailing 12-month asset returns calculated every month.
      <<echo=TRUE,eval=FALSE>>=
# Number of data points
n_rows <- NROW(rutils::etf_env$VTI["2017/"])
# Number of n_points that fit over n_rows
n_points <- 22
n_agg <- n_rows %/% n_points
# end_points with stub interval at beginning
end_points <- 
  n_rows-n_points*n_agg + (0:n_agg)*n_points
      @
    \column{0.5\textwidth}
      The length of the \emph{look-back interval} can be defined either as the number of data points, or as the number of \emph{endpoints} to look back over.
      <<echo=TRUE,eval=FALSE>>=
# look_back defined as number of data points
look_back <- 252
# start_points are end_points lagged by look_back
start_points <- (end_points-look_back+1)
start_points <- ifelse(start_points > 0, 
  start_points, 1)
cbind(start_points, end_points)
# look_back defined as number of end_points
look_back <- 12
start_points <- c(rep_len(1, look_back-1), 
    end_points[1:(NROW(end_points)-look_back+1)])
      @
  \end{columns}
    \vspace{-2em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_overlapping.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining \protect\emph{Non-overlapping} Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Non-overlapping} time intervals can be defined if \emph{startpoints} are equal to the previous \emph{endpoints}.
      \vskip1ex
      In that case the look-back \emph{intervals} are non-overlapping and \emph{contiguous} (each \emph{startpoint} is the \emph{endpoint} of the previous interval).
      \vskip1ex
      If the \emph{startpoints} are defined as the previous \emph{endpoints} plus \texttt{1}, then the \emph{intervals} are \emph{exclusive}.
      \vskip1ex
      \emph{Exclusive intervals} are essential for calculating \emph{out-of-sample} aggregations over future intervals.
    \column{0.5\textwidth}
      <<echo=TRUE,eval=FALSE>>=
# Number of data points
n_rows <- NROW(rutils::etf_env$VTI["2017/"])
# Number of data points per interval
look_back <- 22
# Number of look_backs that fit over n_rows
n_agg <- n_rows %/% look_back
# Define end_points with beginning stub
end_points <- 
  n_rows-look_back*n_agg + (0:n_agg)*look_back
# Define contiguous start_points
start_points <- c(1, end_points[1:(NROW(end_points)-1)])
# Define exclusive start_points
start_points <- c(1, end_points[1:(NROW(end_points)-1)]+1)
      @
  \end{columns}
    \vspace{-2em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_non_overlapping.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using \texttt{sapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      Aggregations performed over time series can be extremely slow if done improperly, therefore it's very important to find the fastest methods of performing aggregations, 
      \vskip1ex
      The \texttt{sapply()} functional allows performing aggregations over the look-back \emph{intervals}, 
      \vskip1ex
      The \texttt{sapply()} functional by default returns a vector or matrix, not an \emph{xts} series,
      \vskip1ex
      The vector or matrix returned by \texttt{sapply()} therefore needs to be coerced into an \emph{xts} series,
      \vskip1ex
      The variable \texttt{look\_back} is the size of the look-back interval, equal to the number of data points used for applying the aggregation function (including the current point), 
    \column{0.6\textwidth}
      \vspace{-1em}
      <<TRUE,eval=FALSE>>=
price_s <- Cl(rutils::etf_env$VTI)
end_points <- seq_along(price_s)  # Define end points
n_rows <- NROW(end_points)
look_back <- 22  # number of data points per look-back interval
# start_points are multi-period lag of end_points
start_points <- c(rep_len(1, look_back-1), 
    end_points[1:(n_rows-look_back+1)])
# Define list of look-back intervals for aggregations over past
look_backs <- lapply(seq_along(end_points), 
  function(in_dex) {
    start_points[in_dex]:end_points[in_dex]
})  # end lapply
# Define aggregation function
agg_regate <- function(x_ts) c(max=max(x_ts), min=min(x_ts))
# Perform aggregations over look_backs list
agg_regations <- sapply(look_backs, 
    function(look_back) agg_regate(price_s[look_back])
)  # end sapply
# Coerce agg_regations into matrix and transpose it
if (is.vector(agg_regations))
  agg_regations <- t(agg_regations)
agg_regations <- t(agg_regations)
# Coerce agg_regations into xts series
agg_regations <- xts(agg_regations, 
                     order.by=index(price_s[end_points]))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using \texttt{lapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The \texttt{lapply()} functional allows performing aggregations over the look-back \emph{intervals}, 
      \vskip1ex
      The \texttt{lapply()} functional by default returns a list, not an \emph{xts} series,
      \vskip1ex
      If \texttt{lapply()} returns a list of \emph{xts} series, then this list can be collapsed into a single \emph{xts} series using the function \texttt{do\_call\_rbind()} from package \emph{rutils}, 
      \vskip1ex
      The function \texttt{chart\_Series()} from package \emph{quantmod} can produce a variety of time series plots, 
      \vskip1ex
      \texttt{chart\_Series()} plots can be modified by modifying \emph{plot objects} or \emph{theme objects},
      \vskip1ex
      A plot \emph{theme object} is a list containing parameters that determine the plot appearance (colors, size, fonts),
      \vskip1ex
      The function \texttt{chart\_theme()} returns the theme object, 
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(HighFreq)  # load package HighFreq
# Perform aggregations over look_backs list
agg_regations <- lapply(look_backs, 
    function(look_back) agg_regate(price_s[look_back])
)  # end lapply
# rbind list into single xts or matrix
agg_regations <- rutils::do_call_rbind(agg_regations)
# Convert into xts
agg_regations <- xts::xts(agg_regations, 
    order.by=index(price_s))
agg_regations <- cbind(agg_regations, price_s)
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red", "green")
x11(width=6, height=5)
chart_Series(agg_regations, theme=plot_theme, 
             name="price aggregations")
legend("top", legend=colnames(agg_regations), 
  bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining Functionals for Rolling Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The functional \texttt{roll\_agg()} performs rolling aggregations of its function argument \texttt{FUN}, over an \emph{xts} series (\texttt{x\_ts}), and a look-back interval (\texttt{look\_back}),
      \vskip1ex
      The argument \texttt{FUN} is an aggregation function over a subset of \texttt{x\_ts} series, 
      \vskip1ex
      The dots \texttt{"..."} argument is passed into \texttt{FUN} as additional arguments,
      \vskip1ex
      The argument \texttt{look\_back} is equal to the number of periods of \texttt{x\_ts} series which are passed to the aggregation function \texttt{FUN}, 
      \vskip1ex
      The functional \texttt{roll\_agg()} calls \texttt{lapply()}, which loops over the length of series \texttt{x\_ts}, 
      \vskip1ex
      Note that two different intervals may be used with \texttt{roll\_agg()},
      \vskip1ex
      The first interval is the argument \texttt{look\_back}, 
      \vskip1ex
      A second interval may be one of the variables bound to the dots \texttt{"..."} argument, and passed to the aggregation function \texttt{FUN} (for example, an \emph{EWMA} window), 
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# Define functional for rolling aggregations
roll_agg <- function(x_ts, look_back, FUN, ...) {
# Define end points at every period
  end_points <- seq_along(x_ts)
  n_rows <- NROW(end_points)
# Define starting points as lag of end_points
  start_points <- c(rep_len(1, look_back-1), 
    end_points[1:(n_rows-look_back+1)])
# Define list of look-back intervals for aggregations over past
look_backs <- lapply(seq_along(end_points), 
  function(in_dex) {
    start_points[in_dex]:end_points[in_dex]
})  # end lapply
# Perform aggregations over look_backs list
  agg_regations <- lapply(look_backs, 
    function(look_back) FUN(x_ts[look_back], ...)
  )  # end lapply
# rbind list into single xts or matrix
  agg_regations <- rutils::do_call_rbind(agg_regations)
# Coerce agg_regations into xts series
  if (!is.xts(agg_regations))
    agg_regations <- xts(agg_regations, order.by=index(x_ts))
  agg_regations
}  # end roll_agg
# Define aggregation function
agg_regate <- function(x_ts)
  c(max=max(x_ts), min=min(x_ts))
# Perform aggregations over rolling interval
agg_regations <- roll_agg(price_s, look_back=look_back, 
                    FUN=agg_regate)
class(agg_regations)
dim(agg_regations)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Benchmarking Speed of Rolling Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The speed of rolling aggregations using \texttt{apply()} loops can be greatly increased by simplifying the aggregation function, 
      \vskip1ex
      For example, an aggregation function that returns a vector is over \texttt{13} times faster than a function that returns an \emph{xts} object, 
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# Define aggregation function that returns a vector
agg_vector <- function(x_ts)
  c(max=max(x_ts), min=min(x_ts))
# Define aggregation function that returns an xts
agg_xts <- function(x_ts)
  xts(t(c(max=max(x_ts), min=min(x_ts))), 
      order.by=end(x_ts))
# Benchmark the speed of aggregation functions
library(microbenchmark)
summary(microbenchmark(
  agg_vector=roll_agg(price_s, look_back=look_back,
                    FUN=agg_vector),
  agg_xts=roll_agg(price_s, look_back=look_back,
                    FUN=agg_xts),
  times=10))[, c(1, 4, 5)]
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Benchmarking Functionals for Rolling Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.45\textwidth}
      Several packages contain functionals designed for performing rolling aggregations:
      \begin{itemize}
        \item \texttt{rollapply.zoo()} from package \emph{zoo},
        \item \texttt{rollapply.xts()} from package \emph{xts},
        \item \texttt{apply.rolling()} from package \emph{PerformanceAnalytics},
      \end{itemize}
      These functionals don't require specifying the \emph{endpoints}, and instead calculate the \emph{endpoints} from the rolling interval width, 
      \vskip1ex
      These functionals can only apply functions that return a single value, not a vector, 
      \vskip1ex
      These functionals return an \emph{xts} series with leading \texttt{NA} values at points before the rolling interval can fit over the data, 
      \vskip1ex
      The argument \texttt{align="right"} of \texttt{rollapply()} determines that aggregations are taken from the past,
      \vskip1ex
      The functional \texttt{rollapply.xts} is the fastest, about as fast as performing an \texttt{lapply()} loop directly, 
    \column{0.55\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# Define aggregation function that returns a single value
agg_regate <- function(x_ts)  max(x_ts)
# Perform aggregations over a rolling interval
agg_regations <- xts:::rollapply.xts(price_s, width=look_back, 
                    FUN=agg_regate, align="right")
# Perform aggregations over a rolling interval
library(PerformanceAnalytics)  # load package PerformanceAnalytics
agg_regations <- apply.rolling(price_s, 
                    width=look_back, FUN=agg_regate)
# Benchmark the speed of the functionals
library(microbenchmark)
summary(microbenchmark(
  roll_agg=roll_agg(price_s, look_back=look_back,
                    FUN=max),
  roll_xts=xts:::rollapply.xts(price_s, width=look_back, 
                       FUN=max, align="right"), 
  apply_rolling=apply.rolling(price_s, 
                              width=look_back, FUN=max), 
  times=10))[, c(1, 4, 5)]
@
  \end{columns}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%
\subsection{Rolling Aggregations Using \protect\emph{Vectorized} Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The generic functions \texttt{cumsum()}, \texttt{cummax()}, and \texttt{cummin()} return the cumulative sums, minima, and maxima of \emph{vectors} and \emph{time series} objects,
      \vskip1ex
      The methods for these functions are implemented as \emph{vectorized compiled} functions, and are therefore much faster than \texttt{apply()} loops, 
      \vskip1ex
      The \texttt{cumsum()} function can be used to efficiently calculate the rolling sum of an an \emph{xts} series, 
      \vskip1ex
      Using the function \texttt{cumsum()} is over \texttt{25} times faster than using \texttt{apply()} loops, 
      \vskip1ex
      But rolling standard deviations and higher moments can't be easily calculated using \texttt{cumsum()}, 
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# rolling sum using cumsum()
roll_sum <- function(x_ts, look_back) {
  cum_sum <- cumsum(na.omit(x_ts))
  out_put <- cum_sum - lag(x=cum_sum, k=look_back)
  out_put[1:look_back, ] <- cum_sum[1:look_back, ]
  colnames(out_put) <- paste0(colnames(x_ts), "_stdev")
  out_put
}  # end roll_sum
agg_regations <- roll_sum(price_s, look_back=look_back)
# Define list of look-back intervals for aggregations over past
look_backs <- lapply(seq_along(end_points), 
  function(in_dex) {
    start_points[in_dex]:end_points[in_dex]
})  # end lapply
# Perform rolling aggregations using apply loop
agg_regations <- sapply(look_backs, 
    function(look_back) sum(price_s[look_back])
)  # end sapply
head(agg_regations)
tail(agg_regations)
# Benchmark the speed of both methods
library(microbenchmark)
summary(microbenchmark(
  roll_sum=roll_sum(price_s, look_back=look_back),
  s_apply=sapply(look_backs, 
    function(look_back) sum(price_s[look_back])), 
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using Package \protect\emph{TTR}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{TTR} contains functions for calculating rolling aggregations over \emph{vectors} and \emph{time series} objects: 
      \begin{itemize}
        \item \texttt{runSum()} for rolling sums,
        \item \texttt{runMin()} and \texttt{runMax()} for rolling minima and maxima, 
        \item \texttt{runSD()} for rolling standard deviations,
        \item \texttt{runMedian()} and \texttt{runMAD()} for rolling medians and Median Absolute Deviations (MAD), 
        \item \texttt{runCor()} for rolling correlations,
      \end{itemize}
      The rolling \emph{TTR} functions are much faster than performing \texttt{apply()} loops, because they are \emph{compiled} functions (compiled from \texttt{C++} or \texttt{Fortran} code),
      \vskip1ex
      But the rolling \emph{TTR} functions are a little slower than using \emph{vectorized compiled} functions such as \texttt{cumsum()}, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the rolling maximum and minimum over a vector of data
roll_maxminr <- function(vec_tor, look_back) {
  n_rows <- NROW(vec_tor)
  max_min <- matrix(numeric(2*n_rows), nc=2)
  # loop over periods
  for (it in 1:n_rows) {
    sub_vec <- vec_tor[max(1, it-look_back+1):it]
    max_min[it, 1] <- max(sub_vec)
    max_min[it, 2] <- min(sub_vec)
  }  # end for
  return(max_min)
}  # end roll_maxminr
max_minr <- roll_maxminr(price_s, look_back)
max_minr <- xts::xts(max_minr, index(price_s))
library(TTR)  # load package TTR
max_min <- cbind(TTR::runMax(x=price_s, n=look_back),
                 TTR::runMin(x=price_s, n=look_back))
all.equal(max_min[-(1:look_back), ], max_minr[-(1:look_back), ], check.attributes=FALSE)
# Benchmark the speed of TTR::runMax
library(microbenchmark)
summary(microbenchmark(
  pure_r=roll_maxminr(price_s, look_back), 
  ttr=TTR::runMax(price_s, n=look_back),
  times=10))[, c(1, 4, 5)]
# Benchmark the speed of TTR::runSum
summary(microbenchmark(
  vector_r=cumsum(coredata(price_s)), 
  rutils=rutils::roll_sum(price_s, look_back=look_back),
  ttr=TTR::runSum(price_s, n=look_back),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using \protect\emph{RcppArmadillo}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{RcppArmadillo} functions for calculating rolling aggregations are often the fastest.
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
// Rcpp header with information for C++ compiler
#include <RcppArmadillo.h> // include C++ header file from Armadillo library
using namespace arma; // use C++ namespace from Armadillo library
// declare dependency on RcppArmadillo
// [[Rcpp::depends(RcppArmadillo)]]

// export the function roll_maxmin() to R
// [[Rcpp::export]]
arma::mat roll_maxmin(const arma::vec& vec_tor, 
                      const arma::uword& look_back) {
  arma::uword n_rows = vec_tor.size();
  arma::mat max_min(n_rows, 2);
  arma::vec sub_vec;
  // startup period
  max_min(0, 0) = vec_tor[0];
  max_min(0, 1) = vec_tor[0];
  for (uword it = 1; it < look_back; it++) {
    sub_vec = vec_tor.subvec(0, it);
    max_min(it, 0) = sub_vec.max();
    max_min(it, 1) = sub_vec.min();
  }  // end for
  // remaining periods
  for (uword it = look_back; it < n_rows; it++) {
    sub_vec = vec_tor.subvec(it-look_back+1, it);
    max_min(it, 0) = sub_vec.max();
    max_min(it, 1) = sub_vec.min();
  }  // end for
  return max_min;
}  // end roll_maxmin
    \end{lstlisting}
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/rolling_maxmin.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Compile Rcpp functions
Rcpp::sourceCpp(file="C:/Develop/R/Rcpp/roll_maxmin.cpp")
max_minarma <- roll_maxmin(price_s, look_back)
max_minarma <- xts::xts(max_minr, index(price_s))
max_min <- cbind(TTR::runMax(x=price_s, n=look_back),
                 TTR::runMin(x=price_s, n=look_back))
all.equal(max_min[-(1:look_back), ], max_minarma[-(1:look_back), ], check.attributes=FALSE)
# Benchmark the speed of TTR::runMax
library(microbenchmark)
summary(microbenchmark(
  arma=roll_maxmin(price_s, look_back), 
  ttr=TTR::runMax(price_s, n=look_back),
  times=10))[, c(1, 4, 5)]
# Dygraphs plot with max_min lines
da_ta <- cbind(price_s, max_minarma)
colnames(da_ta)[2:3] <- c("max", "min")
col_ors <- c("blue", "red", "green")
dygraphs::dygraph(da_ta, main=paste(colnames(price_s), "max and min lines")) %>%
  dyOptions(colors=col_ors)
# standard plot with max_min lines
plot_theme <- chart_theme()
plot_theme$col$line.col <- col_ors
quantmod::chart_Series(da_ta["2008/2009"], theme=plot_theme, 
  name=paste(colnames(price_s), "max and min lines"))
legend(x="topright", title=NULL, legend=colnames(da_ta),
       inset=0.1, cex=0.9, bg="white", bty="n",
       lwd=6, lty=1, col=col_ors)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling \protect\emph{Weighted} Aggregations Using Package \protect\emph{roll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{roll} contains functions for calculating \emph{weighted} rolling aggregations over \emph{vectors} and \emph{time series} objects: 
      \begin{itemize}
        \item \texttt{roll\_sum()}, \texttt{roll\_max()}, \texttt{roll\_mean()}, and \texttt{roll\_median()} for \emph{weighted} rolling sums, maximums, means, and medians,
        \item \texttt{roll\_var()} for \emph{weighted} rolling variance,
        \item \texttt{roll\_scale()} for rolling scaling and centering of time series, 
        \item \texttt{roll\_lm()} for rolling regression,
        \item \texttt{roll\_pcr()} for rolling principal component regressions of time series,
      \end{itemize}
      The \emph{roll} functions are about \texttt{1,000} times faster than \texttt{apply()} loops!
      \vskip1ex
      The \emph{roll} functions are extremely fast because they perform calculations in \emph{parallel} in compiled \texttt{C++} code, using packages \emph{Rcpp} and \emph{RcppArmadillo}.
      \vskip1ex
      The \emph{roll} functions accept \emph{xts} time series, and they return \emph{xts}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)
# Calculate rolling VTI variance using package roll
library(roll)  # load roll
re_turns <- na.omit(rutils::etf_env$re_turns[, "VTI"])
look_back <- 22
# Calculate rolling sum using RcppRoll
sum_roll <- roll::roll_sum(re_turns, width=look_back)
# Calculate rolling sum using rutils
sum_rutils <- rutils::roll_sum(re_turns, look_back=look_back)
all.equal(sum_roll[-(1:look_back), ], sum_rutils[-(1:look_back), ], check.attributes=FALSE)
# Benchmark speed of rolling calculations
library(microbenchmark)
summary(microbenchmark(
  cum_sum=cumsum(re_turns), 
  roll=roll::roll_sum(re_turns, width=look_back),
  RcppRoll=RcppRoll::roll_sum(re_turns, n=look_back),
  rutils=rutils::roll_sum(re_turns, look_back=look_back),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling \protect\emph{Weighted} Aggregations Using Package \protect\emph{RcppRoll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{RcppRoll} contains functions for calculating \emph{weighted} rolling aggregations over \emph{vectors} and \emph{time series} objects: 
      \begin{itemize}
        \item \texttt{roll\_sum()} for \emph{weighted} rolling sums,
        \item \texttt{roll\_min()} and \texttt{roll\_max()} for \emph{weighted} rolling minima and maxima, 
        \item \texttt{roll\_sd()} for \emph{weighted} rolling standard deviations,
        \item \texttt{roll\_median()} for \emph{weighted} rolling medians, 
      \end{itemize}
      The \emph{RcppRoll} functions accept \emph{xts} objects, but they return matrices, not \emph{xts} objects, 
      \vskip1ex
      The rolling \emph{RcppRoll} functions are much faster than performing \texttt{apply()} loops, because they are \emph{compiled} functions (compiled from \texttt{C++} code),
      \vskip1ex
      But the rolling \emph{RcppRoll} functions are a little slower than using \emph{vectorized compiled} functions such as \texttt{cumsum()}, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(RcppRoll)  # load package RcppRoll
# Calculate rolling sum using RcppRoll
sum_roll <- RcppRoll::roll_sum(re_turns, align="right", n=look_back)
# Calculate rolling sum using rutils
sum_rutils <- rutils::roll_sum(re_turns, look_back=look_back)
all.equal(sum_roll, coredata(sum_rutils[-(1:(look_back-1))]), check.attributes=FALSE)
# Benchmark speed of rolling calculations
library(microbenchmark)
summary(microbenchmark(
  cum_sum=cumsum(re_turns), 
  RcppRoll=RcppRoll::roll_sum(re_turns, n=look_back),
  rutils=rutils::roll_sum(re_turns, look_back=look_back),
  times=10))[, c(1, 4, 5)]
# Calculate EWMA prices using RcppRoll
price_s <- na.omit(rutils::etf_env$VTI[, 4])
weight_s <- exp(0.1*1:look_back)
prices_ewma <- RcppRoll::roll_mean(price_s, 
      align="right", n=look_back, weights=weight_s)
prices_ewma <- cbind(price_s, 
  rbind(coredata(price_s[1:(look_back-1), ]), prices_ewma))
colnames(prices_ewma) <- c("VTI", "VTI EWMA")
# Plot an interactive dygraph plot
dygraphs::dygraph(prices_ewma)
# Or static plot of EWMA prices with custom line colors
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red")
chart_Series(prices_ewma, theme=plot_theme, 
             name="EWMA prices")
legend("top", legend=colnames(prices_ewma), 
       bg="white", lty=1, lwd=6, 
       col=plot_theme$col$line.col, bty="n")
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using Package \protect\emph{caTools}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{caTools} contains functions for calculating rolling interval aggregations over a \texttt{vector} of data:
      \begin{itemize}
        \item \texttt{runmin()} and \texttt{runmax()} for rolling minima and maxima, 
        \item \texttt{runsd()} for rolling standard deviations,
        \item \texttt{runmad()} for rolling Median Absolute Deviations (MAD),
        \item \texttt{runquantile()} for rolling quantiles,
      \end{itemize}
      Time series need to be coerced to \emph{vectors} before they are passed to \emph{caTools} functions,
      \vskip1ex
      The rolling \emph{caTools} functions are very fast because they are \emph{compiled} functions (compiled from \texttt{C++} code),
      \vskip1ex
      The argument \texttt{"endrule"} determines how the end values of the data are treated,
      \vskip1ex
      The argument \texttt{"align"} determines whether the interval is centered (default), left-aligned or right-aligned, with \texttt{align="center"} the fastest option,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
library(caTools)  # load package "caTools"
# get documentation for package "caTools"
packageDescription("caTools")  # get short description
help(package="caTools")  # load help page
data(package="caTools")  # list all datasets in "caTools"
ls("package:caTools")  # list all objects in "caTools"
detach("package:caTools")  # remove caTools from search path
# median filter
look_back <- 2
price_s <- Cl(HighFreq::SPY["2012-02-01/2012-04-01"])
med_ian <- runmed(x=price_s, k=look_back)
# vector of rolling volatility
vol_at <- runsd(x=price_s, k=look_back, 
                endrule="constant", align="center")
# vector of rolling quantiles
quan_tiles <- runquantile(x=price_s, 
                  k=look_back, probs=0.9, 
                  endrule="constant", 
                  align="center")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Determining Calendar \protect\emph{Endpoints} of \protect\emph{xts} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{xts::endpoints()} extracts the indices of the last observations in each calendar period of an \emph{xts} series.
      \vskip1ex
      For example:\\ \-\ \texttt{endpoints(x, on="hours")}\\
      extracts the indices of the last observations in each hour.
      \vskip1ex
      The \emph{endpoints} calculated by \texttt{endpoints()} aren't always equally spaced, and aren't the same as those calculated from fixed intervals.
      \vskip1ex
      For example, the last observations in each day aren't equally spaced due to weekends and holidays.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(HighFreq)  # load package HighFreq
# indices of last observations in each hour
end_points <- xts::endpoints(price_s, on="hours")
head(end_points)
# extract the last observations in each hour
head(price_s[end_points, ])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Non-overlapping Aggregations Using \texttt{sapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{apply()} functionals allow for applying a function over intervals of an \emph{xts} series defined by a vector of \emph{endpoints}.
      \vskip1ex
      The \texttt{sapply()} functional by default returns a vector or matrix, not an \emph{xts} series.
      \vskip1ex
      The vector or matrix returned by \texttt{sapply()} therefore needs to be coerced into an \emph{xts} series.
      \vskip1ex
      The function \texttt{chart\_Series()} from package \emph{quantmod} can produce a variety of time series plots.
      \vskip1ex
      \texttt{chart\_Series()} plots can be modified by modifying \emph{plot objects} or \emph{theme objects}.
      \vskip1ex
      A plot \emph{theme object} is a list containing parameters that determine the plot appearance (colors, size, fonts).
      \vskip1ex
      The function \texttt{chart\_theme()} returns the theme object.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
price_s <- Cl(rutils::etf_env$VTI)
# Number of data points
n_rows <- NROW(price_s)
# Number of data points per interval
look_back <- 22
# Number of look_backs that fit over n_rows
n_agg <- n_rows %/% look_back
# Define end_points with beginning stub
end_points <- 
  n_rows-look_back*n_agg + (0:n_agg)*look_back
# Define contiguous start_points
start_points <- c(1, end_points[1:(NROW(end_points)-1)])
# Define list of look-back intervals for aggregations over past
look_backs <- lapply(seq_along(end_points), 
  function(in_dex) {
    start_points[in_dex]:end_points[in_dex]
})  # end lapply
look_backs[[1]]
look_backs[[2]]
# Perform sapply() loop over look_backs list
agg_regations <- sapply(look_backs, 
    function(look_back) {
      x_ts <- price_s[look_back]
      c(max=max(x_ts), min=min(x_ts))
  })  # end sapply
# Coerce agg_regations into matrix and transpose it
if (is.vector(agg_regations))
  agg_regations <- t(agg_regations)
agg_regations <- t(agg_regations)
# Coerce agg_regations into xts series
agg_regations <- xts(agg_regations, 
    order.by=index(price_s[end_points]))
head(agg_regations)
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("red", "green")
chart_Series(agg_regations, theme=plot_theme, 
             name="price aggregations")
legend("top", legend=colnames(agg_regations), 
  bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Non-overlapping Aggregations Using \texttt{lapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{apply()} functionals allow for applying a function over intervals of an \emph{xts} series defined by a vector of \emph{endpoints},
      \vskip1ex
      The \texttt{lapply()} functional by default returns a list, not an \emph{xts} series,
      \vskip1ex
      If \texttt{lapply()} returns a list of \emph{xts} series, then this list can be collapsed into a single \emph{xts} series using the function \texttt{do\_call\_rbind()} from package \emph{rutils}, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# Perform lapply() loop over look_backs list
agg_regations <- lapply(look_backs, 
    function(look_back) {
      x_ts <- price_s[look_back]
      c(max=max(x_ts), min=min(x_ts))
    })  # end lapply
# rbind list into single xts or matrix
agg_regations <- rutils::do_call_rbind(agg_regations)
# Coerce agg_regations into xts series
agg_regations <- xts(agg_regations, 
    order.by=index(price_s[end_points]))
head(agg_regations)
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("red", "green")
chart_Series(agg_regations, theme=plot_theme, 
             name="price aggregations")
legend("top", legend=colnames(agg_regations), 
  bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Interval Aggregations Using \texttt{period.apply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{period.apply()} from package \emph{xts} performs \emph{aggregations} over non-overlapping intervals of an \emph{xts} series defined by a vector of \emph{endpoints}, 
      \vskip1ex
      Internally \texttt{period.apply()} performs an \texttt{sapply()} loop, and is therefore about as fast as an \texttt{sapply()} loop, 
      \vskip1ex
      The package \emph{xts} also has several specialized functionals for aggregating data over \emph{endpoints}:
      \begin{itemize}
        \item \texttt{period.sum()} calculate the sum for each period,
        \item \texttt{period.max()} calculate the maximum for each period,
        \item \texttt{period.min()} calculate the minimum for each period,
        \item \texttt{period.prod()} calculate the product for each period,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# Define functional for rolling aggregations over end_points
roll_agg <- function(x_ts, end_points, FUN, ...) {
  n_rows <- NROW(end_points)
# start_points are single-period lag of end_points
  start_points <- c(1, end_points[1:(n_rows-1)])
# Perform aggregations over look_backs list
  agg_regations <- lapply(look_backs, 
    function(look_back) FUN(x_ts[look_back], ...))  # end lapply
# rbind list into single xts or matrix
  agg_regations <- rutils::do_call_rbind(agg_regations)
  if (!is.xts(agg_regations))
    agg_regations <-  # Coerce agg_regations into xts series
    xts(agg_regations, order.by=index(x_ts[end_points]))
  agg_regations
}  # end roll_agg
# Apply sum() over end_points
agg_regations <- 
  roll_agg(price_s, end_points=end_points, FUN=sum)
agg_regations <- 
  period.apply(price_s, INDEX=end_points, FUN=sum)
# Benchmark the speed of aggregation functions
summary(microbenchmark(
  roll_agg=roll_agg(price_s, end_points=end_points, FUN=sum),
  period_apply=period.apply(price_s, INDEX=end_points, FUN=sum),
  times=10))[, c(1, 4, 5)]
agg_regations <- period.sum(price_s, INDEX=end_points)
head(agg_regations)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Aggregations of \protect\emph{xts} Over Calendar Periods}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{xts} has convenience wrapper functionals for \texttt{period.apply()}, that apply functions over calendar periods:
      \begin{itemize}
        \item \texttt{apply.daily()} applies functions over daily periods,
        \item \texttt{apply.weekly()} applies functions over weekly periods,
        \item \texttt{apply.monthly()} applies functions over monthly periods,
        \item \texttt{apply.quarterly()} applies functions over quarterly periods,
        \item \texttt{apply.yearly()} applies functions over yearly periods,
      \end{itemize}
      These functionals don't require specifying a vector of \emph{endpoints}, because they determine the \emph{endpoints} from the calendar periods, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(HighFreq)  # load package HighFreq
# load package HighFreq
library(HighFreq)
# extract closing minutely prices
price_s <- Cl(HighFreq::SPY["2012-02-01/2012-04-01"])
# Apply "mean" over daily periods
agg_regations <- apply.daily(price_s, FUN=sum)
head(agg_regations)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Aggregations Over Overlapping Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{period.apply()} performs aggregations over \emph{non-overlapping} intervals, 
      \vskip1ex
      But it's often necessary to perform aggregations over \emph{overlapping} intervals, defined by a vector of \emph{endpoints} and a \emph{look-back interval}, 
      \vskip1ex
      The \emph{startpoints} are defined as the \emph{endpoints} lagged by the interval width (number of periods in the \emph{look-back interval}), 
      \vskip1ex
      Each point in time has an associated \emph{look-back interval}, which starts at a certain number of periods in the past (\emph{start\_point}) and ends at that point (\emph{end\_point}), 
      \vskip1ex
      The variable \texttt{look\_back} is equal to the number of end points in the \emph{look-back interval}, while (\texttt{look\_back - 1}) is equal to the number of intervals in the look-back, 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Number of n_points that fit over n_rows
n_points <- 22
n_agg <- n_rows %/% n_points
# end_points with stub interval at beginning
end_points <- # Define end_points with beginning stub
  n_rows-n_points*n_agg + (0:n_agg)*n_points
# Number of data points in look_back interval
look_back <- 252
# start_points are end_points lagged by look_back
start_points <- (end_points-look_back+1)
start_points <- ifelse(start_points > 0, 
  start_points, 1)
# Define list of look-back intervals for aggregations over past
look_backs <- lapply(seq_along(end_points), 
  function(in_dex) {
    start_points[in_dex]:end_points[in_dex]
})  # end lapply
# Perform lapply() loop over look_backs list
agg_regations <- lapply(look_backs, 
    function(look_back) {
      x_ts <- price_s[look_back]
      c(max=max(x_ts), min=min(x_ts))
    })  # end lapply
# rbind list into single xts or matrix
agg_regations <- rutils::do_call_rbind(agg_regations)
# Coerce agg_regations into xts series
agg_regations <- xts(agg_regations, 
    order.by=index(price_s[end_points]))
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("red", "green")
chart_Series(agg_regations, theme=plot_theme, 
             name="price aggregations")
legend("top", legend=colnames(agg_regations), 
  bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Extending Interval Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Interval aggregations produce values only at the \emph{endpoints}, but they can be carried forward in time using the function \texttt{na.locf.xts()} from package \emph{xts}.
      <<echo=(-(1:2)),eval=FALSE>>=
library(HighFreq)  # load package HighFreq
agg_regations <- cbind(price_s, agg_regations)
tail(agg_regations, 22)
agg_regations <- na.omit(xts:::na.locf.xts(agg_regations))
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red", "green")
chart_Series(agg_regations, theme=plot_theme, 
             name="price aggregations")
legend("top", legend=colnames(agg_regations), 
  bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/agg_interval_carryfwd.png}\\
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Interval Aggregations of \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The method \texttt{aggregate.zoo()} performs aggregations of \emph{zoo} series over non-overlapping intervals defined by a vector of aggregation groups (minutes, hours, days, etc.), 
      \vskip1ex
      For example, \texttt{aggregate.zoo()} can calculate the average monthly returns, 
      <<echo=(-(1:3)),eval=FALSE>>=
set.seed(1121)  # reset random number generator
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
library(zoo)  # load package zoo
# Create zoo time series of random returns
in_dex <- Sys.Date() + 0:365
zoo_series <- 
  zoo(rnorm(NROW(in_dex)), order.by=in_dex)
# Create monthly dates
dates_agg <- as.Date(as.yearmon(index(zoo_series)))
# Perform monthly mean aggregation
zoo_agg <- aggregate(zoo_series, by=dates_agg, 
                     FUN=mean)
# merge with original zoo - union of dates
zoo_agg <- cbind(zoo_series, zoo_agg)
# replace NA's using locf
zoo_agg <- na.locf(zoo_agg, na.rm=FALSE)
# extract aggregated zoo
zoo_agg <- zoo_agg[index(zoo_series), 2]
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/zoo_agg-1}
      \vspace{-7em}
      <<zoo_agg,echo=(-(1:1)),eval=FALSE,fig.show="hide">>=
# library(HighFreq)  # load package HighFreq
# Plot original and aggregated cumulative returns
plot(cumsum(zoo_series), xlab="", ylab="")
lines(cumsum(zoo_agg), lwd=2, col="red")
# Add legend
legend("topright", inset=0.05, cex=0.8, bty="n",
       title="Aggregated Prices", 
       leg=c("orig prices", "agg prices"), 
       lwd=2, bg="white", col=c("black", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Interpolating \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{zoo} has two functions for replacing \texttt{NA} values using interpolation:
      \begin{itemize}
        \item \texttt{na.approx()} performs linear interpolation,
        \item \texttt{na.spline()} performs spline interpolation,
      \end{itemize}
      \vspace{-1em}
      <<zoo_interpol,echo=(-(1:1)),eval=FALSE,fig.show="hide">>=
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# Perform monthly mean aggregation
zoo_agg <- aggregate(zoo_series, by=dates_agg, 
                     FUN=mean)
# merge with original zoo - union of dates
zoo_agg <- cbind(zoo_series, zoo_agg)
# replace NA's using linear interpolation
zoo_agg <- na.approx(zoo_agg)
# extract interpolated zoo
zoo_agg <- zoo_agg[index(zoo_series), 2]
# Plot original and interpolated zoo
plot(cumsum(zoo_series), xlab="", ylab="")
lines(cumsum(zoo_agg), lwd=2, col="red")
# Add legend
legend("topright", inset=0.05, cex=0.8, title="Interpolated Prices", 
       leg=c("orig prices", "interpol prices"), lwd=2, bg="white", 
       col=c("black", "red"), bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/zoo_interpol-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Over \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{zoo} has several functions for rolling calculations:
      \begin{itemize}
        \item \texttt{rollapply()} performing aggregations over a rolling (sliding) interval,
        \item \texttt{rollmean()} calculating rolling means,
        \item \texttt{rollmedian()} calculating rolling median,
        \item \texttt{rollmax()} calculating rolling max,
      \end{itemize}
      \vspace{-1em}
      <<zoo_roll,echo=(-(1:1)),eval=FALSE,fig.show="hide">>=
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# "mean" aggregation over interval with width=11
zoo_mean <- rollapply(zoo_series, width=11, 
                      FUN=mean, align="right")
# merge with original zoo - union of dates
zoo_mean <- cbind(zoo_series, zoo_mean)
# replace NA's using na.locf
zoo_mean <- na.locf(zoo_mean, na.rm=FALSE, fromLast=TRUE)
# extract mean zoo
zoo_mean <- zoo_mean[index(zoo_series), 2]
# Plot original and interpolated zoo
plot(cumsum(zoo_series), xlab="", ylab="")
lines(cumsum(zoo_mean), lwd=2, col="red")
# Add legend
legend("topright", inset=0.05, cex=0.8, title="Mean Prices", 
       leg=c("orig prices", "mean prices"), lwd=2, bg="white", 
       col=c("black", "red"), bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/zoo_roll-1}
      \vspace{-3em}
      The argument \texttt{align="right"} determines that aggregations are taken from the past,
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Estimating and Filtering Data}


%%%%%%%%%%%%%%%
\subsection{Robust Estimators of Dispersion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability), defined using the median instead of the mean:
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(x_i - \operatorname{median}(\mathbf{x})))
      \end{displaymath}
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance.
      \vskip1ex
      For normally distributed data the \emph{MAD} has a larger standard error than the standard deviation.
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}.
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$.
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
n_rows <- 1000
r_norm <- rnorm(n_rows)
sd(r_norm)
mad(r_norm)
median(abs(r_norm - median(r_norm)))
median(abs(r_norm - median(r_norm)))/qnorm(0.75)
# Bootstrap of sd and mad estimators
boot_strap <- sapply(1:10000, function(x) {
  boot_sample <-
    r_norm[sample.int(n_rows, replace=TRUE)]
  c(sd=sd(boot_sample), mad=mad(boot_sample))
})  # end sapply
boot_strap <- t(boot_strap)
# Analyze bootstrapped variance
head(boot_strap)
sum(is.na(boot_strap))
# Means and standard errors from bootstrap
apply(boot_strap, MARGIN=2, 
      function(x) c(mean=mean(x), std_error=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # load package parallel
n_cores <- detectCores() - 1  # number of cores
clus_ter <- makeCluster(n_cores)  # initialize compute cluster
boot_strap <- parLapply(clus_ter, 1:10000, 
  function(x, r_norm) {
    boot_sample <- 
      r_norm[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(boot_sample), mad=mad(boot_sample))
  }, r_norm=r_norm)  # end parLapply
# Parallel bootstrap under Mac-OSX or Linux
boot_strap <- mclapply(1:10000, 
  function(x) {
    boot_sample <- 
      r_norm[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(boot_sample), mad=mad(boot_sample))
  }, mc.cores=n_cores)  # end mclapply
stopCluster(clus_ter)  # Stop R processes over cluster
boot_strap <- rutils::do_call(rbind, boot_strap)
# Means and standard errors from bootstrap
apply(boot_strap, MARGIN=2, 
      function(x) c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Hampel Filter} for Outlier Detection}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Hampel filter} uses the \emph{MAD} dispersion measure to detect outliers in data.
      \vskip1ex
      The \emph{Hampel z-score} is equal to the deviation from the median divided by the \emph{MAD}: 
      \begin{displaymath}
        z_i = (x_i - \operatorname{median}(\mathbf{x})) / \operatorname{MAD}
      \end{displaymath}
      A time series of \emph{z-scores} over past data can be calculated using a rolling look-back window.
      <<echo=TRUE,eval=FALSE>>=
price_s <- rutils::etf_env$VTI[, 4]
# Define look-back window and a half window
win_dow <- 11
# Calculate time series of medians
media_n <- TTR::runMedian(price_s, n=win_dow)
# Plot prices and medians
dygraphs::dygraph(cbind(price_s, media_n), main="VTI median") %>% 
  dyOptions(colors=c("black", "red"))
# Calculate time series of z-scores
ma_d <- TTR::runMAD(price_s, n=win_dow)
z_scores <- (price_s - media_n)/ma_d
z_scores[1:win_dow, ] <- 0
tail(z_scores, win_dow)
range(z_scores)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/hampel_zscores.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot histogram of z-scores
x11(width=6, height=5)
histo_gram <- hist(z_scores, col="lightgrey", 
  xlab="z-scores", breaks=50, xlim=c(-4, 4), 
  ylab="frequency", freq=FALSE, 
  main="Z-scores histogram")
lines(density(z_scores, adjust=1.5), 
      lwd=3, col="blue")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{One-sided and Two-sided Data Filters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Filters calculated over past data are referred to as \emph{one-sided} filters, and they are appropriate for filtering real-time data.
      \vskip1ex
      Filters calculated over both past and future data are referred to as \emph{two-sided} (centered) filters, and they are appropriate for filtering historical data.
      \vskip1ex
      The functions \texttt{TTR::runMedian()} and \texttt{TTR::runMAD()} calculate the rolling medians and \emph{MAD} using a trailing look-back interval over past data.
      \vskip1ex
      If the rolling medians and \emph{MAD} are advanced (shifted backward) in time, then they are calculated over both past and future data (centered).
      \vskip1ex
      The function \texttt{rutils::lag\_it()} with a negative \texttt{lagg} parameter value advances (shifts back) future data points to the present.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate one-sided Hampel z-scores
media_n <- TTR::runMedian(price_s, n=win_dow)
ma_d <- TTR::runMAD(price_s, n=win_dow)
z_scores <- (price_s - media_n)/ma_d
z_scores[1:win_dow, ] <- 0
tail(z_scores, win_dow)
range(z_scores)
# Calculate two-sided Hampel z-scores
half_window <- win_dow %/% 2
media_n <- rutils::lag_it(media_n, lagg=-half_window)
ma_d <- rutils::lag_it(ma_d, lagg=-half_window)
z_scores <- (price_s - media_n)/ma_d
z_scores[1:win_dow, ] <- 0
tail(z_scores, win_dow)
range(z_scores)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Classifying Outliers Using the \protect\emph{Hampel Filter}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The data points whose absolute \emph{z-scores} exceed a \emph{threshold value} are classified as outliers.
      \vskip1ex
      This procedure is a \emph{classifier}, which classifies the prices as either good or bad data points.
      \vskip1ex
      We can add jumps to the data to test the performance of the classifier.
      \vskip1ex
      The null hypothesis is that all the prices are good data points.
      \vskip1ex
      A positive result corresponds to rejecting the null hypothesis, while a negative result corresponds to accepting the null hypothesis.
      \vskip1ex
      The classifications are subject to two different types of errors: \emph{type I} and \emph{type II} errors.
      \vskip1ex
      A \emph{type I} error is the incorrect rejection of a \texttt{TRUE} null hypothesis (i.e. a "false positive"), for example, when the data is good, but it's classified as bad.
      \vskip1ex
      A \emph{type II} error is the incorrect acceptance of a \texttt{FALSE} null hypothesis (i.e. a "false negative"), for example, when the data is bad, but it's classified as good.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define threshold value
thresh_old <- 3
# Calculate number of prices classified as bad data
ba_d <- (abs(z_scores) > thresh_old)
sum(ba_d)
# Add 200 random price jumps into price_s
set.seed(1121)
n_bad <- 200
jump_s <- logical(NROW(price_s))
jump_s[sample(x=NROW(jump_s), size=n_bad)] <- TRUE
price_s[jump_s] <- price_s[jump_s]*
  sample(c(0.95, 1.05), size=n_bad, replace=TRUE)
# Plot prices and medians
dygraphs::dygraph(cbind(price_s, media_n), main="VTI median") %>% 
  dyOptions(colors=c("black", "red"))
# Calculate time series of z-scores
media_n <- TTR::runMedian(price_s, n=win_dow)
ma_d <- TTR::runMAD(price_s, n=win_dow)
z_scores <- (price_s - media_n)/ma_d
z_scores[1:win_dow, ] <- 0
# Calculate number of prices classified as bad data
ba_d <- (abs(z_scores) > thresh_old)
sum(ba_d)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Confusion Matrix of a Classification Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The confusion matrix summarizes the performance of a classification model on a set of test data for which the true values are known.
      \vskip1ex
      The \emph{true positive} rate (known as the \emph{sensitivity}) is the fraction of \texttt{FALSE} null hypothesis cases that are correctly classified as \texttt{FALSE}.
      \vskip1ex
      The \emph{false negative} rate is the fraction of \texttt{FALSE} null hypothesis cases that are incorrectly classified as \texttt{TRUE} (\emph{type II} error).
      \vskip1ex
      The sum of the \emph{true positive} plus the \emph{false negative} rate is equal to \texttt{1}.
      \vskip1ex
      The \emph{true negative} rate (known as the \emph{specificity}) is the fraction of \texttt{TRUE} null hypothesis cases that are correctly classified as \texttt{TRUE}.
      \vskip1ex
      The \emph{false positive} rate is the fraction of \texttt{TRUE} null hypothesis cases that are incorrectly classified as \texttt{FALSE} (\emph{type I} error).
      \vskip1ex
      The sum of the \emph{true negative} plus the \emph{false positive} rate is equal to \texttt{1}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate confusion matrix
table(jump_s, ba_d)
sum(ba_d)
# FALSE positive (type I error)
sum(jump_s & !ba_d)
# FALSE negative (type II error)
sum(!jump_s & ba_d)
      @
      \newcommand\MyBox[2]{
        \fbox{\lower0.75cm
          \vbox to 1.2cm{\vfil
            \hbox to 1.7cm{\parbox{\textwidth}{#1\\#2}}
            \vfil}
        }
      }
      \renewcommand\arraystretch{0.3}
      \setlength\tabcolsep{0pt}
      \begin{tabular}{c >{\bfseries}r @{\hspace{0.5em}}c @{\hspace{0.4em}}c @{\hspace{0.5em}}l}
      \multirow{10}{*}{\parbox{0.5cm}{\bfseries Actual}} &
      & \multicolumn{2}{c}{\bfseries Forecast} & \\
      & & \bfseries FALSE & \bfseries TRUE \\
      & FALSE & \MyBox{True Positive}{(sensitivity)} & \MyBox{False Negative}{(type II error)} \\[2.4em]
      & TRUE & \MyBox{False Positive}{(type I error)} & \MyBox{True Negative}{(specificity)}
      \end{tabular}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Receiver Operating Characteristic (ROC) Curve}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The ROC curve is the plot of the \emph{true positive} rate, as a function of the \emph{false positive} rate, and illustrates the performance of a binary classifier.
      \vskip1ex
      The area under the ROC curve (AUC) is a measure of the performance of a binary classification model.
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Confusion matrix as function of thresh_old
con_fuse <- function(actu_al, z_scores, thresh_old) {
    confu_sion <- table(actu_al, (abs(z_scores) > thresh_old))
    confu_sion <- confu_sion / rowSums(confu_sion)
    c(typeI=confu_sion[2, 1], typeII=confu_sion[1, 2])
  }  # end con_fuse
con_fuse(jump_s, z_scores, thresh_old=thresh_old)
# Define vector of thresholds
threshold_s <- seq(from=0.2, to=5.0, by=0.2)
# Calculate error rates
error_rates <- sapply(threshold_s, con_fuse,
  actu_al=jump_s,
  z_scores=z_scores)  # end sapply
error_rates <- t(error_rates)
# Calculate area under ROC curve (AUC)
true_pos <- sort(1- c(error_rates[, "typeII"], 0))
true_pos <- (true_pos + rutils::lag_it(true_pos))/2
false_pos <- c(sort(error_rates[, "typeI"]), 1)
false_pos <- rutils::diff_it(false_pos)
sum(true_pos*false_pos)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/hampel_roc.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot ROC Curve for Defaults
plot(x=error_rates[, "typeI"],
     y=1-error_rates[, "typeII"],
     xlab="FALSE positive rate",
     ylab="TRUE positive rate",
     xlim=c(0, 1), ylim=c(0, 1),
     main="ROC Curve for Hampel Classifier",
     type="l", lwd=3, col="blue")
abline(a=0.0, b=1.0, lwd=3, col="orange")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Active Investment Strategies}


%%%%%%%%%%%%%%%
\subsection{Static Stock and Bond Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the past, the returns of stocks and bonds have usually been negatively correlated.
      \vskip1ex
      Static portfolios consisting of stocks and bonds provide a much better risk versus return tradeoff than either of the assets separately.
      \vskip1ex
      The static weights depend on the investment horizon, with a greater allocation to bonds for a shorter investment horizon.
      \vskip1ex
      Active investment strategies are expected to outperform static stock and bond portfolios. 
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)  # load package rutils
# Calculate ETF returns
re_turns <- 
  rutils::etf_env$re_turns[, c("IEF", "VTI")]
re_turns <- na.omit(re_turns)
re_turns <- cbind(re_turns, 
  0.6*re_turns[, "IEF"]+0.4*re_turns[, "VTI"])
colnames(re_turns)[3] <- "combined"
# Calculate correlations
cor(re_turns)
# Calculate Sharpe ratios
sqrt(252)*sapply(re_turns, function(x) mean(x)/sd(x))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/static_stocks_bonds.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate prices from returns
price_s <- lapply(re_turns, 
  function(x) exp(cumsum(x)))
price_s <- rutils::do_call(cbind, price_s)
# Plot prices
dygraphs::dygraph(price_s, main="Stock and Bond Portfolio") %>% 
  dyOptions(colors=c("green","blue","green")) %>%
  dySeries("combined", color="red", strokeWidth=2) %>%
  dyLegend(show="always")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Moving Average Technical Indicators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Volume-Weighted Average Price (\emph{VWAP}) is defined as the sum of prices multiplied by trading volumes, divided by the sum of volumes.
      \vskip1ex
      Moving averages (such as \emph{VWAP}) are often used to define technical indicators (trading signals).
      <<echo=TRUE,eval=FALSE>>=
# calculate open, close, and lagged prices
oh_lc <- rutils::etf_env$VTI
op_en <- quantmod::Op(oh_lc)
cl_ose <- quantmod::Cl(oh_lc)
star_t <- as.numeric(cl_ose[1, ])
prices_lag <- rutils::lag_it(cl_ose)
# define aggregation interval and calculate VWAP
look_back <- 150
VTI_vwap <- HighFreq::roll_vwap(oh_lc, 
              look_back=look_back)
# calculate VWAP indicator
in_dic <- sign(cl_ose - VTI_vwap)
# determine dates right after VWAP has crossed prices
trade_dates <- (rutils::diff_it(in_dic) != 0)
trade_dates <- which(trade_dates) + 1
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vwap_indic.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# plot prices and VWAP
chart_Series(x=cl_ose, 
  name="VTI prices", col="orange")
add_TA(VTI_vwap, on=1, lwd=2, col="blue")
legend("top", legend=c("VTI", "VWAP"), 
  bg="white", lty=1, lwd=6, 
  col=c("orange", "blue"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Moving Average Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a trend-following \emph{Moving Average Crossover} strategy, when the current price crosses above the \emph{VWAP}, then the strategy switches its position to long risk, and vice versa.
      \vskip1ex
      The strategy trades at the \emph{Open} price in the next period after prices cross the \emph{VWAP}, to reflect that in practice it's impossible to trade immediately.
      <<echo=TRUE,eval=FALSE>>=
# Calculate positions, either: -1, 0, or 1
position_s <- rep(NA_integer_, NROW(oh_lc))
position_s[1] <- 0
position_s[trade_dates] <- in_dic[trade_dates]
position_s <- na.locf(position_s)
position_s <- xts(position_s, order.by=index(oh_lc))
pos_lagged <- rutils::lag_it(position_s)
# Calculate daily profits and losses
pnl_s <- pos_lagged*(cl_ose - prices_lag)
pnl_s[trade_dates] <- pos_lagged[trade_dates] * 
  (op_en[trade_dates] - prices_lag[trade_dates]) +
  position_s[trade_dates] * 
  (cl_ose[trade_dates] - op_en[trade_dates])
# Calculate percentage returns
pnl_s <- pnl_s/cl_ose
# Calculate annualized Sharpe ratio of strategy returns
sqrt(252)*sum(pnl_s)/sd(pnl_s)/NROW(pnl_s)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vwap_strat.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot prices and VWAP
pnl_s <- xts(as.numeric(cl_ose[1])*exp(cumsum(pnl_s)), order.by=index(oh_lc))
chart_Series(x=cl_ose, name="VTI prices", col="orange")
add_TA(pnl_s, on=1, lwd=2, col="blue")
add_TA(position_s > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(position_s < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=c("VTI", "VWAP strategy"), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=c("orange", "blue"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{EWMA} Price Technical Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Exponentially Weighted Moving Average Price} (\emph{EWMA}) is defined as the weighted average of prices over a rolling interval:
      \begin{displaymath}
        P_i^{EWMA} = (1-\exp(-\lambda)) \sum_{j=0}^{\infty} \exp(-\lambda j) P_{i-j}
      \end{displaymath}
      Where the decay parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with larger values of $\lambda$ producing faster decay, giving more weight to recent prices, and vice versa.
      <<echo=TRUE,eval=FALSE>>=
# Define length for weights and decay parameter
wid_th <- 352
lamb_da <- 0.01
# Calculate EWMA prices
weight_s <- exp(-lamb_da*1:wid_th)
weight_s <- weight_s/sum(weight_s)
ew_ma <- stats::filter(cl_ose, filter=weight_s, sides=1)
ew_ma[1:(wid_th-1)] <- ew_ma[wid_th]
ew_ma <- xts(cbind(cl_ose, ew_ma), order.by=index(oh_lc))
colnames(ew_ma) <- c("VTI", "VTI EWMA")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_indic.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot EWMA prices with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(ew_ma["2007/2010"], theme=plot_theme, 
             name="EWMA prices")
legend("bottomleft", legend=colnames(ew_ma), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating The \protect\emph{EWMA} Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a trend-following \emph{EWMA Crossover} strategy, the risk position switches depending if the current price is above or below the \emph{EWMA}.
      \vskip1ex
      If the current price crosses above the \emph{EWMA}, then the strategy switches its risk position to a fixed unit of long risk, and if it crosses below, to a fixed unit of short risk.
      \vskip1ex
      The strategy holds the same position until the \emph{EWMA} crosses over the current price (either from above or below), and then it switches its position.
      \vskip1ex
      The strategy is therefore always either in a long risk, or in a short risk position.
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# determine dates right after EWMA has crossed prices
in_dic <- sign(cl_ose - ew_ma[, 2])
trade_dates <- (rutils::diff_it(in_dic) != 0)
trade_dates <- which(trade_dates) + 1
# calculate positions, either: -1, 0, or 1
position_s <- rep(NA_integer_, NROW(cl_ose))
position_s[1] <- 0
position_s[trade_dates] <- 
  rutils::lag_it(in_dic)[trade_dates]
position_s <- na.locf(position_s)
position_s <- xts(position_s, order.by=index(oh_lc))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_strat.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# plot EWMA prices with position shading
chart_Series(ew_ma["2007/2010"], theme=plot_theme, 
             name="EWMA prices")
add_TA(position_s > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(position_s < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("bottomleft", legend=colnames(ew_ma), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating the Transaction Costs of Trading}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bid-offer spread} can be expressed as a percentage, equal to the difference between the \emph{offer} price minus the \emph{bid}, divided by the \emph{mid} price.
      \vskip1ex
      For institutional investors the \emph{bid-offer spread} is often estimated to be about \texttt{10} basis points (bps).
      \vskip1ex
      In reality the \emph{bid-offer spread} is not static and depends on many factors, such as market liquidity (trading volume), volatility, and time of day.
      \vskip1ex
      Broker commissions are an additional trading cost, but they depend on the size of the trades and on the type of investors, with institutional investors usually enjoying smaller commissions.
    \column{0.5\textwidth}
      <<echo=TRUE,eval=FALSE>>=
# bid_offer is equal to 10 bps for liquid ETFs
bid_offer <- 0.001
# calculate open and lagged prices
op_en <- Op(oh_lc)
prices_lag <- rutils::lag_it(cl_ose)
pos_lagged <- rutils::lag_it(position_s)
# calculate the transaction cost for one share
cost_s <- 0.0*position_s
cost_s[trade_dates] <- 
  0.5*bid_offer*abs(pos_lagged[trade_dates] - 
  position_s[trade_dates])*op_en[trade_dates]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of \protect\emph{EWMA} Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The strategy trades at the \emph{Open} price on the next day after prices cross the \emph{EWMA}, since in practice it may not be possible to trade immediately.
      \vskip1ex
      The Profit and Loss (\emph{PnL}) on a trade date is the sum of the realized \emph{PnL} from closing the old position, plus the unrealized \emph{PnL} after opening the new position.
      <<echo=TRUE,eval=FALSE>>=
# Calculate daily profits and losses
re_turns <- pos_lagged*(cl_ose - prices_lag)
re_turns[trade_dates] <- 
  pos_lagged[trade_dates] * 
  (op_en[trade_dates] - prices_lag[trade_dates]) +
  position_s[trade_dates] * 
  (cl_ose[trade_dates] - op_en[trade_dates]) -
  cost_s
# Calculate percentage returns
re_turns <- re_turns/cl_ose
# Calculate annualized Sharpe ratio of strategy returns
sqrt(252)*sum(re_turns)/sd(re_turns)/NROW(re_turns)
pnl_s <- as.numeric(cl_ose[1])*exp(cumsum(re_turns))
pnl_s <- cbind(cl_ose, pnl_s)
colnames(pnl_s) <- c("VTI", "EWMA PnL")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_strat_pnl.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot EWMA PnL with position shading
chart_Series(pnl_s, theme=plot_theme, 
             name="Performance of EWMA Strategy")
add_TA(position_s > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(position_s < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=colnames(pnl_s), 
       inset=0.05, bg="white", lty=1, lwd=6, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Function for \protect\emph{EWMA} Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The \emph{EWMA} strategy can be simulated by a single function, which allows the analysis of its performance depending on its parameters,
      \vskip1ex
      The function \texttt{simu\_ewma()} performs a simulation of the \emph{EWMA} strategy, given an \emph{OHLC} time series of prices, and a decay parameter $\lambda$, 
      \vskip1ex
      The function \texttt{simu\_ewma()} returns the \emph{EWMA} strategy positions and returns, in a two-column \emph{xts} time series, 
    \column{0.6\textwidth}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
simu_ewma <- function(oh_lc, lamb_da=0.01, wid_th=251, bid_offer=0.001, tre_nd=1) {
  # calculate EWMA prices
  weight_s <- exp(-lamb_da*1:wid_th)
  weight_s <- weight_s/sum(weight_s)
  cl_ose <- quantmod::Cl(oh_lc)
  ew_ma <- stats::filter(as.numeric(cl_ose), filter=weight_s, sides=1)
  ew_ma[1:(wid_th-1)] <- ew_ma[wid_th]
  # determine dates right after EWMA has crossed prices
  in_dic <- tre_nd*xts::xts(sign(as.numeric(cl_ose) - ew_ma), order.by=index(oh_lc))
  trade_dates <- (rutils::diff_it(in_dic) != 0)
  trade_dates <- which(trade_dates) + 1
  trade_dates <- trade_dates[trade_dates<NROW(oh_lc)]
  # calculate positions, either: -1, 0, or 1
  position_s <- rep(NA_integer_, NROW(cl_ose))
  position_s[1] <- 0
  position_s[trade_dates] <- rutils::lag_it(in_dic)[trade_dates]
  position_s <- xts::xts(na.locf(position_s), order.by=index(oh_lc))
  op_en <- quantmod::Op(oh_lc)
  prices_lag <- rutils::lag_it(cl_ose)
  pos_lagged <- rutils::lag_it(position_s)
  # calculate transaction costs
  cost_s <- 0.0*position_s
  cost_s[trade_dates] <- 0.5*bid_offer*abs(pos_lagged[trade_dates] - position_s[trade_dates])*op_en[trade_dates]
  # calculate daily profits and losses
  re_turns <- pos_lagged*(cl_ose - prices_lag)
  re_turns[trade_dates] <- pos_lagged[trade_dates] * (op_en[trade_dates] - prices_lag[trade_dates]) + position_s[trade_dates] * (cl_ose[trade_dates] - op_en[trade_dates]) - cost_s
  # Calculate percentage returns
  re_turns <- re_turns/cl_ose
  out_put <- cbind(position_s, re_turns)
  colnames(out_put) <- c("positions", "returns")
  out_put
}  # end simu_ewma
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Multiple Trend-following \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{EWMA} strategies can be simulated by calling the function \texttt{simu\_ewma()} in a loop over a vector of $\lambda$ parameters.
      \vskip1ex
      But \texttt{simu\_ewma()} returns an \emph{xts} time series, and \texttt{sapply()} cannot merge \emph{xts} time series together.
      \vskip1ex
      So instead the loop is performed using \texttt{lapply()} which returns a list of \emph{xts}, and the list is merged into a single \emph{xts} using functions \texttt{rutils::do\_call()} and \texttt{cbind()}.
      <<echo=TRUE,eval=FALSE>>=
source("C:/Develop/R/lecture_slides/scripts/ewma_model.R")
lamb_das <- seq(0.0001, 0.05, 0.005)
# perform lapply() loop over lamb_das
pnl_s <- lapply(lamb_das, function(lamb_da) {
  # simulate EWMA strategy and calculate re_turns
  star_t*exp(cumsum(simu_ewma(oh_lc=oh_lc, 
    lamb_da=lamb_da, wid_th=wid_th)[, "returns"]))
})  # end lapply
pnl_s <- rutils::do_call(cbind, pnl_s)
colnames(pnl_s) <- paste0("lambda=", lamb_das)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_trend_returns.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# plot EWMA strategies with custom line colors
column_s <- seq(1, NCOL(pnl_s), by=3)
plot_theme <- chart_theme()
plot_theme$col$line.col <- 
  colorRampPalette(c("blue", "red"))(NROW(column_s))
chart_Series(pnl_s[, column_s], 
  theme=plot_theme, name="Cumulative Returns of EWMA Strategies")
legend("topleft", legend=colnames(pnl_s[, column_s]), 
  inset=0.1, bg="white", cex=0.8, lwd=rep(6, NCOL(pnl_s)), 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating \protect\emph{EWMA} Strategies Using Parallel Computing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Simulating \emph{EWMA} strategies naturally lends itself to parallel computing, since the simulations are independent from each other.
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs apply loops under \emph{Windows}, using parallel computing on several CPU cores.
      \vskip1ex
      The resulting list of time series can then be collapsed into a single \emph{xts} series using the functions \texttt{rutils::do\_call()} and \texttt{cbind()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# initialize compute cluster under Windows
library(parallel)
clus_ter <- makeCluster(detectCores()-1)
clusterExport(clus_ter, 
  varlist=c("oh_lc", "wid_th", "simu_ewma"))
# perform parallel loop over lamb_das under Windows
pnl_s <- parLapply(clus_ter, lamb_das, function(lamb_da) {
  library(quantmod)
  # simulate EWMA strategy and calculate re_turns
  star_t*exp(cumsum(simu_ewma(
    oh_lc=oh_lc, lamb_da=lamb_da, wid_th=wid_th)[, "returns"]))
})  # end parLapply
# perform parallel loop over lamb_das under Mac-OSX or Linux
re_turns <- mclapply(lamb_das, function(lamb_da) {
  library(quantmod)
  # simulate EWMA strategy and calculate re_turns
  star_t*exp(cumsum(simu_ewma(
    oh_lc=oh_lc, lamb_da=lamb_da, wid_th=wid_th)[, "returns"]))
})  # end mclapply
stopCluster(clus_ter)  # stop R processes over cluster under Windows
pnl_s <- rutils::do_call(cbind, pnl_s)
colnames(pnl_s) <- paste0("lambda=", lamb_das)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of Trend-following \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sharpe} ratios of \emph{EWMA} strategies with different $\lambda$ parameters can be calculated by performing an \texttt{sapply()} loop over the \emph{columns} of returns.
      \vskip1ex
      \texttt{sapply()} treats the columns of \emph{xts} time series as list elements, and loops over the columns.
      \vskip1ex
      Performing loops in \texttt{R} over the \emph{columns} of returns is acceptable, but \texttt{R} loops over the \emph{rows} of returns should be avoided.
      \vskip1ex
      The performance of trend-following \emph{EWMA} strategies depends on the $\lambda$ parameter, with larger $\lambda$ parameters performing worse than smaller ones.
      <<echo=TRUE,eval=FALSE>>=
sharpe_ratios <- sqrt(252)*sapply(pnl_s, function(x_ts) {
  # calculate annualized Sharpe ratio of strategy returns
  x_ts <- rutils::diff_it(log(x_ts))
  sum(x_ts)/sd(x_ts)
})/NROW(pnl_s)  # end sapply
plot(x=lamb_das, y=sharpe_ratios, t="l", 
     main="Performance of EWMA trend-following strategies 
     as function of the decay parameter lambda")
trend_returns <- rutils::diff_it(log(pnl_s))
trend_sharpe <- sharpe_ratios
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_trend_performance.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Trend-following \protect\emph{EWMA} Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The best performing trend-following \emph{EWMA} strategy has a relatively small $\lambda$ parameter, corresponding to slower weight decay (giving more weight to past prices), and producing less frequent trading.
      <<echo=TRUE,eval=FALSE>>=
# Simulate best performing strategy
ewma_trend <- simu_ewma(oh_lc=oh_lc, 
  lamb_da=lamb_das[which.max(sharpe_ratios)], 
  wid_th=wid_th)
position_s <- ewma_trend[, "positions"]
pnl_s <- star_t*exp(cumsum(ewma_trend[, "returns"]))
pnl_s <- cbind(cl_ose, pnl_s)
colnames(pnl_s) <- c("VTI", "EWMA PnL")
# plot EWMA PnL with position shading
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(pnl_s, theme=plot_theme, 
             name="Performance of Trend-following EWMA Strategy")
add_TA(position_s > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(position_s < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=colnames(pnl_s), 
  inset=0.05, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_trend_best.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Multiple Mean-reverting \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{EWMA} strategies can be backtested by calling the function \texttt{simu\_ewma()} in a loop over a vector of $\lambda$ parameters.
      \vskip1ex
      But \texttt{simu\_ewma()} returns an \emph{xts} time series, and \texttt{sapply()} cannot merge \emph{xts} time series together.
      \vskip1ex
      So instead the loop is performed using \texttt{lapply()} which returns a list of \emph{xts}, and the list is merged into a single \emph{xts} using functions \texttt{rutils::do\_call()} and \texttt{cbind()}.
      <<echo=TRUE,eval=FALSE>>=
source("C:/Develop/R/lecture_slides/scripts/ewma_model.R")
lamb_das <- seq(0.05, 1.0, 0.05)
# perform lapply() loop over lamb_das
pnl_s <- lapply(lamb_das, function(lamb_da) {
  # backtest EWMA strategy and calculate re_turns
  star_t*exp(cumsum(simu_ewma(
    oh_lc=oh_lc, lamb_da=lamb_da, wid_th=wid_th, tre_nd=(-1))[, "returns"]))
})  # end lapply
pnl_s <- rutils::do_call(cbind, pnl_s)
colnames(pnl_s) <- paste0("lambda=", lamb_das)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_revert_returns.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# plot EWMA strategies with custom line colors
column_s <- seq(1, NCOL(pnl_s), by=4)
plot_theme <- chart_theme()
plot_theme$col$line.col <- 
  colorRampPalette(c("blue", "red"))(NROW(column_s))
chart_Series(pnl_s[, column_s], 
  theme=plot_theme, name="Cumulative Returns of Mean-reverting EWMA Strategies")
legend("topleft", legend=colnames(pnl_s[, column_s]), 
  inset=0.1, bg="white", cex=0.8, lwd=rep(6, NCOL(pnl_s)), 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of Mean-reverting \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sharpe} ratios of \emph{EWMA} strategies with different $\lambda$ parameters can be calculated by performing an \texttt{sapply()} loop over the \emph{columns} of returns.
      \vskip1ex
      \texttt{sapply()} treats the columns of \emph{xts} time series as list elements, and loops over the columns.
      \vskip1ex
      Performing loops in \texttt{R} over the \emph{columns} of returns is acceptable, but \texttt{R} loops over the \emph{rows} of returns should be avoided.
      \vskip1ex
      The performance of mean-reverting \emph{EWMA} strategies depends on the $\lambda$ parameter, with performance decreasing for very small or very large $\lambda$ parameters.
      \vskip1ex
      For too large $\lambda$ parameters, the trading frequency is too high, causing high transaction costs.
      \vskip1ex
      For too small $\lambda$ parameters, the trading frequency is too low, causing the strategy to miss profitable trades.
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_revert_performance.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
sharpe_ratios <- sqrt(252)*sapply(pnl_s, function(x_ts) {
  # calculate annualized Sharpe ratio of strategy returns
  x_ts <- rutils::diff_it(log(x_ts))
  sum(x_ts)/sd(x_ts)
})/NROW(pnl_s)  # end sapply
plot(x=lamb_das, y=sharpe_ratios, t="l", 
     main="Performance of EWMA mean-reverting strategies 
     as function of the decay parameter lambda")
revert_returns <- rutils::diff_it(log(pnl_s))
revert_sharpe <- sharpe_ratios
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Mean-reverting \protect\emph{EWMA} Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Reverting the rules of the trend-following \emph{EWMA} strategy creates a mean-reverting strategy.
      \vskip1ex
      The best performing mean-reverting \emph{EWMA} strategy has a relatively large $\lambda$ parameter, corresponding to faster weight decay (giving more weight to recent prices), and producing more frequent trading.
      \vskip1ex
      But a too large $\lambda$ parameter also causes very high trading frequency, and high transaction costs.
      <<echo=TRUE,eval=FALSE>>=
# backtest best performing strategy
ewma_revert <- simu_ewma(oh_lc=oh_lc, 
  lamb_da=lamb_das[which.max(sharpe_ratios)],
  wid_th=wid_th, tre_nd=(-1))
position_s <- ewma_revert[, "positions"]
pnl_s <- star_t*exp(cumsum(ewma_revert[, "returns"]))
pnl_s <- cbind(cl_ose, pnl_s)
colnames(pnl_s) <- c("VTI", "EWMA PnL")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_revert_best.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# plot EWMA PnL with position shading
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(pnl_s, theme=plot_theme, 
             name="Performance of Mean-reverting EWMA Strategy")
add_TA(position_s > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(position_s < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=colnames(pnl_s), 
  inset=0.05, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Combining Trend-following and Mean-reverting Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The returns of trend-following and mean-reverting strategies are usually negatively correlated to each other, so combining them can achieve significant diversification of risk.
      <<echo=TRUE,eval=FALSE>>=
# calculate correlation between trend-following and mean-reverting strategies
trend_ing <- ewma_trend[, "returns"]
colnames(trend_ing) <- "trend"
revert_ing <- ewma_revert[, "returns"]
colnames(revert_ing) <- "revert"
close_rets <- rutils::diff_it(log(cl_ose))
cor(cbind(trend_ing, revert_ing, close_rets))
# Calculate combined strategy
com_bined <- trend_ing + revert_ing
colnames(com_bined) <- "combined"
# calculate annualized Sharpe ratio of strategy returns
re_turns <- cbind(close_rets, trend_ing, revert_ing, com_bined)
sqrt(252)*sapply(re_turns, function(x_ts) 
  sum(x_ts)/sd(x_ts))/NROW(com_bined)
pnl_s <- lapply(re_turns, function(x_ts) star_t*exp(cumsum(x_ts)))
pnl_s <- rutils::do_call(cbind, pnl_s)
colnames(pnl_s) <- c("VTI", "trending", "reverting", "EWMA combined PnL")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_combined.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue", "green", "magenta2")
chart_Series(pnl_s, theme=plot_theme, 
             name="Performance of Combined EWMA Strategies")
legend("topleft", legend=colnames(pnl_s),
       inset=0.05, bg="white", lty=1, lwd=6, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Ensemble of \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Instead of selecting the best performing \emph{EWMA} strategy, one can choose a weighted average of strategies (ensemble), which corresponds to allocating positions according to the weights.
      \vskip1ex
      The weights can be chosen to be proportional to the Sharpe ratios of the \emph{EWMA} strategies.
      <<echo=TRUE,eval=FALSE>>=
sharpe_ratios <- c(trend_sharpe, revert_sharpe)
weight_s <- sharpe_ratios
weight_s[weight_s<0] <- 0
weight_s <- weight_s/sum(weight_s)
re_turns <- cbind(trend_returns, revert_returns)
avg_returns <- re_turns %*% weight_s
avg_returns <- xts(avg_returns, order.by=index(re_turns))
pnl_s <- star_t*exp(cumsum(avg_returns))
pnl_s <- cbind(cl_ose, pnl_s)
colnames(pnl_s) <- c("VTI", "EWMA PnL")
# plot EWMA PnL without position shading
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(pnl_s, theme=plot_theme, 
  name="Performance of Ensemble EWMA Strategy")
legend("top", legend=colnames(pnl_s), 
  inset=0.05, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_ensemble.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Backtesting Active Investment Strategies}


%%%%%%%%%%%%%%%
\subsection{Aggregations Over Look-back and Look-forward Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An example of data aggregations are the past cumulative returns of the \emph{EWMA} strategies calculated over a vector of \emph{end points}. 
      \vskip1ex
      Overlapping aggregations can be specified by a vector of \emph{look-back} intervals attached at \emph{end points}.
      \vskip1ex
      For example, we may specify aggregations at monthly \emph{end points}, over overlapping 12-month \emph{look-back} intervals.
      \vskip1ex
      The variable \texttt{look\_back} is equal to the number of \emph{end points} in the \emph{look-back} interval.
      \vskip1ex
      The \emph{start points} are the \emph{end points} lagged by the length of the \emph{look-back} interval.
      \vskip1ex
      The \emph{look-back} intervals are spanned by the vectors of \emph{start points} and \emph{end points}.
      \vskip1ex
      Non-overlapping aggregations can also be calculated over a list of \emph{look-forward} intervals (\texttt{look\_fwds}).
      \vskip1ex
      The \emph{look-back} intervals should not overlap with the \emph{look-forward} intervals, in order to avoid data snooping.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define end of month end_points
end_points <- rutils::calc_endpoints(re_turns, 
                inter_val="months")
n_rows <- NROW(end_points)
# Start_points equal end_points lagged by 12-month look-back interval
look_back <- 12
start_points <- c(rep_len(1, look_back-1), 
  end_points[1:(n_rows-look_back+1)])
# Calculate past performance over end_points 
perform_ance <-
  function(re_turns) sum(re_turns)/sd(re_turns)
past_perf <- sapply(1:(n_rows-1), function(it_er) {
  sapply(re_turns[start_points[it_er]:end_points[it_er]], perform_ance)
})  # end sapply
past_perf <- t(past_perf)
fut_rets <- sapply(1:(n_rows-1), function(it_er) {
  sapply(re_turns[(end_points[it_er]+1):end_points[it_er+1]], sum)
})  # end sapply
fut_rets <- t(fut_rets)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In \emph{momentum} strategies, the portfolio weights are adjusted over time to be proportional to the past performance of the assets.
      \vskip1ex
      This way \emph{momentum} strategies switch their weights to the best performing assets.
      \vskip1ex
      The weights are scaled to limit the portfolio \emph{leverage} and its market \emph{beta}.
      <<echo=TRUE,eval=FALSE>>=
# calculate weight_s proportional to past_perf
weight_s <- past_perf
weight_s[weight_s<0] <- 0
# scale weight_s so their sum is equal to 1
weight_s <- weight_s/rowSums(weight_s)
# set NA values to zero
weight_s[is.na(weight_s)] <- 0
sum(is.na(weight_s))
in_dex <- index(re_turns[end_points[-n_rows]])
trend_weights <- rowMeans(weight_s[, 1:NCOL(trend_returns)])
revert_weights <- rowMeans(weight_s[, -(1:NCOL(trend_returns))])
diff_weights <- xts(trend_weights-revert_weights, order.by=in_dex)
# Find best and worst EWMA Strategies in each period
bes_t <- apply(weight_s, 1, which.max)
wors_t <- apply(weight_s, 1, which.min)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_diff_weights.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot the mean weights of EWMA Strategies
zoo::plot.zoo(cbind(diff_weights, 
  cl_ose[end_points[-n_rows]]), 
  oma = c(3, 0, 3, 0), mar = c(0, 4, 0, 1), 
  xlab=NULL, ylab=c("diff weights", "VTI"), 
  main="Trend minus Revert Weights of EWMA strategies")
best_worst <- xts(cbind(bes_t, wors_t), order.by=in_dex)
zoo::plot.zoo(best_worst,
  oma = c(3, 0, 3, 0), mar = c(0, 4, 0, 1), 
  xlab=NULL, ylab=c("best EWMA", "worst EWMA"), 
  main="Best and Worst EWMA strategies")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting the \protect\emph{EWMA} \protect\emph{Momentum} Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Backtesting} is the testing of the accuracy of a forecasting model using simulation on historical data.
      \vskip1ex
      \emph{Backtesting} is a type of \emph{cross-validation} applied to time series data.
      \vskip1ex
      \emph{Backtesting} is performed by \emph{training} the model on past data and \emph{testing} it on future out-of-sample data.
      \vskip1ex
      The \emph{training} data is specified by the \emph{look-back} intervals (\texttt{past\_aggs}), and the model forecasts are applied to the future data defined by the \emph{look-forward} intervals (\texttt{fwd\_rets}).
      \vskip1ex
      The out-of-sample \emph{momentum} strategy returns can be calculated by multiplying the \texttt{fwd\_rets} by the forecast \emph{ETF} portfolio weights.
      <<echo=TRUE,eval=FALSE>>=
# Calculate backtest returns
pnl_s <- rowSums(weight_s*fut_rets)
pnl_s <- xts(pnl_s, order.by=in_dex)
colnames(pnl_s) <- "ewma momentum"
close_rets <- rutils::diff_it(cl_ose[in_dex])
cor(cbind(pnl_s, close_rets))
pnl_s <- star_t*exp(cumsum(pnl_s))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/backtest_ewma.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# plot the backtest
chart_Series(x=cl_ose[end_points[-n_rows]], 
  name="backtest of EWMA strategies", col="orange")
add_TA(pnl_s, on=1, lwd=2, col="blue")
legend("top", legend=c("VTI", "EWMA"), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=c("orange", "blue"), bty="n")
# shad_e <- xts(index(pnl_s) < as.Date("2008-01-31"), order.by=index(pnl_s))
# add_TA(shad_e, on=-1, col="lightgrey", border="lightgrey")
# text(x=7, y=0, labels="warmup period")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy for an \protect\emph{ETF} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{momentum} strategy can be \emph{backtested} for a portfolio of \emph{ETFs} or stocks over a series of \emph{end points} as follows:
      \setlength{\leftmargini}{1.0em}
      \begin{enumerate}
        \item Trade a single unit (dollar) of capital, 
        \item Allocate capital to the assets in proportion to their \emph{relative} past performance, 
        \item Calculate the portfolio weights as the number of units (shares) of each asset,
        \item At each \emph{end point} repeat the above and rebalance the asset allocations,
        \item Calculate the future out-of-sample portfolio returns in each period (without reinvestment).
        \item Calculate the transaction costs (as the bid-offer spread times the asset prices, times the change in weights), and subtract them from the returns.
      \end{enumerate}
      The above points \texttt{\#1, \#2} and \texttt{\#3} represent the \emph{momentum} strategy, while points \texttt{\#4, \#5} and \texttt{\#6} represent the \emph{backtest} procedures.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate ETF prices and simple returns
sym_bols <- c("VTI", "IEF", "DBC")
price_s <- rutils::etf_env$price_s[, sym_bols]
price_s <- na.omit(zoo::na.locf(price_s))
re_turns <- rutils::diff_it(price_s)
# Define look-back and look-forward intervals
end_points <- rutils::calc_endpoints(re_turns, 
  inter_val="months")
n_cols <- NCOL(re_turns)
n_rows <- NROW(end_points)
look_back <- 12
start_points <- c(rep_len(1, look_back-1), 
  end_points[1:(n_rows-look_back+1)])
# Calculate past performance over end_points
perform_ance <- 
  function(re_turns) sum(re_turns)/sd(re_turns)
agg_s <- sapply(1:(n_rows-1), function(it_er) {
  c(past_perf=sapply(re_turns[start_points[it_er]:end_points[it_er]], perform_ance),
    fut_rets=sapply(re_turns[(end_points[it_er]+1):end_points[it_er+1]], sum))
})  # end sapply
agg_s <- t(agg_s)
# Select look-back and look-forward aggregations
past_perf <- agg_s[, 1:n_cols]
fut_rets <- agg_s[, n_cols+1:n_cols]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting the \protect\emph{Momentum} Strategy for an \protect\emph{ETF} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The hypothetical \emph{momentum} strategy returns can be calculated by multiplying the forecast portfolio weights times the forward (future) out-of-sample returns.
      <<echo=TRUE,eval=FALSE>>=
# Calculate portfolio weights equal to number of shares
end_prices <- price_s[end_points[-n_rows]]
weight_s <- 
  past_perf/rowSums(abs(past_perf))/end_prices
weight_s[is.na(weight_s)] <- 0
colnames(weight_s) <- colnames(re_turns)
# Calculate profits and losses
pnl_s <- rowSums(weight_s*fut_rets)
pnl_s <- xts(pnl_s, index(end_prices))
colnames(pnl_s) <- "pnls"
# Calculate transaction costs
bid_offer <- 0.001
cost_s <- 
  0.5*bid_offer*end_prices*abs(rutils::diff_it(weight_s))
cost_s <- rowSums(cost_s)
pnl_s <- (pnl_s - cost_s)
pnl_s <- cumsum(pnl_s)
# plot momentum strategy with VTI
cl_ose <- price_s[index(end_prices), "VTI"]
zoo::plot.zoo(cbind(cl_ose, pnl_s, weight_s), 
  oma = c(3, 1, 3, 0), mar = c(0, 4, 0, 1), nc=1,
  xlab=NULL, main="ETF Momentum Strategy")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_weights.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Functional for \protect\emph{Momentum} Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# define backtest functional
backtest_ep <- function(re_turns, price_s, perform_ance=sum, 
    look_back=12, re_balance="months", bid_offer=0.001,
    end_points=rutils::calc_endpoints(re_turns, inter_val=re_balance), 
    with_weights=FALSE, ...) {
  stopifnot("package:quantmod" %in% search() || require("quantmod", quietly=TRUE))
  # Define look-back and look-forward intervals
  n_cols <- NCOL(re_turns)
  n_rows <- NROW(end_points)
  start_points <- c(rep_len(1, look_back-1), end_points[1:(n_rows-look_back+1)])
  # Calculate past performance over end_points
  agg_s <- sapply(1:(n_rows-1), function(it_er) {
    c(past_perf=sapply(re_turns[start_points[it_er]:end_points[it_er]], perform_ance, ...),  # end sapply
    fut_rets=sapply(re_turns[(end_points[it_er]+1):end_points[it_er+1]], sum))  # end sapply
  })  # end sapply
  agg_s <- t(agg_s)
  # Select look-back and look-forward aggregations
  past_perf <- agg_s[, 1:n_cols]
  fut_rets <- agg_s[, n_cols+1:n_cols]
  # Calculate portfolio weights equal to number of shares
  end_prices <- price_s[end_points[-n_rows]]
  weight_s <- past_perf/rowSums(abs(past_perf))/end_prices
  weight_s[is.na(weight_s)] <- 0
  colnames(weight_s) <- colnames(re_turns)
  # Calculate profits and losses
  pnl_s <- rowSums(weight_s*fut_rets)
  pnl_s <- xts(pnl_s, index(end_prices))
  colnames(pnl_s) <- "pnls"
  # Calculate transaction costs
  cost_s <- 0.5*bid_offer*end_prices*abs(rutils::diff_it(weight_s))
  cost_s <- rowSums(cost_s)
  pnl_s <- (pnl_s - cost_s)
  pnl_s <- cumsum(pnl_s)
  if (with_weights)
    cbind(pnl_s, weight_s)
  else
    pnl_s
}  # end backtest_ep
      @
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimization of \protect\emph{Momentum} Strategy Parameters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The performance of the \emph{momentum} strategy depends on the length of the \emph{look-back interval} used for calculating the past performance.
      \vskip1ex
      Performing a \emph{backtest} allows finding the optimal \emph{momentum} (trading) strategy parameters, such as the \emph{look-back interval}.
      \vskip1ex
      But using a different rebalancing frequency in the \emph{backtest} can produce different values for the optimal trading strategy parameters.
      \vskip1ex
      So \emph{backtesting} just redefines the problem of finding (tuning) the optimal trading strategy parameters, into the problem of finding the optimal \emph{backtest} (meta-model) parameters.
      \vskip1ex
      But the advantage of using the \emph{backtest} meta-model is that it can reduce the number of parameters that need to be optimized.
      \vskip1ex
      Performing many \emph{backtests} on multiple trading strategies risks identifying inherently unprofitable trading strategies as profitable, purely by chance (\emph{p-value hacking}).
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/look_back_profile.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
source("C:/Develop/R/lecture_slides/scripts/back_test.R")
look_backs <- seq(5, 60, by=5)
perform_ance <- function(re_turns) sum(re_turns)/sd(re_turns)
pro_files <- sapply(look_backs, function(x) {
  last(backtest_ep(re_turns=re_turns, price_s=price_s, 
    re_balance="weeks", look_back=x, perform_ance=perform_ance))
})  # end sapply
plot(x=look_backs, y=pro_files, t="l", 
  main="Strategy PnL as function of look_back", 
  xlab="look_back (weeks)", ylab="pnl")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy Performance}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The hypothetical out-of-sample \emph{momentum} strategy returns can be calculated by multiplying the \texttt{fwd\_rets} by the forecast \emph{ETF} portfolio weights.
      \vskip1ex
      The \emph{training} data is specified by the \emph{look-back} intervals (\texttt{back\_aggs}), and the forecasts are applied to the future data defined by the \emph{look-forward} intervals (\texttt{fwd\_rets}).
      <<echo=TRUE,eval=FALSE>>=
look_back <- look_backs[which.max(pro_files)]
pnl_s <- backtest_ep(re_turns=re_turns, price_s=price_s, 
  re_balance="weeks", look_back=look_back, perform_ance=perform_ance,
  with_weights=TRUE)
cl_ose <- Cl(rutils::etf_env$VTI[index(pnl_s)])
# bind model returns with VTI
da_ta <- star_t
da_ta <- cbind(cl_ose, da_ta*pnl_s[, 1]+da_ta)
colnames(da_ta) <- c("VTI", "momentum")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_backtest.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot momentum strategy with VTI
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(da_ta, theme=plot_theme, lwd=2, 
             name="Momentum PnL")
legend("topleft", legend=colnames(da_ta), 
  inset=0.1, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Combining the \protect\emph{Momentum} and Static Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{momentum} strategy has attractive returns compared to a static buy-and-hold strategy.
      \vskip1ex
      But the \emph{momentum} strategy suffers from draw-downs called \emph{momentum crashes}, especially after the market rallies from a sharp-sell-off.
      \vskip1ex
      This suggests that combining the \emph{momentum} strategy with a static buy-and-hold strategy can achieve significant diversification of risk.
      <<echo=TRUE,eval=FALSE>>=
# combine momentum strategy with static
da_ta <- cbind(da_ta, 0.5* (da_ta[, "VTI"] + da_ta[, "momentum"]))
colnames(da_ta) <- c("VTI", "momentum", "combined")
# calculate strategy annualized Sharpe ratios
sapply(da_ta, function(cumu_lative) {
  x_ts <- na.omit(diff(log(cumu_lative)))
  sqrt(52)*sum(x_ts)/sd(x_ts)/NROW(x_ts)
})  # end sapply
# calculate strategy correlations
cor(na.omit(diff(log(da_ta))))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_combined.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot momentum strategy combined with VTI
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue", "green")
chart_Series(da_ta, theme=plot_theme, 
             name="Momentum strategy combined with VTI")
legend("topleft", legend=colnames(da_ta), 
  inset=0.1, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy Versus the \protect\emph{All-Weather} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{All-Weather} portfolio is a static portfolio of stocks (30\%), bonds (55\%), and commodities and precious metals (15\%) (approximately), and was designed by Bridgewater Associates, the largest hedge fund in the world:\\
      \url{https://www.bridgewater.com/research-library/the-all-weather-strategy/}
      \url{http://www.nasdaq.com/article/remember-the-allweather-portfolio-its-having-a-killer-year-cm685511}
      \vskip1ex
      The three different asset classes (stocks, bonds, commodities) provide positive returns under different economic conditions (recession, expansion, inflation).
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define all-weather symbols and weights
weight_s <- c(0.30, 0.55, 0.15)
all_weather <- (re_turns / price_s) %*% weight_s
all_weather <- cumsum(all_weather)
all_weather <- xts(all_weather, index(re_turns))[index(pnl_s)]
all_weather <- star_t*all_weather + 
  star_t
colnames(all_weather) <- "all_weather"
# combine momentum strategy with all-weather
da_ta <- cbind(da_ta, all_weather)
# calculate strategy annualized Sharpe ratios
sapply(da_ta, function(cumu_lative) {
  x_ts <- na.omit(diff(log(cumu_lative)))
  sqrt(52)*sum(x_ts)/sd(x_ts)/NROW(x_ts)
})  # end sapply
# calculate strategy correlations
cor(na.omit(diff(log(da_ta))))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_all_weather.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot momentum strategy, combined, and all-weather
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue", "green", "violet")
chart_Series(da_ta, theme=plot_theme, lwd=2, name="Momentum PnL")
legend("topleft", legend=colnames(da_ta),
  inset=0.1, bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
      \vspace{-1em}
      The combination of bonds, stocks, and commodities in the \emph{All-Weather} portfolio is designed to provide positive returns under most economic conditions, without the costs of trading.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy Market Beta}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{momentum} strategy market beta can be calculated by multiplying the \emph{ETF} betas by the \emph{ETF} portfolio weights.
      <<echo=TRUE,eval=FALSE>>=
# calculate betas
beta_s <- c(1, rutils::etf_env$capm_stats[
  match(sym_bols[-1], 
        rownames(rutils::etf_env$capm_stats)), 
  "Beta"])
names(beta_s)[1] <- sym_bols[1]
# weights times betas
weight_s <- price_s[index(pnl_s)]*pnl_s[, -1]
beta_s <- weight_s %*% beta_s
beta_s <- xts(beta_s, order.by=index(weight_s))
colnames(beta_s) <- "portf_beta"
zoo::plot.zoo(cbind(beta_s, cl_ose),
  oma = c(3, 1, 3, 0), mar = c(0, 4, 0, 1), 
  main="betas & VTI", xlab="")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_betas.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy Market Timing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{market timing} ability of the \emph{momentum} strategy can be measured by performing \emph{linear regression} tests on its returns.
      \vskip1ex
      The \emph{market timing} tests are generalizations of the \emph{Capital Asset Pricing Model}.
      \vskip1ex
      The \emph{Merton-Henriksson} test measures the ability to forecast the direction of market returns:
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + \gamma \max{(0, R_m - R_f)} + {\varepsilon}
      \end{displaymath}
      The \emph{Treynor-Mazuy} test measures the ability to forecast both the direction and magnitude of future market returns:
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + \gamma (R_m - R_f)^2 + {\varepsilon}
      \end{displaymath}
      Where $R$ are the model returns, $R_m$ are the market returns, and $R_f$ are the risk-free returns.
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
momentum_rets <- as.numeric(rutils::diff_it(pnl_s[, 1]))
vti_rets <- as.numeric(rutils::diff_it(cl_ose)/100)
# Merton-Henriksson test
vti_b <- cbind(vti_rets, vti_rets+abs(vti_rets))
colnames(vti_b) <- c("rets", "sign")
mod_el <- lm(momentum_rets ~ vti_b)
summary(mod_el)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_timing.png}
    \vspace{-2em}
      <<echo=(-(1:4)),eval=FALSE>>=
# open x11 for plotting
x11(width=6, height=4)
# set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 3, 1), oma=c(0, 0, 0, 0))
# Treynor-Mazuy test
vti_b <- cbind(vti_rets, vti_rets^2)
colnames(vti_b) <- c("rets", "squared")
mod_el <- lm(momentum_rets ~ vti_b)
summary(mod_el)
# plot scatterplot
plot(x=vti_rets, y=momentum_rets,
     xlab="VTI", ylab="momentum")
title(main="Treynor-Mazuy market timing test\n for Momentum vs VTI", line=0.5)
# plot fitted (predicted) response values
points(x=vti_rets, y=mod_el$fitted.values,
       pch=16, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy Returns Skew}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The distribution of returns of the \emph{momentum} strategy has about one half of the negative skew compared to \emph{VTI}.
      \vskip1ex
      The \emph{momentum} strategy indicates the existence of a market anomaly (outsized profits), because it has a smaller negative skew than the market, while having comparable returns to the market.
      <<echo=TRUE,eval=FALSE>>=
# Normalize the returns
momentum_rets <- 
  (momentum_rets-mean(momentum_rets))
momentum_rets <- 
  sd(vti_rets)*momentum_rets/sd(momentum_rets)
vti_rets <- (vti_rets-mean(vti_rets))
# calculate ratios of moments
sapply(2:4, FUN=moments::moment, x=vti_rets)/
  sapply(2:4, FUN=moments::moment, x=momentum_rets)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_distr.png}
    \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# plot histogram
x_lim <- 4*sd(momentum_rets)
hist(momentum_rets, breaks=30, 
  main="Momentum and VTI Return Distributions", 
  xlim=c(-x_lim, x_lim), 
  xlab="", ylab="", freq=FALSE)
# draw kernel density of histogram
lines(density(momentum_rets), col='red', lwd=2)
lines(density(vti_rets), col='blue', lwd=2)
# add legend
legend("topright", inset=0.05, cex=0.8, title=NULL,
       leg=c("Momentum", "VTI"),
       lwd=6, bg="white", col=c("red", "blue"))
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Homework Assignment}


%%%%%%%%%%%%%%%
\subsection{Homework Assignment}
\begin{frame}[t]{\secname}
\vspace{-1em}
\begin{block}{Required}
  \begin{itemize}[]
    \item Study all the lecture slides in \texttt{FRE7241\_Lecture\_2.pdf}, and run all the code in \texttt{FRE7241\_Lecture\_2.R},
    \item Study \emph{bootstrap simulation} from the files \emph{bootstrap\_technique.pdf} and \emph{doBootstrap\_primer.pdf},
    \item Study the following sections in the file \texttt{numerical\_analysis.pdf}: 
    \begin{itemize}[]
    {\scriptsize
      \item Numerical Calculations, 
      \item Optimizing \texttt{R} Code for Speed and Memory Usage, 
      \item Writing Fast \texttt{R} Code Using Vectorized Operations, 
      \item Simulation, 
      \item Parallel Computing in \texttt{R}, 
      \item Run the code corresponding to the above sections from \texttt{numerical\_analysis.R}
    }
    \end{itemize}
  \end{itemize}
\end{block}

\begin{block}{Recommended}
  Read the following sections in the file \texttt{R\_environment.pdf}: 
  \begin{itemize}[]
    \item \emph{Environments in \texttt{R}}, 
    \item \emph{Data Input and Output}, 
    \item Run the code corresponding to the above sections from \texttt{R\_environment.R}
  \end{itemize}
\end{block}

\end{frame}


\end{document}
